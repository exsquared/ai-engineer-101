{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udf1f Introduction : AI Engineer 101","text":""},{"location":"#the-tech-world-changed-fast","title":"\ud83c\udf2a\ufe0f The Tech World Changed \u2014 Fast","text":"<p>If you're feeling overwhelmed right now, you're not alone.</p> <p>Since the ChatGPT moment, the tech industry has been running in fast-forward. You open Twitter or LinkedIn, and suddenly everyone is talking about:</p> <ul> <li>Vector databases</li> <li>LangChain, RAG, agents</li> <li>Transformers, finetuning, quantization</li> <li>Prompts, few-shot learning, LLMOps</li> </ul> <p>And it\u2019s easy to feel like you\u2019re already behind.</p> <p>Maybe you\u2019ve thought:</p> <p>\u201cAm I still relevant?\u201d \u201cWill I be replaced by AI?\u201d \u201cShould I stop writing backend code and start writing prompts?\u201d</p> <p>Pause. Breathe. You\u2019re not behind. You\u2019re right on time.</p>"},{"location":"#fundamentals-didnt-change-they-got-more-valuable","title":"\ud83e\uddf1 Fundamentals Didn\u2019t Change \u2014 They Got More Valuable","text":"<p>If you\u2019ve been building production systems \u2014 shipping, debugging, scaling, reasoning \u2014 you already have the most important skill AI can\u2019t replace: engineering judgment.</p> <p>That\u2019s what this book is about: giving builders the AI tools to stay ahead, not by abandoning what they know, but by upgrading it.</p> <p>The pace of AI may be fast. But great engineering is still slow thinking:</p> <ul> <li>What\u2019s the right abstraction?</li> <li>What can fail?</li> <li>Can this system be debugged, monitored, and improved?</li> </ul> <p>If fundamentals didn\u2019t matter, we wouldn\u2019t still be learning the Gang of Four or Unix philosophy. But we are \u2014 because principles last, tools change.</p> <p>LLMs are just a new layer of capability. The rules of good software still apply \u2014 now you just have more powerful building blocks.</p>"},{"location":"#why-senior-engineers-matter-more-than-ever","title":"\ud83e\udde0 Why Senior Engineers Matter More Than Ever","text":"<p>The internet is now flooded with GenAI demos. But very few people can:</p> <ul> <li>Turn those ideas into real, reliable systems</li> <li>Integrate them into products with security, latency, cost in mind</li> <li>Design workflows that combine LLMs, APIs, databases, and humans</li> </ul> <p>And that\u2019s where you come in.</p> <p>If you\u2019ve been building APIs, frontends, microservices, or databases \u2014 congratulations. You\u2019re not behind. You\u2019re now in the most critical position of all:</p> <p>To lead the next generation of intelligent systems \u2014 and build things that actually work.</p>"},{"location":"#why-this-book-exists","title":"\ud83c\udfaf Why This Book Exists","text":"<p>Artificial Intelligence is no longer just a futuristic buzzword. It\u2019s here. It\u2019s useful. And it\u2019s changing how software is built.</p> <p>But with so many tutorials, frameworks, and papers out there, it\u2019s easy to fall into tutorial hell\u2014watching endlessly, building nothing.</p> <p>This book is your way out.</p> <p>We\u2019ve designed this for developers who learn by doing, want to ship things that matter, and don\u2019t have time for fluff.</p> <p>You\u2019ll learn to: - Build real AI systems that solve real problems - Use confidence scores, fallback logic, and thresholds like a pro - Think like a system architect, not just a model tinkerer - Deploy and monitor your models in actual production flows</p> <p>Whether you're automating support tickets or experimenting with GPT-powered workflows, this book gets you shipping AI with confidence.</p>"},{"location":"#who-this-book-is-for","title":"\ud83d\udc65 Who This Book Is For","text":"<ul> <li>Engineers who can code, but are new to AI/ML</li> <li>Full-stack developers who want to add intelligence to their apps</li> <li>Builders who want to go from \u201cWhat\u2019s an embedding?\u201d to \u201cHere\u2019s my deployed AI system\u201d</li> </ul> <p>You don\u2019t need to know what a \u201cvectorizer\u201d or \u201cdecision boundary\u201d is. We\u2019ll meet you where you are \u2014 and move forward, together.</p>"},{"location":"#what-this-book-is-and-isnt","title":"\ud83e\udded What This Book Is (and Isn\u2019t)","text":"<p>This book isn\u2019t:</p> <ul> <li>A math-heavy theory textbook</li> <li>A prompt engineering magic trick guide</li> <li>A build-your-own-GPT manual</li> </ul> <p>This book is:</p> <ul> <li>A fast-moving, hands-on course for working engineers</li> <li>Focused on intuition, system design, and reliability</li> <li>Layered with real examples, deployment patterns, and tradeoffs</li> <li>Crafted to help you use AI intelligently, not just integrate it</li> </ul>"},{"location":"#what-youll-learn","title":"\ud83e\uddf0 What You'll Learn","text":"<ul> <li>What \u201clearning from data\u201d really means (and when to avoid it)</li> <li>How to go from hard-coded logic \u2192 models \u2192 agents</li> <li>How to use <code>.predict_proba()</code>, thresholds, fallbacks, and evaluation metrics</li> <li>Where GenAI fits into a real software stack (and where it breaks)</li> <li>How to design production-friendly workflows using small models, LLM APIs, and rules</li> </ul> <p>Every chapter includes:</p> <ul> <li>\ud83d\udd0d Real-world analogies to simplify concepts</li> <li>\ud83e\uddea Code examples you can run and modify</li> <li>\ud83d\udcdd Quizzes and reflections to reinforce intuition</li> <li>\ud83d\udcbb Mini-projects and checkpoints</li> <li>\ud83d\udce6 \u201cDesign Diary\u201d prompts for thinking like an engineer</li> </ul>"},{"location":"#what-youll-walk-away-with","title":"\ud83d\ude80 What You'll Walk Away With","text":"<ul> <li>Clear intuition about models and learning</li> <li>The confidence to debug or critique AI outputs</li> <li>A working knowledge of GenAI and classic ML techniques</li> <li>Projects that prove you understand the tools, not just watched them</li> </ul> <p>You\u2019ll finish with:</p> <ul> <li>Confidence  </li> <li>Code  </li> <li>A working AI intuition</li> </ul>"},{"location":"#how-to-use-this-book","title":"\ud83d\udee0 How to Use This Book","text":"<ul> <li>Read the chapters in order. They build on each other.</li> <li>Run the code, even if it looks simple.</li> <li>Use the reflection and design sections \u2014 they\u2019re how you think like an AI engineer.</li> <li>Don\u2019t get stuck chasing perfection. Build and revisit.</li> </ul> <p>And when something clicks, don\u2019t stop there \u2014 ship it.</p>"},{"location":"#where-this-leads","title":"\ud83c\udf31 Where This Leads","text":"<p>This is the Beginner Track \u2014 the first step in your AI engineering journey.</p> <p>When you\u2019re done, you\u2019ll be ready for:</p> <ul> <li>Intermediate AI Engineering (retrieval, vector search, agents)</li> <li>Advanced AI Architectures (RAG pipelines, memory, orchestration)</li> </ul> <p>But first: let's escape tutorial hell. Let\u2019s build real AI systems.</p> <p>Let\u2019s go. \ud83d\udca1</p>"},{"location":"capstone_project/","title":"\ud83d\ude80 Final Capstone Project: Compose and Deploy Your Own AI-Powered System","text":""},{"location":"capstone_project/#why-this-project","title":"\ud83c\udfaf Why This Project?","text":"<p>You\u2019ve completed the beginner track \u2014 from classical ML to modern LLMs, from prompting to agents, from local prototypes to scalable APIs.</p> <p>Now it\u2019s time to build a complete AI system \u2014 one that shows your skills as a builder, engineer, and system thinker.</p>"},{"location":"capstone_project/#what-youll-build","title":"\ud83d\udce6 What You'll Build","text":"<p>Design and deploy a real AI-powered application using:</p> <ul> <li>LLMs (GPT-4 / OpenAI / Claude)</li> <li>Prompt composition + context injection</li> <li>Tool calling or API orchestration</li> <li>Optional: memory, agent loop, or multimodal input</li> </ul> <p>This is your chance to demonstrate: \u2705 Technical depth \u2705 Practical creativity \u2705 Product-minded design</p>"},{"location":"capstone_project/#pick-one-of-these-capstone-tracks","title":"\ud83e\udde0 Pick One of These Capstone Tracks","text":""},{"location":"capstone_project/#track-a-knowledge-assistant-rag-bot","title":"\ud83c\udd70\ufe0f Track A: Knowledge Assistant / RAG Bot","text":"<p>Build a system that:</p> <ul> <li>Answers user questions from private documents (e.g., support tickets, product docs, HR policies)</li> <li>Injects context into LLM prompts (manual or vector search)</li> <li>Decides when to escalate or disclaim confidently</li> </ul> <p>Bonus:</p> <ul> <li>Add a feedback loop for user corrections  </li> <li>Add memory (conversation tracking)  </li> <li>Add a UI or chat interface</li> </ul>"},{"location":"capstone_project/#track-b-agent-based-task-orchestrator","title":"\ud83c\udd71\ufe0f Track B: Agent-Based Task Orchestrator","text":"<p>Build a mini agent that:</p> <ul> <li>Accepts a task or instruction</li> <li>Plans a sequence of steps</li> <li>Calls tools or APIs (math, search, databases, etc.)</li> <li>Responds with the completed result</li> </ul> <p>Bonus:</p> <ul> <li>Use LangChain or function-calling  </li> <li>Show logging for thought/action/observation steps  </li> <li>Add retry/fallback logic</li> </ul>"},{"location":"capstone_project/#track-c-multimodal-genai-app","title":"\ud83c\udd72 Track C: Multimodal GenAI App","text":"<p>Build a vision-enhanced app that:</p> <ul> <li>Takes image or screenshot input</li> <li>Uses GPT-4V or DALL\u00b7E to process or generate images</li> <li>Returns a visual or structured response</li> </ul> <p>Examples:</p> <ul> <li>Screenshot explainer  </li> <li>Product image generator  </li> <li>Visual bug checker or chart interpreter</li> </ul>"},{"location":"capstone_project/#tech-stack-options","title":"\ud83d\udd28 Tech Stack Options","text":"<p>Use anything you\u2019ve already seen in this course:</p> <ul> <li>FastAPI or Streamlit (Ch. 9\u201310)  </li> <li>OpenAI API (Ch. 8 onward)  </li> <li>Manual prompt composition (Ch. 8c)  </li> <li>Basic fallback logic (Ch. 8c)  </li> <li>Function-calling or LangChain (Ch. 12)  </li> <li>Vision tools (Ch. 13)</li> </ul>"},{"location":"capstone_project/#capstone-deliverables","title":"\ud83d\udccb Capstone Deliverables","text":"<p>Your submission must include:</p> <ol> <li> <p>\u2705 System Description</p> <ul> <li>What problem are you solving?</li> <li>What use cases does it serve?</li> </ul> </li> <li> <p>\u2705 Architecture Diagram</p> <ul> <li>Show how prompts, tools, and APIs connect</li> </ul> </li> <li> <p>\u2705 Prompt Examples</p> <ul> <li>Show at least 3 prompt variations you tested and refined</li> </ul> </li> <li> <p>\u2705 Codebase (GitHub or Zip)</p> <ul> <li>Clean, runnable app or notebook with clear readme</li> </ul> </li> <li> <p>\u2705 Output Examples</p> <ul> <li>Screenshots or logs showing real user flows</li> </ul> </li> <li> <p>\u2705 Reflection Write-Up</p> <ul> <li>What worked? What failed? What surprised you?</li> <li>What would you improve in a v2?</li> </ul> </li> <li> <p>\u2705 Ethics / Reliability Statement</p> <ul> <li>How do you prevent misuse, hallucination, or bias?</li> <li>What fallback logic did you implement?</li> </ul> </li> </ol>"},{"location":"capstone_project/#capstone-badge","title":"\ud83c\udfc5 Capstone Badge","text":"<p>Completing this project earns you the \u201cAI Systems Engineer\u201d badge \u2014 recognition that you\u2019re not just prompting, you\u2019re architecting.</p>"},{"location":"capstone_project/#reflection-prompts-optional","title":"\ud83e\uddd0 Reflection Prompts (Optional)","text":"<ul> <li>What part of the system challenged your assumptions?  </li> <li>What design decisions had the biggest impact on usability?  </li> <li>Where did you choose to trust the model, and where not to?</li> </ul>"},{"location":"capstone_project/#congratulations","title":"\ud83c\udf1f Congratulations!","text":"<p>You\u2019ve built your own AI-powered app \u2014 from problem to prompt to product. This is just the beginning. The next step? Start solving problems in the real world with what you now know.</p> <p>You\u2019re not just an AI engineer. You\u2019re a system builder. \ud83d\ude80</p>"},{"location":"chapter_1/","title":"From Rules to Learning","text":""},{"location":"chapter_1/#why-are-we-here","title":"\ud83c\udfaf Why Are We Here?","text":"<p>Ever wondered how Netflix seems to know exactly what you\u2019ll binge next or why your Instagram Explore page feels creepily accurate? They're not using billions of hand-coded <code>if/else</code> statements\u2014they've upgraded from rules to smarter systems that learn from patterns.</p> <p>But before diving into these trendy AI models (yes, like ChatGPT or HuggingFace Transformers!), we need to grasp how our trusty old logic breaks down and why machine learning became necessary.</p> <p>Ready to level up? \ud83d\ude80</p>"},{"location":"chapter_1/#youre-already-halfway-there","title":"\ud83e\uddf1 You're Already Halfway There","text":"<p>As a developer, you're already a decision-making pro:</p> <ul> <li>Writing <code>if/else</code> to filter data  </li> <li>Scoring items based on conditions  </li> <li>Automating alerts based on logic  </li> </ul> <p>You\u2019ve built systems that make decisions \u2014 and for a while, they work great.</p>"},{"location":"chapter_1/#lets-take-an-example","title":"\ud83d\udee0\ufe0f Let\u2019s Take an Example","text":"<p>Imagine you\u2019re building a simple tool to categorize customer feedback into:</p> <ul> <li>Praise  </li> <li>Complaint  </li> <li>Suggestion</li> </ul> <p>You start with rules:</p> <pre><code>if \"great\" in text or \"love\" in text:\n    return \"Praise\"\nelif \"should\" in text or \"hate\" in text:\n    return \"Complaint\"\nelse:\n    return \"Suggestion\"\n</code></pre> <p>It works! \ud83c\udf89</p> <p>...for about 10 examples.</p> <p>Then someone writes:</p> <p>\u201cI was hoping for more details, but overall it's fine.\u201d</p> <p>Uhhh... is that a praise? A complaint? A suggestion? All three?</p>"},{"location":"chapter_1/#where-rules-break-down","title":"\ud83d\ude29 Where Rules Break Down","text":"<p>This is the moment all devs hit: the rule that felt so clever suddenly can\u2019t keep up.</p> <p>And then, it begins:</p> <ul> <li>Rules explode \u2014 Edge cases multiply like gremlins  </li> <li>Maintenance becomes a nightmare \u2014 Logic updates faster than your morning coffee cools  </li> <li>Inflexibility sets in \u2014 Users write like humans, not boolean expressions  </li> </ul> <p>Suddenly, your elegant logic system is buried under exceptions, overrides, and weird hacks to make \u201cfine but also frustrated\u201d map to something coherent.</p> <p>You\u2019re not writing rules anymore. You\u2019re debugging language.</p>"},{"location":"chapter_1/#a-new-user-interface-paradigm","title":"\ud83c\udf0d A New User Interface Paradigm","text":"<p>Let\u2019s zoom out for a second.</p> <p>Before ChatGPT, most systems were built around clear sequences. Want to cancel your subscription? You\u2019d go to:</p> <p>Home \u2192 Account \u2192 Plans \u2192 Cancel</p> <p>Now?</p> <p>Users just type:</p> <p>\"I\u2019m moving out of the country. Can you help me stop the service next month?\"</p> <p>Same intent. Different form.</p> <ul> <li>No button.  </li> <li>No dropdown.  </li> <li>No exact keyword.  </li> <li>Just intent, embedded in phrasing and context.</li> </ul> <p>Your old rule:</p> <pre><code>if \"cancel\" in message:\n</code></pre> <p>won\u2019t cut it anymore.</p> <p>This is why we need systems that can listen, interpret, and adapt \u2014 not just match words.</p>"},{"location":"chapter_1/#variation-is-the-real-problem","title":"\ud83d\udc41\ufe0f Variation Is the Real Problem","text":"<p>Real-world input is messy:</p> <ul> <li>Users express the same intent a dozen different ways</li> <li>Words change meaning based on context</li> <li>Tone, grammar, slang all vary wildly</li> </ul> <p>Soon, your carefully coded rules become duct tape on a leaky pipe.</p> <p>\u201cI want to cancel everything.\u201d \u201cNeed to stop my plan after August.\u201d \u201cHow do I turn this thing off?\u201d</p> <p>All mean the same thing. But rules don\u2019t see that.</p>"},{"location":"chapter_1/#so-whats-the-alternative","title":"\ud83e\udde0 So What\u2019s the Alternative?","text":"<p>Let the machine learn from examples.</p> <p>Give it labeled examples like:</p> <pre><code>\"This is amazing!\" \u2192 Praise\n\"You should fix the layout.\" \u2192 Complaint\n\"It\u2019d be nice if it supported dark mode.\" \u2192 Suggestion\n</code></pre> <p>Then train a model:</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\nX = [\"This is amazing!\", \"You should fix the layout.\", \"It\u2019d be nice if...\"]\ny = [\"Praise\", \"Complaint\", \"Suggestion\"]\n\nvec = CountVectorizer()\nX_vec = vec.fit_transform(X)\n\nmodel = MultinomialNB()\nmodel.fit(X_vec, y)\n</code></pre> <p>Then:</p> <pre><code>model.predict(vec.transform([\"I loved the new design!\"]))\n# \u2192 \"Praise\"\n</code></pre> <p>Welcome to your first classifier. \ud83c\udf89</p>"},{"location":"chapter_1/#dont-worry-about-the-fancy-words","title":"\ud83d\ude4b\u200d\u2640\ufe0f Don\u2019t Worry About the Fancy Words","text":"<p>MultinomialNB? CountVectorizer?  Think of them as prebuilt tools, like <code>.filter()</code> or <code>.map()</code> \u2014 but for text classification.</p> <p>We'll dig deeper later. For now, just know: you didn\u2019t handwrite the logic, the machine learned it.</p>"},{"location":"chapter_1/#quick-reality-check-ai-isnt-always-the-answer","title":"\u26d4 Quick Reality Check: AI Isn\u2019t Always the Answer","text":"<p>Just because you can train a model doesn\u2019t mean you should.</p> <p>Some problems are better solved with code you already know:</p> <ul> <li>Is the logic simple and stable?</li> <li>Are there only a few clear cases?</li> <li>Do you need full explainability?</li> </ul> <p>Stick to rules or automation.</p> <p>If your condition is:</p> <pre><code>if hours_worked &gt; 40:\n    send_alert()\n</code></pre> <p>You don\u2019t need AI. You just need good engineering.</p> <p>Use the cheapest tool that solves the problem:</p> Level Tool Example Use 1 Rule-based Exact match, short form logic 2 Automation / Scripting Pattern detection, structured decisions 3 AI / ML Intent understanding, fuzzy matching"},{"location":"chapter_1/#tradeoff-table-rules-vs-models","title":"\u2696\ufe0f Tradeoff Table: Rules vs. Models","text":"Option Pros Cons When to Use Hard-coded Rules Fast, transparent, easy to debug Brittle, doesn\u2019t adapt Small, deterministic logic ML Model Learns from data, handles fuzziness Needs training, harder to debug Patterns too complex to hand-code Hybrid (Rules + ML) Best of both worlds More complex to manage When fallback or guardrails needed"},{"location":"chapter_1/#mini-experiment","title":"\ud83e\uddea Mini-Experiment","text":"<p>Try tweaking the examples:</p> <ul> <li>What if you add more complaints with the word \u201cslow\u201d?</li> <li>What happens if you feed it a sarcastic sentence?</li> <li>Try switching <code>MultinomialNB</code> to <code>LogisticRegression</code></li> </ul> <p>You\u2019re not just coding. You\u2019re choosing between behaviors.</p>"},{"location":"chapter_1/#reflection-corner-quiz","title":"\ud83e\udde0 Reflection Corner + Quiz","text":"<ul> <li>What kind of logic have you hardcoded in past projects?</li> <li>Where did it start breaking down?</li> <li>Can you imagine giving examples instead of rules?</li> <li>Give an example of a system where rules would still be better than AI.</li> <li>Think of a product you've built: where would learning-from-examples improve it?</li> </ul>"},{"location":"chapter_1/#quick-summary","title":"\ud83c\udfc1 Quick Summary","text":"<ul> <li>Rules work until they don\u2019t.</li> <li>Models let us learn from data.</li> <li>Even basic models can outperform rigid logic.</li> <li>AI is powerful, but not always necessary.</li> </ul> <p>You\u2019re already thinking like an AI engineer. Let\u2019s push further.</p>"},{"location":"chapter_1/#production-design-diary","title":"\ud83d\udce6 Production Design Diary","text":"<p>\ud83d\udee0\ufe0f Imagine your tool needs to handle 1,000 users a day instead of 1.</p> <p>Start with what you know:</p> <ul> <li>Have you used <code>try/except</code> to avoid crashes?</li> <li>Have you added logs or print statements to debug weird behavior?</li> <li>Ever added flags or rules to patch up last-minute edge cases?</li> </ul> <p>Then you\u2019re already thinking like a system designer.</p> <p>\ud83e\udde0 Try sketching this out:</p> <pre><code>Input \u2192 Rules \u2192 Model \u2192 Confidence Logic \u2192 Output\n</code></pre> <p>How would you:</p> <ul> <li>Detect silent failures?</li> <li>Handle bad GPT output?</li> <li>Know when it\u2019s time to update the model?</li> <li>Log decisions for debugging later?</li> </ul> <p>That\u2019s how real-world AI engineers operate.</p>"},{"location":"chapter_1/#whats-next","title":"\ud83d\udd2e What\u2019s Next","text":"<p>Next, you\u2019ll build your first real ML model from scratch. We\u2019ll keep it fun, small, and hands-on \u2014 but this time, with more control.</p> <p>No magic. No notebooks. Just Python, patterns, and progress.</p> <p>Let\u2019s go. \ud83d\ude80</p>"},{"location":"chapter_10/","title":"\ud83d\ude80 Scaling, Microservices &amp; DevOps for AI Systems","text":"<p>From Local Notebooks to Real APIs That Don\u2019t Fall Over</p>"},{"location":"chapter_10/#why-are-we-here","title":"\ud83e\udded Why Are We Here?","text":"<p>It\u2019s one thing to build a model. It\u2019s another to ship it \u2014 safely, scalably, and observably.</p> <p>In this chapter, we\u2019ll show how to:</p> <ul> <li>Scale a FastAPI AI service into production</li> <li>Handle retries, caching, queues, and timeouts</li> <li>Introduce observability and health monitoring</li> <li>Think like a microservice engineer, not a data scientist</li> </ul> <p>By the end, your model won\u2019t just work \u2014 it\u2019ll work reliably under pressure.</p>"},{"location":"chapter_10/#system-design-at-a-glance","title":"\ud83c\udfd7\ufe0f System Design at a Glance","text":"<p>Here\u2019s what a real-world AI service looks like in production:</p> <pre><code>Client \u2500\u2500\u2500&gt; API Gateway \u2500\u2500\u2500&gt; FastAPI AI Service \u2500\u2500\u2500&gt; Model\n                           \u2514\u2500\u2500&gt; Redis / Queue / Logs\n</code></pre> <p>Key components:</p> <ul> <li>FastAPI server: Exposes model as a REST API</li> <li>Queue (optional): Buffers long-running jobs</li> <li>Cache (Redis): Prevents repeated work</li> <li>Logger / Tracer: Tracks usage and errors</li> <li>Retry layer: Handles flaky services</li> <li>Health probe: Signals readiness</li> </ul>"},{"location":"chapter_10/#build-a-scalable-fastapi-ai-service","title":"\u2699\ufe0f Build a Scalable FastAPI AI Service","text":"<pre><code>from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\napp = FastAPI()\n\nclient = OpenAI()\n\nclass Query(BaseModel):\n    prompt: str\n\n@app.post(\"/generate\")\ndef generate(q: Query):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": q.prompt}],\n            timeout=10,\n        )\n        return {\"result\": response.choices[0].message.content}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre>"},{"location":"chapter_10/#add-caching-with-redis","title":"\ud83e\udde0 Add Caching with Redis","text":"<pre><code>import redis\nimport hashlib\n\nr = redis.Redis()\n\ndef cache_key(prompt: str):\n    return \"gen:\" + hashlib.sha1(prompt.encode()).hexdigest()\n\n@app.post(\"/generate\")\ndef generate(q: Query):\n    key = cache_key(q.prompt)\n    if (cached := r.get(key)):\n        return {\"result\": cached.decode()}\n\n    response = client.chat.completions.create(...)\n    result = response.choices[0].message.content\n    r.set(key, result, ex=3600)\n    return {\"result\": result}\n</code></pre>"},{"location":"chapter_10/#add-retry-logic","title":"\ud83d\udd01 Add Retry Logic","text":"<pre><code>import backoff\n\n@backoff.on_exception(backoff.expo, Exception, max_tries=3)\ndef call_model(prompt):\n    return client.chat.completions.create(...)\n</code></pre> <p>Retries help handle flaky upstreams (rate limits, timeouts, etc.).</p>"},{"location":"chapter_10/#add-logging-and-tracing","title":"\ud83d\udcca Add Logging and Tracing","text":"<pre><code>import logging\n\nlogger = logging.getLogger(\"uvicorn.error\")\n\n@app.post(\"/generate\")\ndef generate(q: Query):\n    logger.info(f\"Prompt: {q.prompt}\")\n    ...\n</code></pre> <p>\u2705 Use structured logs (JSON) for observability platforms like:</p> <ul> <li>Datadog</li> <li>Prometheus</li> <li>Grafana Loki</li> <li>OpenTelemetry + LangSmith</li> </ul>"},{"location":"chapter_10/#add-health-check-endpoint","title":"\u2705 Add Health Check Endpoint","text":"<pre><code>@app.get(\"/health\")\ndef health():\n    return {\"status\": \"ok\"}\n</code></pre> <p>Your orchestrator (e.g., Kubernetes) uses this to decide whether to route traffic or restart.</p>"},{"location":"chapter_10/#queueing-for-long-jobs-optional","title":"\ud83e\uddf5 Queueing for Long Jobs (Optional)","text":"<p>For heavyweight tasks (e.g. fine-tuning, PDF parsing), offload to a queue:</p> <pre><code>FastAPI \u2500\u2500\u2500&gt; Redis Queue \u2500\u2500\u2500&gt; Worker (Celery, RQ)\n</code></pre> <p>This lets you return 202 Accepted and poll for result later.</p>"},{"location":"chapter_10/#microservice-thinking-what-to-separate","title":"\ud83e\udde0 Microservice Thinking: What to Separate?","text":"<ul> <li>Separate UI from inference </li> <li>Separate retrieval from generation </li> <li>Separate controller logic (agent) from tools</li> </ul> <p>Split services by:</p> <ul> <li>Different latency profiles</li> <li>Different observability and security needs</li> <li>Different ownership teams</li> </ul>"},{"location":"chapter_10/#infra-tools-youll-want","title":"\ud83e\uddf0 Infra Tools You\u2019ll Want","text":"Tool Why It Helps Docker Consistent deployment across envs Kubernetes / ECS Scales your containerized API Redis Cache or queue layer Prometheus + Grafana Metrics &amp; dashboards Traceloop / LangSmith / Phoenix LLM tracing &amp; debugging"},{"location":"chapter_10/#recap-what-you-just-built","title":"\ud83e\udde0 Recap: What You Just Built","text":"<p>\u2705 A FastAPI service that wraps a model \u2705 Observability: logging, retries, health checks \u2705 Caching + optional queue \u2705 A pattern to grow into a real AI backend</p>"},{"location":"chapter_10/#reflection-questions","title":"\u2753Reflection Questions","text":"<ol> <li>What\u2019s the first bottleneck you\u2019d hit if 10k users hit this model?  </li> <li>How would you version and deploy a new model without downtime?  </li> <li>Which part of your API stack needs retries, caching, or batching?</li> </ol>"},{"location":"chapter_10/#mini-quiz","title":"\ud83e\uddea Mini Quiz","text":"<p>Q1. What\u2019s the benefit of Redis here? \u2705 a) Avoid recomputing the same prompt response</p> <p>Q2. Why should long-running jobs use queues? \u2705 b) So they don\u2019t block incoming API requests</p>"},{"location":"chapter_10/#microproject","title":"\ud83c\udfaf Microproject","text":"<p>\ud83d\udd27 Build a deployable GPT-backed FastAPI app with:</p> <ul> <li><code>/generate</code> endpoint</li> <li>Redis caching</li> <li>Basic logging</li> <li>A <code>/health</code> endpoint</li> </ul> <p>Bonus: Deploy it on Render or Fly.io for free.</p>"},{"location":"chapter_10/#exit-outcome","title":"\u2705 Exit Outcome","text":"<p>You now:</p> <ul> <li>Can structure and scale your AI microservice</li> <li>Know how to deploy, observe, retry, and cache safely</li> <li>Understand when to queue, when to batch, and when to split logic</li> </ul> <p>In the next chapter: Generative AI meets images, vision, and multimodal prompts. Let\u2019s go visual.</p>"},{"location":"chapter_11/","title":"Let LLMs Read Your Data","text":""},{"location":"chapter_11/#the-power-of-retrieval-augmented-generation-rag","title":"The Power of Retrieval-Augmented Generation (RAG)","text":""},{"location":"chapter_11/#why-are-we-here","title":"\ud83e\udded Why Are We Here?","text":"<p>Let\u2019s say you\u2019ve just deployed a customer-facing chatbot powered by GPT-4.</p> <p>It\u2019s slick. It\u2019s confident. But then a customer asks:  </p> <p>\u201cWhat\u2019s your enterprise SLA?\u201d</p> <p>And the bot responds:  </p> <p>\u201cWe do not currently offer SLAs.\u201d</p> <p>\ud83e\udd26\u200d\u2642\ufe0f Problem: The bot knows everything... except your actual business.</p> <p>That\u2019s because LLMs aren\u2019t trained on your internal data \u2014 your: - PDFs - FAQs - Wiki pages - Meeting notes - Slack threads</p> <p>To fix this, we need to teach our LLM to search and use your private data \u2014 without retraining it.</p> <p>That\u2019s what RAG does.</p>"},{"location":"chapter_11/#what-is-rag","title":"\ud83d\ude80 What Is RAG?","text":"<p>RAG = Retrieval-Augmented Generation It\u2019s a system design pattern that makes LLMs smarter by:</p> <ol> <li>Retrieving relevant content from your data</li> <li>Injecting that content into the prompt</li> <li>Letting the LLM generate an informed response</li> </ol>"},{"location":"chapter_11/#real-world-analogy","title":"\ud83e\uddea Real-World Analogy","text":"<p>You: \u201cHey, what's the refund policy in the 2023 docs?\u201d</p> <p>Your assistant: - Doesn\u2019t know off-hand - Searches \u201crefund\u201d in your docs - Finds the right paragraph - Reads it and replies:</p> <p>\u201cThe 2023 policy allows refunds within 30 days.\u201d</p> <p>That\u2019s RAG: search first, generate later.</p>"},{"location":"chapter_11/#new-concepts-youll-meet","title":"\ud83d\udee0\ufe0f New Concepts You\u2019ll Meet","text":"Concept Analogy Why It Matters Embeddings Like hashing text into meaning-space Makes text searchable by meaning Vector DB Like Elasticsearch for meaning Lets you find \u201csimilar\u201d content Context Injection Like pre-filling a prompt with facts Gives the model the right info at the right time LangChain Like Django/Express for LLM apps Helps you build LLM pipelines faster LangFlow Like Node-RED or n8n for LLMs Drag-and-drop builder for RAG + agents"},{"location":"chapter_11/#how-it-all-fits-together","title":"\ud83e\uddf1 How It All Fits Together","text":"<p>Let\u2019s say your app needs to answer questions from internal docs.</p> <p>You\u2019ll need:</p> <ol> <li>Ingest the documents (PDF, markdown, etc.)</li> <li>Chunk them into smaller paragraphs</li> <li>Embed each chunk into a vector</li> <li>Store the vectors in a vector database</li> <li>At query time: embed the question, find the most similar chunks</li> <li>Inject those chunks into the LLM\u2019s prompt</li> <li>Get a smart, grounded answer</li> </ol> <p>This pipeline is your RAG stack.</p>"},{"location":"chapter_11/#whats-a-vector-database","title":"\ud83d\uddc3\ufe0f What\u2019s a Vector Database?","text":"<p>Think: Search engine for meaning.</p> <p>You give it: - A sentence like: \u201cI want a refund\u201d - It returns: chunks semantically similar to that, even if the words are different (e.g., \u201cCan I return my item?\u201d)</p> <p>Popular options: - FAISS (lightweight, fast, local) - Chroma (easy to use, Python-native) - Weaviate, Pinecone, Qdrant (scalable, cloud-ready)</p>"},{"location":"chapter_11/#lets-build-a-mini-rag-system","title":"\ud83d\udcbb Let\u2019s Build a Mini RAG System","text":"<p>We\u2019ll use LangChain (a framework that glues all the pieces together).</p>"},{"location":"chapter_11/#step-1-load-and-chunk-documents","title":"Step 1: Load and Chunk Documents","text":"<pre><code>from langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nloader = TextLoader(\"company_policy.txt\")\ndocs = loader.load()\n\nsplitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\nchunks = splitter.split_documents(docs)\n</code></pre>"},{"location":"chapter_11/#step-2-embed-and-store-in-vector-db-faiss","title":"Step 2: Embed and Store in Vector DB (FAISS)","text":"<pre><code>from langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS\n\nembeddings = OpenAIEmbeddings()\nvectorstore = FAISS.from_documents(chunks, embeddings)\n</code></pre> <p>\ud83e\udde0 Think of <code>vectorstore</code> as a mini search engine based on meaning, not keywords.</p>"},{"location":"chapter_11/#step-3-run-a-rag-chain","title":"Step 3: Run a RAG Chain","text":"<pre><code>from langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\n\nllm = ChatOpenAI()\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    retriever=vectorstore.as_retriever()\n)\n\nresponse = qa_chain.run(\"What\u2019s the refund deadline?\")\nprint(response)\n</code></pre> <p>Boom \u2014 the model reads your docs and answers in plain English.</p>"},{"location":"chapter_11/#visual-workflow-optional-via-langflow","title":"\ud83d\uddbc\ufe0f Visual Workflow (Optional via LangFlow)","text":"<p>Want to see this flow as a drag-and-drop UI? Try LangFlow \u2014 a visual builder for LangChain apps.</p> <p>It\u2019s like Figma for AI workflows.</p> <p>You can: - Drop in nodes (loader, splitter, embedder, retriever, LLM) - Connect them visually - Export Python code</p>"},{"location":"chapter_11/#langchain-vs-barebones-code","title":"\ud83e\udde0 LangChain vs. Barebones Code","text":"Approach Use When... LangChain You want to move fast and glue pieces easily Raw Python You want maximum control, minimal abstraction LangFlow You want to prototype without code (great for teams!) <p>No lock-in. All tools work with OpenAI, Cohere, local models, etc.</p>"},{"location":"chapter_11/#why-rag-wins","title":"\ud83d\udd01 Why RAG Wins","text":"Fine-Tuning RAG Expensive to train Just needs vector search Slow to update Instant document refresh Bakes in data Keeps data external <p>Use RAG when: - Your data changes frequently - You want explainability - You\u2019re building internal tools, knowledge bases, support bots, etc.</p>"},{"location":"chapter_11/#recap-you-now-know","title":"\ud83e\udde0 Recap: You Now Know\u2026","text":"<p>\u2705 What embeddings, vector DBs, and RAG are \u2705 How to build a retrieval pipeline \u2705 When to use LangChain and LangFlow \u2705 Why RAG is the practical way to let LLMs \u201cread\u201d your data</p>"},{"location":"chapter_11/#reflection-questions","title":"\u2753Reflection Questions","text":"<ol> <li>How would you update your RAG system when a new policy is released?</li> <li>What are the trade-offs between fine-tuning and retrieval?</li> <li>Could your frontend search bar be upgraded with RAG? How?</li> </ol>"},{"location":"chapter_11/#mini-quiz","title":"\ud83e\uddea Mini Quiz","text":"<p>Q1. A vector DB helps you: a) Store full documents b) Find semantically similar chunks \u2705 c) Host your API d) Visualize neural nets</p> <p>Q2. LangChain is like: a) A new LLM model b) A framework to build LLM apps \u2705 c) A tokenizer d) An analytics tool</p>"},{"location":"chapter_11/#microproject","title":"\ud83e\uddea Microproject","text":"<p>Build a mini \u201cInternal Wiki Bot\u201d using: - A <code>.txt</code> or <code>.pdf</code> with company info - LangChain + FAISS (or raw Python + OpenAI embeddings) - Ask 3\u20135 questions and test retrieval accuracy</p> <p>Bonus: Add LangFlow to visualize your pipeline.</p>"},{"location":"chapter_11/#next-agents-tool-use","title":"\ud83d\udd1c Next: Agents &amp; Tool Use","text":"<p>Now that your LLM can read \u2014 can it also act?</p> <p>Next chapter: turning LLMs into agents that reason, plan, and use tools like APIs, calculators, or even Python code. Let\u2019s build a thinking assistant.</p>"},{"location":"chapter_12/","title":"Let LLMs Act, Not Just Answer","text":""},{"location":"chapter_12/#agents-reasoning-loops-and-tool-use","title":"Agents, Reasoning Loops, and Tool Use","text":""},{"location":"chapter_12/#why-are-we-here","title":"\ud83e\udded Why Are We Here?","text":"<p>Up until now, we\u2019ve used LLMs like really smart parrots.</p> <p>You feed them some context. They give you a great answer. That\u2019s it. One-shot, one-turn.</p> <p>But what if we want more?</p> <p>\u201cSearch Google for flight options, check the weather, compare it to my calendar, and suggest the best time to travel.\u201d</p> <p>This isn\u2019t just Q&amp;A. This is multi-step reasoning + tool use. You don\u2019t want the model to just answer. You want it to think, decide, and act.</p> <p>That\u2019s the world of agents.</p>"},{"location":"chapter_12/#what-is-an-agent","title":"\ud83e\udd16 What Is an Agent?","text":"<p>An Agent is an LLM system that: 1. Reads the user's intent 2. Plans a series of actions 3. Uses tools (APIs, functions, calculators, etc.) 4. Loops until it reaches a final answer</p> <p>Think of it like: - The LLM is the brain - The tools are its hands - The agent system is the \u201cnervous system\u201d orchestrating decisions</p>"},{"location":"chapter_12/#real-world-analogy","title":"\ud83e\udde0 Real-World Analogy","text":"<p>You: \u201cWhat\u2019s the cheapest non-stop flight from NYC to SF next Friday under 5 hours?\u201d</p> <p>Your intern: 1. Opens Google Flights 2. Applies filters 3. Looks at durations 4. Picks the cheapest one 5. Summarizes it for you</p> <p>They don\u2019t know the answer a priori \u2014 they figure it out by taking actions.</p>"},{"location":"chapter_12/#agent-loop-in-plain-english","title":"\ud83d\udd01 Agent Loop in Plain English","text":"<p>Every time the agent thinks: 1. Observe: What\u2019s the current task or question? 2. Think: What tool do I need? 3. Act: Use a tool (e.g., search API, calculator) 4. Read result: Did it solve the task? 5. Repeat: Until it reaches an answer</p> <p>This is called a Reasoning-Acting Loop.</p>"},{"location":"chapter_12/#tools-you-can-give-an-agent","title":"\ud83e\uddf0 Tools You Can Give an Agent","text":"Tool Type Example Calculator <code>\"4 * 17\"</code> Python code <code>\"Sort by date\"</code> Web search <code>\"Scrape this page\"</code> File reader <code>\"Read the invoice PDF\"</code> Custom API <code>\"GET /user/profile\"</code> <p>Each tool is just a function the agent is allowed to call.</p>"},{"location":"chapter_12/#frameworks-that-help","title":"\ud83d\udee0\ufe0f Frameworks That Help","text":"Framework Purpose LangChain Build agents from pre-built toolkits OpenAI Function Calling Let GPT \u201cchoose\u201d and call your functions CrewAI / AutoGen Multi-agent orchestration (advanced) <p>We\u2019ll focus on LangChain + OpenAI Function Calling for now.</p>"},{"location":"chapter_12/#how-function-calling-works-openai","title":"\u2699\ufe0f How Function Calling Works (OpenAI)","text":"<ol> <li>You define functions you want to expose  </li> <li>The LLM is told about their names and parameters  </li> <li>It chooses one and sends a structured JSON payload  </li> <li>Your code runs the function and feeds the result back  </li> <li>The model continues reasoning with the new info</li> </ol> <p>The model \u201cthinks\u201d and asks to use tools \u2014 your code runs the tool for it.</p>"},{"location":"chapter_12/#lets-build-a-simple-agent","title":"\ud83d\udcbb Let\u2019s Build a Simple Agent","text":""},{"location":"chapter_12/#step-1-define-tools","title":"\ud83e\uddee Step 1: Define Tools","text":"<pre><code>from langchain.agents import tool\n\n@tool\ndef get_exchange_rate(currency: str) -&gt; str:\n    if currency == \"USD\":\n        return \"1 USD = 83 INR\"\n    return f\"No data for {currency}\"\n</code></pre>"},{"location":"chapter_12/#step-2-create-the-agent","title":"\ud83e\udde0 Step 2: Create the Agent","text":"<pre><code>from langchain.chat_models import ChatOpenAI\nfrom langchain.agents import initialize_agent, AgentType\n\nllm = ChatOpenAI(model=\"gpt-4\")\n\nagent = initialize_agent(\n    tools=[get_exchange_rate],\n    llm=llm,\n    agent=AgentType.OPENAI_FUNCTIONS,\n    verbose=True,\n)\n\nagent.run(\"What is the exchange rate for USD?\")\n</code></pre>"},{"location":"chapter_12/#whats-happening-under-the-hood","title":"\ud83d\udd0d What\u2019s Happening Under the Hood?","text":"<ol> <li>The LLM reads your query  </li> <li>It decides to call <code>get_exchange_rate()</code> </li> <li>It sends the input <code>\"USD\"</code> </li> <li>Your Python function runs  </li> <li>The result is injected back into the prompt  </li> <li>The LLM completes the answer using the result</li> </ol> <p>You didn\u2019t hardcode any logic \u2014 the LLM figured it out.</p>"},{"location":"chapter_12/#agents-vs-rag","title":"\ud83e\udde0 Agents vs RAG","text":"Use Case RAG Agent Need doc info? \u2705 Use vector search \u2796 Only if you combine with RAG Need to act? \u2796 No tool usage \u2705 Call APIs, calculate, fetch, etc. Need both? \u2705 Combine RAG + Agents \u2705 Smartest choice"},{"location":"chapter_12/#gotchas-design-tips","title":"\u26a0\ufe0f Gotchas &amp; Design Tips","text":"<ul> <li>Tool reliability matters \u2014 bad APIs = broken agents  </li> <li>Loop limits prevent infinite reasoning  </li> <li>Logging is essential \u2014 observe the agent\u2019s thought chain  </li> <li>Prompting still matters \u2014 agents need structure and constraints</li> </ul>"},{"location":"chapter_12/#what-advanced-agents-can-do-preview","title":"\ud83d\udd2c What Advanced Agents Can Do (Preview)","text":"<ul> <li>Chain tools together  </li> <li>Maintain short-term memory  </li> <li>Work as multiple agents with roles  </li> <li>Execute plans from scratch (planner/executor)  </li> <li>Monitor their own uncertainty and retry</li> </ul>"},{"location":"chapter_12/#patterns-emerging-from-agents","title":"\ud83d\udce6 Patterns Emerging from Agents","text":""},{"location":"chapter_12/#toolformer","title":"\ud83d\udd39 Toolformer","text":"<p>Models trained not just to generate text \u2014 but to learn when and how to use tools during pretraining. Instead of hard-coding tool use, Toolformer predicts when a tool is needed and automatically integrates API usage into its generation. Great for automating complex decision + data workflows.</p>"},{"location":"chapter_12/#react","title":"\ud83d\udd39 ReAct","text":"<p>Short for Reasoning + Acting. A prompt strategy where the model moves step by step:</p> <pre><code>Thought: I need to check the current exchange rate  \nAction: get_exchange_rate(\"USD\")  \nObservation: 1 USD = 83 INR  \nThought: Now I can respond  \nAnswer: The rate is 83 INR\n</code></pre> <p>ReAct is helpful when you want transparent, debuggable steps and looping behavior.</p>"},{"location":"chapter_12/#planner-executor","title":"\ud83d\udd39 Planner-Executor","text":"<p>One LLM plans the high-level steps, another executes them. For example: - Planner: \u201cStep 1: Get exchange rate. Step 2: Calculate budget. Step 3: Suggest destination.\u201d - Executor: Runs each step using tools or APIs.</p> <p>This pattern: - Separates concerns - Enables modular debugging - Scales better in complex tasks</p>"},{"location":"chapter_12/#whats-coming-next-think-beyond-the-model","title":"\ud83e\udded What\u2019s Coming Next: Think Beyond the Model","text":"<p>Now that you\u2019ve mastered retrieval and reasoning, the next frontier is system-level intelligence. Here are key architecture patterns you'll encounter:</p>"},{"location":"chapter_12/#mcp-model-context-protocol","title":"\ud83e\uddf1 MCP \u2014 Model Context Protocol","text":"<p>A spec for how you feed models information. MCP defines: - What context types a model should receive (e.g. user history, environment, goals) - How that context is prioritized, formatted, and trimmed to fit the token budget - How to trace which context influenced which outputs</p> <p>It\u2019s like OpenAPI \u2014 but for prompts.</p>"},{"location":"chapter_12/#a2a-agent-to-agent-communication","title":"\ud83d\udd01 A2A \u2014 Agent-to-Agent Communication","text":"<p>Just like services talk via APIs, agents can talk via language. - A Planner Agent can send instructions to Worker Agents - Agents with different capabilities or memory scopes can cooperate</p> <p>You\u2019ll learn how to route subtasks, track ownership, and design protocols between agents.</p>"},{"location":"chapter_12/#context-engineering","title":"\ud83e\udde0 Context Engineering","text":"<p>LLMs are context-hungry. The trick isn\u2019t just more tokens \u2014 it\u2019s better tokens.</p> <p>You\u2019ll learn: - How to select the right document chunks - How to rank/filter based on user intent - How to format context (inline vs. structured)</p> <p>This is where retrieval meets UX meets architecture.</p>"},{"location":"chapter_12/#recap-you-now-know","title":"\ud83e\udde0 Recap: You Now Know\u2026","text":"<p>\u2705 What agents are and why they matter \u2705 How tool-calling works using function APIs \u2705 How the reasoning-act loop works \u2705 How to build agents with LangChain \u2705 When to use agents vs RAG \u2705 Common agent patterns \u2705 What\u2019s coming next at the system level</p>"},{"location":"chapter_12/#reflection-questions","title":"\u2753Reflection Questions","text":"<ol> <li>When should you prefer RAG over agents? When should you combine both?  </li> <li>What real-world apps in your org could benefit from tool-calling?  </li> <li>What risks should you monitor in agent loops and tool chaining?  </li> <li>How would you explain the ReAct pattern to a teammate?  </li> <li>How might MCP help your engineering team make LLM use more robust and auditable?</li> </ol>"},{"location":"chapter_12/#mini-quiz","title":"\ud83e\uddea Mini Quiz","text":"<p>Q1. What does an agent do that a normal LLM call doesn\u2019t? \u2705 c) Plans and calls tools</p> <p>Q2. Why are function-calling APIs safer than raw prompting? \u2705 b) You get structured outputs</p>"},{"location":"chapter_12/#microproject","title":"\ud83e\uddea Microproject","text":"<p>Build a Currency &amp; Travel Advisor Agent: - Ask the user: \u201cWhat currency do you want to check?\u201d - Use <code>get_exchange_rate()</code> to respond - Add another tool: <code>get_travel_advice(country)</code> - Chain them: currency \u2192 visa \u2192 safety tips - Log the agent\u2019s full reasoning steps</p>"},{"location":"chapter_12/#core-track-finale-what-youve-built","title":"\ud83c\udf89 Core Track Finale: What You\u2019ve Built","text":"<p>You\u2019ve now created: - A confident classifier - A doc-aware assistant (RAG) - A tool-using agent - And built the foundation of real LLM applications</p>"},{"location":"chapter_12/#next-chapter-let-the-model-see","title":"\ud83d\udd1c Next Chapter: Let the Model See","text":"<p>Next, you\u2019ll explore GPT-4V and multimodal models \u2014 where LLMs can read images, screenshots, graphs, and more.</p> <p>Welcome to the GenAI Playground.</p>"},{"location":"chapter_13/","title":"GenAI Playground","text":""},{"location":"chapter_13/#when-llms-can-see-draw-and-understand-the-world","title":"When LLMs Can See, Draw, and Understand the World","text":""},{"location":"chapter_13/#why-are-we-here","title":"\ud83e\udded Why Are We Here?","text":"<p>Until now, we treated LLMs as text-only geniuses. But the real world isn\u2019t just text. What if our LLMs could also see images, generate pictures, and understand screenshots?</p> <p>Welcome to the multimodal revolution.</p>"},{"location":"chapter_13/#what-is-a-multimodal-model","title":"\ud83e\udde0 What Is a Multimodal Model?","text":"<p>A multimodal model can process more than one type of input or output \u2014 like text, images, audio, or video.</p> <p>For this chapter, we focus on:</p> <ul> <li>Image + Text input \u2192 GPT-4V</li> <li>Text \u2192 Image output \u2192 DALL\u00b7E 3</li> </ul>"},{"location":"chapter_13/#key-models-and-tools","title":"\ud83d\udd0d Key Models and Tools","text":"Tool / Model What It Does GPT-4V Reads and understands images (vision + language) DALL\u00b7E 3 Generates high-quality images from text CLIP Links image and text embeddings (used for image search) BLIP Generates captions and answers questions from images VLM General term for Vision-Language Models"},{"location":"chapter_13/#gpt-4v-seeing-and-understanding-images","title":"\ud83d\uddbc\ufe0f GPT-4V: Seeing and Understanding Images","text":"<p>Example prompts:</p> <ul> <li>\u201cWhat\u2019s shown in this chart?\u201d  </li> <li>\u201cTranslate this handwritten note\u201d  </li> <li>\u201cWhy is my code build failing in this screenshot?\u201d</li> </ul> <p>Use cases:</p> <ul> <li>UI/UX audits</li> <li>Code debugging</li> <li>Accessibility checks</li> <li>Homework help</li> <li>Data visualization interpretation</li> </ul>"},{"location":"chapter_13/#dalle-3-generating-images-from-text","title":"\ud83c\udfa8 DALL\u00b7E 3: Generating Images from Text","text":"<p>Example prompts:</p> <ul> <li>\u201cA cartoon robot building a web app using a laptop\u201d  </li> <li>\u201cAn architecture diagram of an AI assistant system\u201d  </li> <li>\u201cEdit this image to remove the watermark\u201d</li> </ul> <p>Use cases:</p> <ul> <li>Product mockups</li> <li>Slide deck visuals</li> <li>Quick idea sketching</li> <li>Inpainting (semantic edits)</li> </ul>"},{"location":"chapter_13/#try-it-yourself-vision-api-example","title":"\ud83e\uddea Try It Yourself (Vision API Example)","text":"<pre><code>response = client.chat.completions.create(\n  model=\"gpt-4-vision-preview\",\n  messages=[\n    {\"role\": \"user\", \"content\": [\n      {\"type\": \"text\", \"text\": \"Describe this image\"},\n      {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://...\"}}\n    ]}\n  ],\n  max_tokens=300\n)\n</code></pre>"},{"location":"chapter_13/#where-this-matters","title":"\ud83e\uddf0 Where This Matters","text":"Domain Use Case QA Testing Screenshot analysis EdTech Homework/photo understanding Healthcare Reading prescriptions Retail Shelf detection, visual audits Docs &amp; Support Visual bug reports"},{"location":"chapter_13/#how-it-works-intuition-only","title":"\ud83e\udde0 How It Works (Intuition Only)","text":"<ol> <li>Images are split into small patches (like tokens)  </li> <li>A vision transformer understands them  </li> <li>That \u201cvisual embedding\u201d is fed into a language model  </li> <li>The LLM continues generating text \u2014 informed by the image</li> </ol>"},{"location":"chapter_13/#reflection-questions","title":"\u2753Reflection Questions","text":"<ol> <li>What can GPT-4V do that would help your QA, testing, or support workflows?  </li> <li>Where would visual generation save you time or design effort?</li> </ol>"},{"location":"chapter_13/#mini-quiz","title":"\ud83e\uddea Mini Quiz","text":"<p>Q1. Which of these models can take an image as input? \u2705 a) GPT-4V</p> <p>Q2. Which model turns text into an image? \u2705 b) DALL\u00b7E 3</p>"},{"location":"chapter_13/#microproject","title":"\ud83e\uddea Microproject","text":"<p>\ud83c\udfaf Build a Screenshot QA Bot  </p> <ul> <li>Upload a UI screenshot  </li> <li>Let GPT-4V:</li> <li>Describe layout  </li> <li>Flag accessibility issues  </li> <li>Recommend improvements</li> </ul> <p>Bonus: Add a DALL\u00b7E-generated architecture diagram.</p>"},{"location":"chapter_13/#recap","title":"\u2705 Recap","text":"<p>\u2705 What multimodal models are \u2705 What GPT-4V and DALL\u00b7E can do \u2705 Use cases across domains \u2705 How to start playing with vision + language</p>"},{"location":"chapter_14/","title":"What\u2019s Next?","text":"<p>Your Beginner AI Engineering Journey Ends Here \u2014 But You're Just Getting Started</p>"},{"location":"chapter_14/#youve-built-a-real-foundation","title":"\ud83c\udfc1 You\u2019ve Built a Real Foundation","text":"<p>By now, you\u2019ve gone from:</p> <ul> <li>Rule-based decisions \u2192 Model-based reasoning  </li> <li>Single-turn prompts \u2192 Tool-using agents  </li> <li>Text-only LLMs \u2192 Multimodal intelligence</li> </ul> <p>You\u2019ve shipped:</p> <ul> <li>Classifiers  </li> <li>APIs  </li> <li>RAG systems  </li> <li>Agents  </li> <li>Vision-enhanced prototypes</li> </ul> <p>You're not just playing with AI. You\u2019re engineering it.</p>"},{"location":"chapter_14/#where-youre-going-next-intermediate-track-preview","title":"\ud83d\ude80 Where You\u2019re Going Next (Intermediate Track Preview)","text":""},{"location":"chapter_14/#embeddings-and-vector-search","title":"\ud83e\udde0 Embeddings and Vector Search","text":"<ul> <li>How semantic search works</li> <li>Chunking, indexing, filtering, and reranking</li> </ul>"},{"location":"chapter_14/#rag-architecture-at-scale","title":"\ud83d\udee0\ufe0f RAG Architecture at Scale","text":"<ul> <li>Structured documents</li> <li>Metadata filtering</li> <li>Hybrid search strategies</li> </ul>"},{"location":"chapter_14/#prompt-composition-dynamic-routing","title":"\ud83e\udde0 Prompt Composition + Dynamic Routing","text":"<ul> <li>Few-shot, zero-shot, train-of-thought</li> <li>Switching between prompts and tools</li> </ul>"},{"location":"chapter_14/#observability-logging-tracing","title":"\u2699\ufe0f Observability, Logging, Tracing","text":"<ul> <li>Monitoring AI behavior like APIs</li> <li>Tools like LangSmith, Traceloop, Phoenix</li> </ul>"},{"location":"chapter_14/#advanced-agents-memory","title":"\ud83d\udc65 Advanced Agents + Memory","text":"<ul> <li>Stateful assistants</li> <li>Task planning and retries</li> <li>Auto mode vs. human-in-the-loop</li> </ul>"},{"location":"chapter_14/#reflection-questions-before-you-continue","title":"\ud83c\udfaf Reflection Questions Before You Continue","text":"<ol> <li>What kind of AI systems do you want to build?</li> <li>What were your biggest mindset shifts through this beginner track?</li> <li>What part of the stack do you want to specialize in next?</li> </ol>"},{"location":"chapter_14/#onward","title":"\ud83c\udf89 Onward","text":"<p>You're no longer an \u201cAI dabbler.\u201d You\u2019re a confident, production-minded AI engineer.</p> <p>Welcome to the intermediate track. Let\u2019s get to work.</p>"},{"location":"chapter_2/","title":"Plug In a Model","text":""},{"location":"chapter_2/#feel-the-power-of-ai-before-you-build-it-one-line-of-code-full-confidence-shift","title":"Feel the power of AI before you build it \u2014 one line of code, full confidence shift","text":""},{"location":"chapter_2/#side-notes-lets-demystify-the-jargon","title":"\ud83c\udf1f Side Notes: Let\u2019s Demystify the Jargon","text":"<p>Before we dive in, here\u2019s a cheat sheet to help decode the terms we\u2019re about to use:</p> <ul> <li>Model: Think of this as a really smart function. You give it input (like a message), and it gives you an output (like \"spam\" or \"not spam\") \u2014 based on patterns it has learned from lots of examples.</li> <li>Classifier: A specific kind of model that puts things into buckets or categories. E.g., Is this spam or not? Is this urgent or normal?</li> <li>Pretrained Model: Someone else already trained it using massive data. You get to use it right away without building it yourself.</li> <li>Transformer: The architecture behind modern language models like ChatGPT. It\u2019s what makes them good at understanding context in language.</li> <li>HuggingFace: A platform that hosts thousands of pre-trained models. Like an app store, but for ML models.</li> <li>Pipeline: A ready-to-use shortcut from HuggingFace that loads a model and runs it on your input. It's like calling <code>useModel()</code> and getting predictions right away.</li> </ul>"},{"location":"chapter_2/#why-are-we-here","title":"\ud83c\udfaf Why Are We Here?","text":"<p>In the last chapter, you saw how rule-based logic falls apart when it meets real-world language. It was messy, brittle, and hard to scale.</p> <p>Now let\u2019s shift from theory to thrill.</p> <p>You're about to run your first real AI model \u2014 in just one line of Python. No math. No configuration. No data science degree.</p> <p>You\u2019ll feel the power of machine learning before we explain how it works.</p> <p>Let\u2019s go. \ud83d\ude80</p>"},{"location":"chapter_2/#language-breaks-your-logic","title":"Language Breaks Your Logic","text":"<p>Say you work in support and need to detect when incoming messages are urgent:</p> <p>\"My server is down. Help NOW!\"</p> <p>Sure, you could write this:</p> <pre><code>if \"urgent\" in msg or \"ASAP\" in msg or \"now\" in msg:\n    return \"High Priority\"\n</code></pre> <p>But what about:</p> <p>\"This is blocking our launch.\"</p> <p>Or:</p> <p>\"Can someone check this before EOD?\"</p> <p>Language is subtle. Hard to hardcode.</p>"},{"location":"chapter_2/#one-line-of-magic-pretrained-classifier","title":"\ud83e\udd16 One Line of Magic: Pretrained Classifier","text":"<p>Let\u2019s use a small but powerful pretrained model from HuggingFace:</p> <pre><code>from transformers import pipeline\n\nclassifier = pipeline(\"text-classification\", model=\"mrm8488/bert-tiny-finetuned-sms-spam-detection\")\n\nmsg = \"This is absolutely urgent. Please respond ASAP!\"\nresult = classifier(msg)\n\nprint(result)\n</code></pre> <p>\ud83c\udf89 Boom \u2014 instant result. The model returns something like:</p> <pre><code>[{'label': 'spam', 'score': 0.987}]\n</code></pre> <p>This model was trained on thousands of SMS messages. It learned to spot patterns in how humans signal urgency (or spamminess).</p> <p>You didn\u2019t teach it rules. You didn\u2019t tune anything. You just gave it text \u2014 and it guessed the meaning.</p>"},{"location":"chapter_2/#what-just-happened","title":"\ud83e\udde0 What Just Happened?","text":"<ul> <li>You loaded a pretrained transformer model</li> <li>It interpreted your message</li> <li>It made a prediction using learned patterns</li> </ul> <p>This is your first taste of pattern-based decision-making.</p> <p>You\u2019re no longer hand-authoring rules \u2014 you\u2019re handing off judgment to a model.</p> <p>And that\u2019s a massive shift.</p>"},{"location":"chapter_2/#dont-let-the-confidence-fool-you","title":"\u26a0\ufe0f Don\u2019t Let the Confidence Fool You","text":"<p>Just because it feels smart doesn\u2019t mean it\u2019s always right.</p> <p>Models can:</p> <ul> <li>Misfire on sarcasm</li> <li>Misclassify subtle language</li> <li>Drift if user language changes over time</li> </ul> <p>So yes \u2014 AI can feel like cheating. But you still have to think like an engineer.</p> <p>Where are the guardrails? How do you monitor quality? When do you fall back to rules?</p> <p>Let\u2019s sketch that next.</p>"},{"location":"chapter_2/#light-production-thinking","title":"\ud83d\udee0\ufe0f Light Production Thinking","text":"<p>Imagine this urgency classifier is now part of your real workflow \u2014 classifying thousands of messages a day.</p> <p>How would you design for safety?</p> <pre><code>Incoming Message \u2192 Classifier \u2192 Priority \u2192 Triage Queue\n        \u2198 Logging      \u2198 Confidence Thresholds\n</code></pre> <p>Ask yourself:</p> <ul> <li>What if it flags too many false positives?</li> <li>How do support agents correct its decisions?</li> <li>What would you log for auditing?</li> <li>How will you know if \u201curgency language\u201d evolves?</li> </ul> <p>Even without knowing the internals \u2014 you\u2019re already designing like a real AI engineer.</p>"},{"location":"chapter_2/#tradeoff-table-rules-vs-pretrained-models","title":"\u2696\ufe0f Tradeoff Table: Rules vs. Pretrained Models","text":"Method Pros Cons Best Used When Rules Simple, fast, transparent Brittle, can\u2019t scale with nuance Few patterns, clear logic Pretrained ML Generalizes, fast to deploy Needs monitoring, less explainable Real-world language, fuzziness Hybrid Best of both worlds Needs more orchestration When trust, speed, and flexibility matter"},{"location":"chapter_2/#real-world-flash-spambusters-inc","title":"\ud83d\udd25 Real-World Flash: SpamBusters Inc.","text":"<p>One startup, SpamBusters Inc., tried to fight spam using logic rules:</p> <pre><code>if \"win money\" in text or \"prize\" in text:\n</code></pre> <p>Worked great \u2014 until it didn\u2019t.</p> <p>Users started writing:</p> <p>\u201cYou may qualify for a reward \ud83e\udd11\u201d</p> <p>Their rules exploded in complexity. Maintenance was a mess.</p> <p>They switched to a simple transformer-based model \u2014 accuracy soared and engineering time dropped.</p> <p>Real ML. Real payoff.</p>"},{"location":"chapter_2/#reflection-corner","title":"\ud83e\udde0 Reflection Corner","text":"<ul> <li>Have you built systems where rules started to break down?</li> <li>Can you imagine where a plug-and-play classifier might help?</li> <li>What\u2019s something you would want to classify today?</li> <li>What would you want a fallback rule to do when the model fails?</li> </ul>"},{"location":"chapter_2/#quick-summary","title":"\ud83c\udfc1 Quick Summary","text":"<ul> <li>You just ran a real AI model \u2014 instantly.</li> <li>It learned from patterns, not rules.</li> <li>You felt the shift from hardcoding to guiding.</li> <li>You started thinking like an AI product builder.</li> </ul> <p>Next up? You\u2019ll build your own model. No more plug-and-play. You\u2019ll choose the data, train the system, and see how models actually learn.</p> <p>Let\u2019s keep going. \ud83d\ude80</p>"},{"location":"chapter_3/","title":"Train Your First Model","text":"<p>How learning models work and draw decision boundaries</p>"},{"location":"chapter_3/#why-this-chapter-exists","title":"\ud83c\udfaf Why This Chapter Exists","text":"<p>In Chapter 2, you used a plug-and-play model to detect urgency \u2014 and it worked like magic.</p> <p>Now it\u2019s time to look under the hood.</p> <p>This chapter is about training your own model from scratch. No AI magic, no pretraining \u2014 just raw data, logic, and a bit of code.</p> <p>By the end, you\u2019ll understand:</p> <ul> <li>What it means for a model to \"learn\"</li> <li>How models draw decision boundaries</li> <li>Why confidence matters when models predict</li> </ul> <p>Let\u2019s build your first real model.</p>"},{"location":"chapter_3/#models-dont-follow-instructions-they-learn-patterns","title":"Models Don\u2019t Follow Instructions \u2014 They Learn Patterns","text":"<p>Let\u2019s get one thing clear:</p> <ul> <li>Rules are exact. You write them. The system follows them.</li> <li>Models are flexible. You don\u2019t tell them what to do \u2014 you show them examples, and they figure out the pattern.</li> </ul> <p>You\u2019ve probably written logic like:</p> <pre><code>if \"ASAP\" in message: return \"urgent\"\n</code></pre> <p>That\u2019s a rule. Precise, manual, brittle.</p> <p>A model sees dozens of urgent and non-urgent messages \u2014 even the vague ones like:</p> <p>\u201cWould be great to get this today.\u201d</p> <p>And it learns to guess what\u2019s urgent \u2014 even when there\u2019s no keyword.</p> <p>This chapter is your first taste of that kind of learning.</p>"},{"location":"chapter_3/#side-notes-new-terms-well-use","title":"\ud83d\udce6 Side Notes: New Terms We\u2019ll Use","text":"<ul> <li>Model: Think of it like a smart function. You give it inputs, it gives you an output \u2014 based on patterns it has learned.</li> <li>Classifier: A model that puts inputs into categories (like \"urgent\" vs. \"not urgent\").</li> <li>Logistic Regression: A very simple classifier that learns to draw a line between categories.</li> <li>Features: The pieces of input data the model uses (e.g., email length, presence of keywords).</li> <li>Decision Boundary: The dividing line (or curve) the model learns to separate categories.</li> <li>Weights: Numbers the model learns to decide how important each feature is. Higher weight = more influence on the outcome.</li> <li>Bias: A base value the model adds before deciding the final score.</li> <li>scikit-learn (sklearn): A Python library that gives you prebuilt ML models like logistic regression, decision trees, etc.</li> <li>NumPy: A numerical library. Think of it as Python\u2019s math-and-arrays toolbox.</li> <li>matplotlib (plt): A plotting library for visualizing data. Not for production \u2014 just to help you see what the model learned.</li> </ul>"},{"location":"chapter_3/#lets-build-an-urgency-classifier","title":"\u270f\ufe0f Let's Build an Urgency Classifier","text":"<p>Let\u2019s walk through a mini-project together \u2014 not to build a production tool, but to get a feel for how models actually learn.</p>"},{"location":"chapter_3/#the-problem","title":"\ud83e\udde9 The Problem","text":"<p>Imagine your support team handles hundreds of messages a day. Some are critical (\u201cThis is blocking production\u201d), others can wait (\u201cCan I reschedule?\u201d).</p> <p>You want to automatically flag messages that feel urgent.</p> <p>You\u2019ve seen this done with a pretrained model (Chapter 2), but now we\u2019ll train our own.</p>"},{"location":"chapter_3/#step-1-simulate-labeled-data","title":"\ud83e\uddea Step 1: Simulate Labeled Data","text":"<p>We\u2019ll simulate incoming support tickets. Each one has two features:</p> <ul> <li>A length score (how long the message is)</li> <li>A binary urgency flag (does it mention \u201cASAP\u201d, \u201cnow\u201d, \u201curgent\u201d?)</li> </ul> <pre><code>from sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_samples=100,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=42\n)\n</code></pre> <p>\ud83d\udee0\ufe0f A quick note on what these arguments mean:</p> <ul> <li><code>n_samples=100</code>: We\u2019re generating 100 fake messages (rows of data).</li> <li><code>n_features=2</code>: Each message has 2 features (e.g., length and urgency flag).</li> <li><code>n_informative=2</code>: Both features are useful for the model to decide.</li> <li><code>n_redundant=0</code>: No extra junk columns \u2014 keep it clean.</li> <li><code>n_clusters_per_class=1</code>: Makes the classification task more predictable.</li> <li><code>random_state=42</code>: Guarantees the same output every time (useful for demos). It\u2019s like setting a random seed \u2014 it makes sure you always get the same random dataset. Why 42? It\u2019s a geeky nod to The Hitchhiker\u2019s Guide to the Galaxy \u2014 the \u201canswer to life, the universe, and everything.\u201d You can use any number, but 42 is the classic default.</li> </ul>"},{"location":"chapter_3/#step-2-train-a-simple-model","title":"\ud83e\uddea Step 2: Train a Simple Model","text":"<p>We\u2019ll use Logistic Regression, one of the simplest ML models. It tries to draw a straight line that best separates class <code>0</code> from class <code>1</code>.</p> <pre><code>from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\n</code></pre> <p>That\u2019s it. You\u2019ve now trained your first classifier.</p>"},{"location":"chapter_3/#step-3-visualize-the-result-optional-but-helpful","title":"\ud83e\uddea Step 3: Visualize the Result (Optional but Helpful)","text":"<p>This isn\u2019t something you\u2019d do in production \u2014 but plotting is a great way to see what the model learned.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', alpha=0.8)\n\ncoef = model.coef_[0]           # learned weights\nintercept = model.intercept_[0] # learned bias\nx_vals = np.array([X[:, 0].min(), X[:, 0].max()])\ny_vals = -(coef[0] * x_vals + intercept) / coef[1]\n\nplt.plot(x_vals, y_vals, label='Decision Boundary')\nplt.title(\"How the model separates urgent vs. not urgent\")\nplt.legend()\nplt.show()\n</code></pre> <p>\ud83d\uddbc\ufe0f Why the graph? Most devs don\u2019t visualize models \u2014 and that\u2019s okay. But here, plotting helps you see how a model thinks. This is a teaching tool \u2014 not a production feature.</p>"},{"location":"chapter_3/#what-about-the-fuzzy-edge","title":"\ud83e\udd14 What About the Fuzzy Edge?","text":"<p>Here\u2019s a question worth pausing on:</p> <p>What happens if a message lands right on the line?</p> <p>The model may not be confident. It might say:</p> <pre><code>predict_proba \u2192 [0.48, 0.52]  # almost a coin flip\n</code></pre> <p>This is where models become uncertain \u2014 when examples are near the decision boundary.</p> <p>We\u2019ll explore this more deeply in the next chapter \u2014 including:</p> <ul> <li>What confidence means</li> <li>How to set thresholds</li> <li>What to do when the model says, \u201cI\u2019m not sure\u201d</li> </ul>"},{"location":"chapter_3/#what-just-happened","title":"\ud83e\udde0 What Just Happened?","text":"<p>You:</p> <ul> <li>Created fake emails (as 2D data points)</li> <li>Trained a model on labeled examples</li> <li>Let it learn weights (how important each feature is)</li> <li>Visualized how it draws a boundary line between \"urgent\" and \"not urgent\"</li> </ul> <p>The model is just a function that scores inputs:</p> <pre><code>score = weight1 * input1 + weight2 * input2 + bias\n</code></pre> <p>If the score is high \u2192 predict urgent. If low \u2192 not urgent.</p> <p>This is pattern learning.</p>"},{"location":"chapter_3/#can-you-trick-the-model","title":"\ud83d\udd0d Can You Trick the Model?","text":"<p>Try running:</p> <pre><code>model.predict([[4.2, 1]])\nmodel.predict_proba([[4.2, 1]])\n</code></pre> <p>What\u2019s happening?</p> <ul> <li>The first line gives you the model\u2019s guess (0 = not urgent, 1 = urgent).</li> <li>The second gives you the confidence \u2014 a probability score like <code>[0.14, 0.86]</code></li> </ul> <p>Now tweak the input:</p> <ul> <li>What happens if you change the length?</li> <li>Or remove the urgency flag?</li> </ul> <p>You\u2019re not just using the model now. You\u2019re probing it \u2014 testing its judgment boundaries.</p>"},{"location":"chapter_3/#best-practices-gotchas","title":"\u26a0\ufe0f Best Practices &amp; Gotchas","text":"<ul> <li>Data matters: Your model is only as good as the examples it sees.</li> <li>Simplicity wins early: Start with simple models. They're easier to debug.</li> <li>Combine with rules when needed: Models aren\u2019t magic. Add fallbacks when the stakes are high.</li> </ul>"},{"location":"chapter_3/#production-sketch","title":"\ud83d\udce6 Production Sketch","text":"<p>Let\u2019s say this model is classifying emails in your system.</p> <p>How would you deploy it safely?</p> <pre><code>Incoming Email \u2192 Extract Features \u2192 Model \u2192 Label\n      \u2198 Log Inputs/Outputs   \u2198 Add Confidence Filter\n</code></pre> <p>Would you:</p> <ul> <li>Let agents override predictions?</li> <li>Use a threshold (e.g., only auto-label if &gt;90% confident)?</li> <li>Retrain weekly as new language comes in?</li> </ul> <p>You\u2019re not just building a model \u2014 you\u2019re architecting a system.</p>"},{"location":"chapter_3/#reflection-corner","title":"\ud83e\udde0 Reflection Corner","text":"<ul> <li>Why does it help to think of a model as a smart scoring function?</li> <li>What part of the learning pipeline surprised you?</li> <li>Where would you use a simple model like this at work?</li> <li>What might break if your input features were poorly chosen?</li> </ul>"},{"location":"chapter_3/#quick-recap","title":"\ud83c\udfc1 Quick Recap","text":"<ul> <li>You trained a working model using scikit-learn.</li> <li>You visualized how it separates classes with a decision boundary.</li> <li>You experimented with inputs to understand model confidence.</li> <li>You started thinking about deploying models, not just building them.</li> </ul> <p>Up next? We\u2019ll zoom in on that idea of confidence \u2014 and how to build systems that act only when sure. Because good AI systems don\u2019t just guess \u2014 they know when not to guess.</p> <p>Let\u2019s go. \ud83d\ude80</p>"},{"location":"chapter_4/","title":"When to Trust the Model","text":""},{"location":"chapter_4/#how-smart-systems-know-their-limits-and-act-accordingly","title":"How smart systems know their limits \u2014 and act accordingly","text":""},{"location":"chapter_4/#why-this-chapter-exists","title":"\ud83c\udfaf Why This Chapter Exists","text":"<p>In Chapter 3, you trained a model and saw how it drew a decision boundary \u2014 a line between \u201cyes\u201d and \u201cno.\u201d</p> <p>But not all decisions are clear.</p> <p>Some inputs land close to that boundary.</p> <p>What happens when the model isn\u2019t confident?</p> <p>This chapter is about designing systems that can say:</p> <p>\u201cI think it\u2019s spam\u2026 but I\u2019m only 60% sure.\u201d</p> <p>And then act accordingly.</p>"},{"location":"chapter_4/#what-is-model-confidence","title":"\ud83e\udde0 What Is Model Confidence?","text":"<p>When you call <code>.predict_proba()</code> on a model like logistic regression or Naive Bayes, it gives you a probability.</p> <pre><code>model.predict_proba([[4.2, 1]])\n# Output: [0.13, 0.87] \u2192 87% confidence in class 1\n</code></pre> <p>That number isn\u2019t magic. It\u2019s just how far the point is from the decision boundary.</p> <ul> <li>Far from the line = confident</li> <li>Close to the line = uncertain</li> </ul> <p>This is critical for real systems.</p>"},{"location":"chapter_4/#visualizing-confidence-zones","title":"\ud83d\udcc9 Visualizing Confidence Zones","text":"<p>Imagine your decision boundary as a line:</p> <pre><code>\u2190 0% confident \u2014\u2014\u2014|\u2014\u2014\u2014 100% confident \u2192\n             boundary\n</code></pre> <p>Everything near the middle (45\u201355%) is dangerous territory. That\u2019s where misclassifications are most likely.</p>"},{"location":"chapter_4/#what-should-a-smart-system-do","title":"\ud83d\udca1 What Should a Smart System Do?","text":"<p>\ud83e\udd14 If the model is 51% confident, should it act like it\u2019s 100% sure?</p> <p>Of course not.</p> <p>Here\u2019s a better plan:</p> <pre><code>if confidence &gt; 0.85:\n    auto_accept()\nelif confidence &lt; 0.6:\n    flag_for_review()\nelse:\n    fall_back_to_rules()\n</code></pre> <p>You\u2019re designing not just a model \u2014 but a system that knows its own limits.</p>"},{"location":"chapter_4/#threshold-tuning-a-developers-superpower","title":"\ud83d\udd04 Threshold Tuning \u2014 A Developer\u2019s Superpower","text":"<p>Most models default to a 50% cutoff:</p> <pre><code>if prob &gt; 0.5: predict class 1\n</code></pre> <p>But you can shift that line to make the system more conservative or aggressive.</p>"},{"location":"chapter_4/#example","title":"Example:","text":"<pre><code>y_scores = model.predict_proba(X_test)[:, 1]  # probability for class 1\n\ny_custom = (y_scores &gt; 0.7).astype(int)  # only predict 1 if very confident\n</code></pre> <p>Tuning this threshold changes everything:</p> <ul> <li>Higher threshold \u2192 fewer false positives, but more misses</li> <li>Lower threshold \u2192 more flags, but more risk</li> </ul>"},{"location":"chapter_4/#mini-experiment-try-threshold-tuning","title":"\ud83e\uddea Mini-Experiment: Try Threshold Tuning","text":"<p>Train a model using your support ticket data. Then try these:</p> <pre><code>from sklearn.metrics import classification_report\n\nfor t in [0.3, 0.5, 0.7]:\n    y_custom = (y_scores &gt; t).astype(int)\n    print(f\"\\nThreshold = {t}\")\n    print(classification_report(y_test, y_custom))\n</code></pre> <p>You\u2019ll quickly see how your model behaves under different confidence strategies.</p>"},{"location":"chapter_4/#quick-intro-what-is-naive-bayes","title":"\ud83e\udde0 Quick Intro: What Is Naive Bayes?","text":"<p>Before we dive into the next classifier, here's a quick intro:</p> <p>Naive Bayes is a fast and effective algorithm for classifying text. It works by calculating how often certain words appear in each category \u2014 then makes a guess based on probability.</p> <p>It\u2019s called \u201cnaive\u201d because it assumes each word contributes independently to the outcome \u2014 even though that\u2019s not always true.</p> <p>Despite the name, it works surprisingly well for things like spam detection or support ticket classification.</p> <p>\ud83e\udde0 Want to dig into how it works behind the scenes? We\u2019ve got a clear explanation in the Fun ML Toolbox.</p>"},{"location":"chapter_4/#real-world-nlp-classifier-with-confidence","title":"\ud83c\udfae Real-World NLP Classifier with Confidence","text":"<p>Let\u2019s apply this to a real NLP example using Naive Bayes.</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Sample training data\ntickets = [\"help ASAP!\", \"urgent issue\", \"payment failed\", \"slow loading\"]\nlabels = [\"urgent\", \"urgent\", \"non-urgent\", \"non-urgent\"]\n\nvectorizer = CountVectorizer()\nX_train = vectorizer.fit_transform(tickets)\n\nmodel = MultinomialNB().fit(X_train, labels)\n\n# Predict with confidence\nnew_ticket = vectorizer.transform([\"can't login, help now\"])\nprediction = model.predict(new_ticket)\nprediction_proba = model.predict_proba(new_ticket)\n\nprint(\"Prediction:\", prediction)\nprint(\"Confidence scores:\", prediction_proba)\n</code></pre> <p>\ud83c\udf89 You\u2019ve now used <code>.predict_proba()</code> on real text. This is how modern AI systems decide when to be bold \u2014 and when to back off.</p>"},{"location":"chapter_4/#confidence-in-production","title":"\u2699\ufe0f Confidence in Production","text":"<p>Here\u2019s how real teams use confidence scores:</p> Confidence Action &gt; 0.9 Auto-process, log result 0.6\u20130.9 Flag for human review &lt; 0.6 Don\u2019t act \u2014 trigger fallback rule <p>This is how mature AI systems operate \u2014 not just \u201cguess and go,\u201d but \u201cguess, evaluate, then act.\u201d</p>"},{"location":"chapter_4/#real-world-case-smartassist-co","title":"\ud83d\udd25 Real-World Case: SmartAssist Co.","text":"<p>SmartAssist had an urgency classifier.</p> <ul> <li>If the model was 95%+ confident, it auto-routed tickets</li> <li>If below 60%, it flagged the ticket and asked a support agent</li> </ul> <p>That simple confidence logic reduced routing errors by 70%.</p>"},{"location":"chapter_4/#optional-deep-dive-how-does-it-know-that","title":"\ud83d\udcda Optional Deep Dive: \u201cHow Does It Know That?\u201d","text":"<p>Feeling curious?</p> <p>Ever wondered:</p> <ul> <li>How does <code>.predict_proba()</code> calculate that number?</li> <li>Why does logistic regression give smooth scores?</li> <li>How does Naive Bayes combine probabilities?</li> </ul> <p>Then take a detour to the next chapter:</p>"},{"location":"chapter_4/#the-fun-ml-toolbox","title":"\ud83d\udc49 \ud83d\udee0\ufe0f The Fun ML Toolbox","text":"<p>Get clear, visual explanations of vectorization, logistic regression, Naive Bayes, and more.</p> <p>Not essential \u2014 but very helpful if you want to peek under the hood.</p>"},{"location":"chapter_4/#reflection-corner","title":"\ud83e\udde0 Reflection Corner","text":"<ul> <li>When should a model\u2019s output be trusted without question?</li> <li>Where in your own product could confidence thresholds improve performance?</li> <li>Can you recall a time a \u201cmaybe\u201d prediction should\u2019ve been stopped or escalated?</li> </ul>"},{"location":"chapter_4/#quick-summary","title":"\ud83d\udccc Quick Summary","text":"<ul> <li><code>.predict_proba()</code> shows how confident the model is</li> <li>Confidence \u2260 correctness, but it helps design safer systems</li> <li>Use thresholds and fallbacks to handle low-certainty predictions</li> <li>Great AI systems don\u2019t just predict \u2014 they know when they\u2019re unsure</li> </ul> <p>Up next: If you're curious how models calculate confidence, jump into the Fun ML Toolbox. Otherwise, we\u2019ll move on to evaluating how well your model is performing \u2014 beyond just being \u201caccurate.\u201d</p>"},{"location":"chapter_5/","title":"Evaluating &amp; Debugging AI Models\u2014Beyond Accuracy","text":""},{"location":"chapter_5/#why-are-we-here","title":"\ud83c\udfaf Why Are We Here?","text":"<p>Ever wonder how Netflix knows exactly what shows you'll binge or how Instagram keeps you scrolling endlessly? These apps don't just work\u2014they work brilliantly, thanks to meticulous evaluation and debugging of their AI models. In this chapter, you'll master how to evaluate and debug your AI with a fun, practical approach, ensuring your apps stay smart, efficient, and user-friendly!</p>"},{"location":"chapter_5/#the-accuracy-trap","title":"\u26a0\ufe0f The Accuracy Trap","text":"<p>Imagine you've built an AI to identify urgent customer-support tickets. Your model proudly announces: </p> <p>\u201cI'm 95% accurate!\u201d</p> <p>Sounds fantastic! But wait:</p> <ul> <li>Only 5% of your tickets are genuinely urgent.</li> <li>Your AI simply predicts \"non-urgent\" every time\u2014and still hits 95% accuracy!</li> </ul> <p>That's not AI; that's cheating.</p> <p>High accuracy \u2260 useful AI.</p>"},{"location":"chapter_5/#understanding-evaluationthe-fun-way","title":"\ud83d\udcc8 Understanding Evaluation\u2014The Fun Way!","text":""},{"location":"chapter_5/#precision-and-recall-explained-with-pizza","title":"\ud83c\udf55 Precision and Recall Explained (With Pizza!)","text":"<ul> <li>Precision: If your AI shouts \"Pizza's here!\", how often does the pizza actually arrive?</li> <li>Recall: If pizza really does arrive, did your AI call it correctly?</li> </ul> <p>Precision ensures you're not crying wolf, and recall ensures you're not missing any delicious pizzas.</p>"},{"location":"chapter_5/#practical-hands-on-metrics-thresholds","title":"\ud83d\udee0 Practical Hands-On: Metrics &amp; Thresholds","text":"<p>Let's explore how thresholds impact your model practically:</p> <pre><code>from sklearn.metrics import classification_report\n\n# Model predictions and probabilities\ny_scores = model.predict_proba(X_test)[:, 1]  # Probability for 'urgent'\n\n# Adjust thresholds to tune sensitivity\nfor threshold in [0.3, 0.5, 0.7]:\n    print(f\"Threshold: {threshold}\")\n    predictions = (y_scores &gt; threshold).astype(int)\n    print(classification_report(y_test, predictions))\n</code></pre> <p>Different thresholds change your model's behavior:</p> <ul> <li>Lower threshold (0.3): More urgent tickets flagged (high recall, lower precision).</li> <li>Higher threshold (0.7): Fewer urgent tickets flagged (high precision, lower recall).</li> </ul>"},{"location":"chapter_5/#debugging-your-ai-detective-moment","title":"\ud83d\udc1e Debugging: Your AI Detective Moment","text":"<p>Let's turn debugging into exciting detective work:</p>"},{"location":"chapter_5/#step-1-spot-the-misclassifications","title":"\ud83d\udd0d Step 1: Spot the Misclassifications","text":"<pre><code>actual_orders = [\"pizza\", \"salad\", \"pizza\", \"salad\"]\npredicted_orders = [\"pizza\", \"salad\", \"salad\", \"salad\"]\n\nfor idx, (actual, predicted) in enumerate(zip(actual_orders, predicted_orders), 1):\n    if actual != predicted:\n        print(f\"Order #{idx} misclassified! (Predicted: {predicted}, Actual: {actual})\")\n</code></pre>"},{"location":"chapter_5/#step-2-find-out-why","title":"\ud83e\uddd0 Step 2: Find Out Why","text":"<ul> <li>Check if the AI confuses \"salad\" toppings with pizza toppings.</li> <li>Spot common mistakes like unclear examples or poor labeling.</li> </ul>"},{"location":"chapter_5/#step-3-fix-it","title":"\ud83d\udee0 Step 3: Fix It!","text":"<ul> <li>Improve your data: clearer, better-labeled examples.</li> <li>Adjust thresholds: make the model more or less sensitive.</li> <li>Blend methods: combine AI predictions with simpler rules.</li> </ul>"},{"location":"chapter_5/#mini-experiment-pizza-debugging-adventure","title":"\ud83c\udfae Mini-Experiment: Pizza Debugging Adventure","text":"<p>Put your detective skills to practical use:</p> <pre><code>food_orders = [\"cheese pizza\", \"veggie pizza\", \"green salad\"]\norder_labels = [\"pizza\", \"pizza\", \"salad\"]\n\nX_test = vectorizer.transform(food_orders)\npredictions = model.predict(X_test)\n\nprecision = precision_score(order_labels, predictions, pos_label=\"pizza\")\nrecall = recall_score(order_labels, predictions, pos_label=\"pizza\")\n\nprint(\"Pizza Precision:\", precision)\nprint(\"Pizza Recall:\", recall)\n\n# Identify issues clearly\nfor food, actual, predicted in zip(food_orders, order_labels, predictions):\n    if actual != predicted:\n        print(f\"Misclassified: '{food}' (Predicted: {predicted}, Actual: {actual})\")\n</code></pre> <p>\ud83c\udf89 Congratulations, you've just debugged your AI pizza detector!</p>"},{"location":"chapter_5/#reflection-corner","title":"\ud83e\uddd0 Reflection Corner","text":"<ul> <li>When is high accuracy misleading?</li> <li>How do precision and recall practically impact user experience?</li> <li>How would you clearly explain precision and recall to your team or PM?</li> </ul>"},{"location":"chapter_5/#sneak-peek-ahead","title":"\ud83c\udf1f Sneak Peek Ahead","text":"<p>Next chapter, you'll dive into the latest and greatest: Large Language Models (LLMs), Vision models, and Generative AI. Get ready to supercharge your applications!</p>"},{"location":"chapter_5/#quick-summary","title":"\ud83d\udccc Quick Summary","text":"<ul> <li>Accuracy isn't everything: Evaluate using precision, recall, and thresholds.</li> <li>Debugging AI is detective work: Find errors, understand causes, and fix them.</li> <li>Evaluating and debugging effectively are essential skills for every practical AI engineer.</li> </ul> <p>Now you've got evaluation and debugging down\u2014let's move into cutting-edge AI next! \ud83d\ude80</p>"},{"location":"chapter_6/","title":"Beyond Yes/No \u2014 Handling Multi-Class Predictions","text":""},{"location":"chapter_6/#why-are-we-here","title":"\ud83c\udf1f Why Are We Here?","text":"<p>In the real world, most problems are not just about saying \"yes\" or \"no.\" Imagine building a support system. It's not enough to say whether a message is urgent. You also need to route it: Should it go to the billing team? Tech support? Product feedback?</p> <p>This chapter is about building models that make real decisions, not just binary ones. These are the kinds of systems you actually deploy.</p>"},{"location":"chapter_6/#goal","title":"\ud83c\udfaf Goal","text":"<p>To understand how models handle more than two outcomes, what changes in evaluation and debugging, and how to build and test a multi-class classifier \u2014 just like you\u2019d ship in a real system.</p>"},{"location":"chapter_6/#real-world-motivation","title":"\ud83e\udde0 Real-World Motivation","text":"<p>So far, you\u2019ve built classifiers to answer yes/no questions:</p> <ul> <li>Is this message urgent?</li> <li>Is it spam?</li> <li>Is the feedback positive?</li> </ul> <p>But in actual product workflows, you need your AI to decide among multiple options. For example:</p>"},{"location":"chapter_6/#helpdesk-ticket-routing","title":"\ud83c\udfaf Helpdesk Ticket Routing","text":"Message Desired Routing \"App crashed on checkout.\" Technical Support \"Can I get a refund?\" Billing \"Love the new UI!\" Product Feedback \"Are you open on weekends?\" General Inquiry <p>You now need a model that doesn't just say yes or no \u2014 it has to pick the right bucket.</p>"},{"location":"chapter_6/#whats-new-with-multi-class","title":"\ud83e\udde9 What's New with Multi-Class?","text":"<p>You still:</p> <ul> <li>Clean your data</li> <li>Create features</li> <li>Train a model</li> </ul> <p>But now:</p> <ul> <li>Labels are not binary (<code>0</code>/<code>1</code>), they are categories.</li> <li>The model outputs a probability distribution over all possible classes.</li> <li>You evaluate how well it performs for each class individually, not just overall.</li> </ul>"},{"location":"chapter_6/#code-walkthrough-first-multi-class-model","title":"\ud83e\uddea Code Walkthrough: First Multi-Class Model","text":"<p>Let's take a simple dataset of support tickets and build a multi-class classifier.</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n\n# Simple dataset\ndata = [\n    (\"Refund request for last month\", \"Billing\"),\n    (\"App crashes on login\", \"Technical\"),\n    (\"Love the new dashboard design!\", \"Feedback\"),\n    (\"What time does support close?\", \"Other\"),\n    (\"My card was charged twice\", \"Billing\"),\n    (\"Can't reset my password\", \"Technical\"),\n    (\"UI is much smoother now\", \"Feedback\"),\n    (\"Need help logging in\", \"Technical\"),\n]\n\n# Labels and dummy features (just for structure)\ny = [label for _, label in data]\nX = [[1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 1]]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n\nmodel = LogisticRegression(multi_class='multinomial', solver='lbfgs')\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred))\n</code></pre>"},{"location":"chapter_6/#lets-visualize-whats-going-on","title":"\ud83e\udde0 Let\u2019s Visualize What\u2019s Going On","text":"<p>Once you've trained your multi-class model and printed the classification report, it\u2019s time to visualize where your model shines \u2014 and where it fumbles.</p>"},{"location":"chapter_6/#confusion-matrix","title":"\ud83d\udd0d Confusion Matrix","text":"<pre><code>from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred, labels=model.classes_)\nConfusionMatrixDisplay(cm, display_labels=model.classes_).plot(cmap=\"Blues\")\n</code></pre> <p>This confusion matrix shows exactly which classes your model tends to confuse. For example, are \u201cBilling\u201d and \u201cOther\u201d frequently mixed up? This tells you what to fix next.</p>"},{"location":"chapter_6/#visualize-per-class-precision-recall","title":"\ud83d\udcc8 Visualize Per-Class Precision &amp; Recall","text":"<p>Let\u2019s look at how your model performs on each class individually:</p> <pre><code>from sklearn.metrics import classification_report\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nreport = classification_report(y_test, y_pred, output_dict=True)\ndf = pd.DataFrame(report).transpose().drop(['accuracy', 'macro avg', 'weighted avg'])\n\ndf[['precision', 'recall', 'f1-score']].plot(kind='bar', figsize=(10,6))\nplt.title('How well are we doing per class?')\nplt.ylim(0, 1.05)\nplt.xticks(rotation=45)\nplt.grid(axis='y')\nplt.show()\n</code></pre> <p>This helps you see which class is getting the best precision, which one has poor recall, and where your model might be over- or under-sensitive.</p>"},{"location":"chapter_6/#want-to-go-deeper-try-visualizing-feature-space","title":"\ud83d\udd2c Want to Go Deeper? Try Visualizing Feature Space","text":"<p>If you're curious whether your inputs are even separable, try visualizing them in 2D using t-SNE:</p> <pre><code>from sklearn.manifold import TSNE\n\nX_proj = TSNE(n_components=2, random_state=42).fit_transform(X_test)\nplt.scatter(X_proj[:, 0], X_proj[:, 1], c=[model.classes_.tolist().index(label) for label in y_test], cmap='tab10')\nplt.title('t-SNE Projection of Your Feature Space')\nplt.show()\n</code></pre> <p>If \u201cBilling\u201d and \u201cTechnical\u201d are overlapping blobs, your model will struggle too \u2014 it\u2019s not magic.</p>"},{"location":"chapter_6/#why-evaluation-gets-harder","title":"\ud83e\udde0 Why Evaluation Gets Harder","text":"<p>With binary classification, your mistake is simple: right or wrong.</p> <p>In multi-class, mistakes become directional:</p> <ul> <li>Predicting \"Billing\" when it was actually \"Feedback\" is very different than predicting \"Technical\" when it was \"Other.\"</li> </ul>"},{"location":"chapter_6/#real-debugging-tips","title":"\ud83d\udd0e Real Debugging Tips","text":"<ul> <li>Label Overlap: Are your classes too similar? (e.g., \"Technical\" vs. \"Login Issues\")</li> <li>Imbalanced Data: Do you have very few examples for some categories?</li> <li>Generic Language: Is the input too vague? (e.g., \"It doesn\u2019t work\")</li> </ul> <p>Try printing low-confidence predictions:</p> <pre><code>for probs in model.predict_proba(X_test):\n    print(f\"Confidence: {max(probs):.2f}, Prediction: {model.classes_[probs.argmax()]}\")\n</code></pre>"},{"location":"chapter_6/#understanding-class-imbalance","title":"\ud83d\udcca Understanding Class Imbalance","text":"<p>Let\u2019s say 80% of your data is \"Technical.\" Your model may just learn to default to that class to optimize accuracy.</p> <p>Solution:</p> <ul> <li>Use <code>class_weight='balanced'</code></li> <li>Collect more examples for rare classes</li> <li>Downsample dominant classes</li> </ul>"},{"location":"chapter_6/#real-world-analogy-doctor-diagnoses","title":"\ud83c\udf93 Real-World Analogy: Doctor Diagnoses","text":"<p>Imagine an AI diagnosing diseases:</p> <ul> <li>80% of patients are healthy</li> <li>15% have cold</li> <li>5% have something rare</li> </ul> <p>A model that always predicts \"healthy\" might look accurate \u2014 but it\u2019s dangerous.</p> <p>That\u2019s why per-class evaluation and confidence scores matter.</p>"},{"location":"chapter_6/#bonus-top-k-predictions","title":"\ud83e\udde0 Bonus: Top-K Predictions","text":"<p>Sometimes your model isn't confident. In those cases, showing the top 2 guesses can help:</p> <pre><code>import numpy as np\n\nprobs = model.predict_proba(X_test)\nfor p in probs:\n    top2 = np.argsort(p)[-2:][::-1]\n    print(\"Top-2:\", [(model.classes_[i], round(p[i], 2)) for i in top2])\n</code></pre> <p>This is a great setup for using fallback logic or even LLMs in Chapter 7.</p>"},{"location":"chapter_6/#reflection-questions","title":"\ud83d\udca1 Reflection Questions","text":"<ol> <li>Have you built or used systems with multiple possible outcomes?</li> <li>When does it make sense to say \"I\u2019m not sure\"?</li> <li>What\u2019s the cost of predicting the wrong class?</li> <li>How can you improve class separation?</li> <li>Can you think of cases where Top-2 prediction might be safer?</li> </ol>"},{"location":"chapter_6/#hands-on-project-smart-ticket-router","title":"\ud83d\udcbb Hands-On Project: Smart Ticket Router","text":"<p>Build a full pipeline:</p> <ul> <li>Create 30\u201340 sample messages (Billing, Tech, Feedback, Other)</li> <li>Extract simple features (keyword flags, text length)</li> <li>Train a <code>LogisticRegression</code> model</li> <li>Evaluate with confusion matrix + precision/recall</li> <li>Print confidence scores and top-2 guesses</li> <li>If max confidence &lt; 0.5, label as \"uncertain\"</li> </ul> <p>Bonus: Add a basic rule like \"If the message has fewer than 3 words, skip classification\"</p>"},{"location":"chapter_6/#exit-outcome","title":"\ud83c\udf1f Exit Outcome","text":"<p>You now:</p> <ul> <li>Understand how to build and debug multi-class models</li> <li>Can evaluate performance by class</li> <li>Know how to detect ambiguity and confidence issues</li> </ul> <p>Up next: We\u2019ll combine everything you\u2019ve learned into a hybrid system that mixes logic + learning \u2014 and knows when to ask for help.</p> <p>Let\u2019s keep going!</p>"},{"location":"chapter_7/","title":"Hybrid Systems \u2014 When Rules and Learning Work Together","text":""},{"location":"chapter_7/#why-are-we-here","title":"\ud83c\udf09 Why Are We Here?","text":"<p>In the last chapter, we built a model that could route tickets across multiple departments \u2014 from billing to tech support to feedback. It was smart. But not smart enough.</p> <p>Because here\u2019s the thing about real-world AI systems:</p> <p>They don\u2019t just guess \u2014 they decide with caution.</p> <p>What happens when the model isn\u2019t confident? Or when it sees something it\u2019s never encountered before? What about business rules \u2014 like escalating VIP users, or ignoring empty messages?</p> <p>This is where hybrid systems shine.</p> <p>In this chapter, you\u2019ll learn how to combine:</p> <ul> <li>\ud83e\udde0 Machine learning (classifiers, confidence scores)</li> <li>\ud83e\uddfe Hand-written rules (edge cases, exceptions)</li> <li>\ud83e\udd16 LLMs (as a backup brain for ambiguous inputs)</li> </ul> <p>This is how production AI really works: not all-learning, not all-logic \u2014 but a smart, explainable blend.</p>"},{"location":"chapter_7/#goal","title":"\ud83c\udfaf Goal","text":"<p>To design a robust, flexible pipeline that combines classical models, business rules, and fallback strategies \u2014 and knows when to ask for help.</p>"},{"location":"chapter_7/#the-setup-why-multi-class-isnt-enough","title":"\ud83e\udde0 The Setup: Why Multi-Class Isn\u2019t Enough","text":"<p>Remember our ticket classifier? It could choose from <code>Billing</code>, <code>Technical</code>, <code>Feedback</code>, or <code>Other</code>.</p> <p>But what happens when:</p> <ul> <li>The message is: \u201cHello?\u201d</li> <li>The message is blank, or just an emoji</li> <li>A new ticket type appears: \u201cI want to delete my account\u201d (which doesn\u2019t fit any current label)</li> <li>The model is only 41% confident in its guess</li> <li>The message is from a VIP customer</li> </ul> <p>None of these are classification problems. They\u2019re design problems.</p> <p>We\u2019ve now moved from \u201cwhat class?\u201d to \u201cwhat should we do?\u201d</p> <p>That\u2019s the key shift: from prediction \u2192 to action. And it\u2019s why we need a hybrid approach.</p>"},{"location":"chapter_7/#whats-a-hybrid-system","title":"\ud83c\udfd7\ufe0f What\u2019s a Hybrid System?","text":"<p>A hybrid system is like a traffic cop:</p> <ul> <li>Sometimes it lets the model take the wheel.</li> <li>Sometimes it steps in to redirect, override, or pause.</li> </ul> <p>Think of it as a smart AI assistant with a supervisor \u2014 the rules.</p> <p>You might:</p> <ul> <li>Use rules before the model to catch garbage inputs</li> <li>Use the model to make predictions</li> <li>Use rules after to decide what to do with the prediction</li> <li>Use an LLM when the model\u2019s answer isn\u2019t confident or convincing</li> </ul>"},{"location":"chapter_7/#building-the-logic-model-pipeline","title":"\ud83d\udd27 Building the Logic + Model Pipeline","text":"<p>Let\u2019s sketch the core idea:</p> <pre><code>def triage_pipeline(message, user_tags):\n    if not message or len(message.strip()) &lt; 5:\n        return {\"action\": \"ignore\", \"reason\": \"Empty or too short\"}\n\n    if \"vip\" in user_tags:\n        return {\"action\": \"escalate\", \"reason\": \"VIP customer\"}\n\n    prediction = model.predict(message)\n    confidence = max(model.predict_proba([message])[0])\n\n    if confidence &lt; 0.5:\n        return {\"action\": \"defer_to_llm\", \"input\": message, \"confidence\": confidence}\n\n    return {\"action\": \"route\", \"label\": prediction, \"confidence\": confidence}\n</code></pre> <p>You\u2019ve just built:</p> <ul> <li>\u2705 Pre-model logic (message validation, VIP handling)</li> <li>\u2705 Model prediction</li> <li>\u2705 Post-model routing</li> <li>\u2705 Fallback hook for LLMs</li> </ul> <p>This is real-world AI plumbing \u2014 flexible, transparent, and safe.</p>"},{"location":"chapter_7/#when-to-use-an-llm","title":"\ud83e\udd16 When to Use an LLM","text":"<p>LLMs are not magic. But they are useful when things get messy:</p> <ul> <li>The model has low confidence</li> <li>The user says something open-ended</li> <li>You want to explain, rephrase, or reclassify</li> </ul> <pre><code>if confidence &lt; 0.5:\n    prompt = f\"Classify this message: '{message}'\"\n    llm_response = call_llm(prompt)\n    return {\"action\": \"llm_fallback\", \"suggested_label\": llm_response}\n</code></pre> <p>Think of the LLM as a junior analyst \u2014 smart, flexible, and verbose. But not in charge of critical decisions without supervision.</p>"},{"location":"chapter_7/#real-world-analogy-triage-nurse","title":"\ud83d\udd0d Real-World Analogy: Triage Nurse","text":"<p>In hospitals, the triage nurse doesn\u2019t diagnose \u2014 they decide where a patient should go:</p> <ul> <li>Chest pain? \u2192 Cardiologist</li> <li>Broken bone? \u2192 Ortho</li> <li>Unclear symptoms? \u2192 Doctor review</li> </ul> <p>Your pipeline does the same:</p> <ul> <li>Obvious input? \u2192 Model predicts</li> <li>Sensitive case? \u2192 Rule escalates</li> <li>Ambiguous? \u2192 LLM suggests</li> </ul> <p>This analogy helps product teams, PMs, and engineers alike reason about AI behavior.</p>"},{"location":"chapter_7/#build-your-own-hybrid-system","title":"\ud83e\uddea Build Your Own Hybrid System","text":"<p>Take your ticket classifier from Chapter 6 and wrap it in a pipeline:</p> <ol> <li>Pre-check for empty messages</li> <li>Route VIPs straight to escalation</li> <li>Run the model and extract confidence</li> <li>Defer to an LLM if confidence &lt; 0.5</li> <li>Log every decision and why it happened</li> </ol> <p>Bonus: Print a side-by-side comparison of model vs. LLM label.</p>"},{"location":"chapter_7/#reflection-corner","title":"\ud83d\udcac Reflection Corner","text":"<ul> <li>What\u2019s a case in your product where you\u2019d want the model to not decide?</li> <li>Where might you use hard rules to override model behavior?</li> <li>What\u2019s your threshold for \u201clow confidence\u201d? Why?</li> <li>When does it make sense to blend prediction + human judgment?</li> </ul>"},{"location":"chapter_7/#exit-outcome","title":"\u2705 Exit Outcome","text":"<p>You now understand:</p> <ul> <li>Why real AI systems are not just models</li> <li>How to build a safe, explainable ML pipeline</li> <li>When to rely on logic, and when to ask for help</li> </ul> <p>This is the foundation for trustworthy AI in production.</p>"},{"location":"chapter_7/#coming-up","title":"\u23ed\ufe0f Coming Up","text":"<p>In the next chapter, we\u2019ll take a bold step forward into the world of Large Language Models (LLMs) \u2014 the powerful engines behind tools like ChatGPT and GitHub Copilot.</p> <p>You\u2019ve already used LLMs as fallbacks. Now, you\u2019ll learn how to work with them directly:</p> <ul> <li>Crafting smart prompts</li> <li>Automating workflows</li> <li>Using LLMs for classification, summarization, and structured reasoning</li> </ul> <p>This is your entry into the modern AI toolkit \u2014 fast, flexible, and incredibly powerful.</p> <p>Let\u2019s unlock it together. \ud83d\ude80</p>"},{"location":"chapter_8/","title":"Chapter 8: Serving Your Model \u2014 Building an API with FastAPI","text":""},{"location":"chapter_8/#goal","title":"\ud83c\udfaf Goal","text":"<p>To wrap your trained model into a real, callable service using FastAPI \u2014 so other apps, teammates, or products can use it like any other backend system.</p>"},{"location":"chapter_8/#why-this-matters","title":"\ud83e\udd14 Why This Matters","text":"<p>You\u2019ve built a model. You\u2019ve written logic around it. Now it\u2019s time to make it useful \u2014 outside your notebook.</p> <p>In production, ML isn\u2019t just about accuracy. It\u2019s about access \u2014 letting others call your model safely, reliably, and repeatedly.</p> <p>That\u2019s where APIs come in.</p>"},{"location":"chapter_8/#what-youll-learn","title":"\ud83e\uddf0 What You\u2019ll Learn","text":"<ul> <li>Basics of FastAPI (Python framework)</li> <li>How to wrap a model in a REST endpoint</li> <li>Input validation and response design</li> <li>Returning predictions and confidence scores</li> <li>Logging inputs and outputs</li> </ul>"},{"location":"chapter_8/#what-is-fastapi","title":"\ud83e\udde0 What Is FastAPI?","text":"<p>If you\u2019ve used Flask or Django, this will feel familiar.</p> <p>FastAPI is a lightweight Python web framework that makes it fast to build APIs, with:</p> <ul> <li>Built-in validation using type hints</li> <li>Easy JSON handling</li> <li>Auto-generated docs (<code>/docs</code>)</li> </ul> <p>You\u2019ll use it to expose your model to the outside world \u2014 like:</p> <pre><code>POST /predict\n{\n  \"message\": \"HELP! I got charged twice\"\n}\n</code></pre>"},{"location":"chapter_8/#your-first-model-api","title":"\ud83d\ude80 Your First Model API","text":"<p>Let\u2019s wrap the classifier you built in Project 2.</p>"},{"location":"chapter_8/#step-1-install-fastapi","title":"Step 1: Install FastAPI","text":"<pre><code>pip install fastapi uvicorn\n</code></pre>"},{"location":"chapter_8/#step-2-build-the-service-mainpy","title":"Step 2: Build the service (<code>main.py</code>)","text":"<pre><code>from fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n# Load your models and logic here\n# from model_logic import triage_ticket\n\nclass TicketRequest(BaseModel):\n    message: str\n\n@app.post(\"/predict\")\ndef predict_ticket(req: TicketRequest):\n    result = triage_ticket(req.message)\n    return result\n</code></pre> <p><code>triage_ticket()</code> is the logic you built in Chapter 7 \u2014 rule + model + LLM fallback.</p>"},{"location":"chapter_8/#step-3-run-your-api","title":"Step 3: Run your API","text":"<pre><code>uvicorn main:app --reload\n</code></pre> <p>Visit: http://localhost:8000/docs for auto-generated Swagger UI \ud83d\ude80</p>"},{"location":"chapter_8/#sample-output","title":"\ud83d\udce6 Sample Output","text":"<pre><code>{\n  \"urgent\": true,\n  \"category\": \"Billing\",\n  \"confidence\": 0.82,\n  \"action\": \"route\"\n}\n</code></pre>"},{"location":"chapter_8/#add-logging","title":"\ud83d\udee1\ufe0f Add Logging","text":"<p>Wrap your function to track input/output:</p> <pre><code>import logging\n\n@app.post(\"/predict\")\ndef predict_ticket(req: TicketRequest):\n    logging.info(f\"Input: {req.message}\")\n    result = triage_ticket(req.message)\n    logging.info(f\"Output: {result}\")\n    return result\n</code></pre> <p>Optional: log to a file or DB, and log confidence scores separately</p>"},{"location":"chapter_8/#exercise-build-and-test-your-own-service","title":"\ud83e\uddea Exercise: Build and Test Your Own Service","text":"<ol> <li>Load your model and logic</li> <li>Create a FastAPI app</li> <li>Add one <code>/predict</code> endpoint</li> <li>Use a tool like Postman or curl to test it</li> <li>Bonus: add <code>/health</code> and <code>/version</code> endpoints</li> </ol>"},{"location":"chapter_8/#reflection","title":"\ud83d\udcad Reflection","text":"<ul> <li>How does exposing an API change how you think about error handling?</li> <li>What\u2019s the right amount of detail to return from a model?</li> <li>What would you log in production (inputs, outputs, confidence, user ID)?</li> </ul>"},{"location":"chapter_8/#exit-outcome","title":"\u2705 Exit Outcome","text":"<p>You now know how to:</p> <ul> <li>Wrap ML logic into an API</li> <li>Use FastAPI to receive structured input and return predictions</li> <li>Add logging and testability for real-world use</li> </ul> <p>This is the first step to deployable AI \u2014 not just code that works, but code that serves.</p>"},{"location":"chapter_8/#going-further-optional","title":"\ud83d\ude80 Going Further (Optional)","text":"<p>If you\u2019re curious about model tracking, versioning, and collaboration, here are some tools worth exploring after this chapter:</p> Tool What It Does MLflow Track experiments, store models, compare results Weights &amp; Biases Visualize training, track hyperparams &amp; metrics Hugging Face Hub Upload and reuse pretrained models from others BentoML / Seldon Model packaging + deployment frameworks <p>These tools become more useful as your models grow in complexity \u2014 and as your team needs to collaborate and ship responsibly.</p>"},{"location":"chapter_8/#coming-up","title":"\u23ed\ufe0f Coming Up","text":"<p>Now that your model is callable, we\u2019ll explore how things go wrong in production \u2014 and how to build safe, observable AI with logging, alerts, and human-in-the-loop.</p>"},{"location":"chapter_8a/","title":"How LLMs Work \u2014 Context, Embeddings, and Reasoning","text":""},{"location":"chapter_8a/#why-are-we-here","title":"\ud83c\udf09 Why Are We Here?","text":"<p>In the last chapter, you built logic pipelines combining rules, models, and LLMs. But you\u2019ve probably noticed something weird:</p> <ul> <li>Sometimes the LLM gives a brilliant answer.</li> <li>Sometimes it sounds confident but is dead wrong.</li> <li>Sometimes changing one word in your prompt completely changes the outcome.</li> </ul> <p>What\u2019s going on?</p> <p>To build anything reliable with LLMs, you need to understand how they actually \u201cthink.\u201d This chapter is that foundation.</p> <p>You\u2019re not just learning how to prompt. You\u2019re learning how the machine predicts.</p>"},{"location":"chapter_8a/#goal","title":"\ud83c\udfaf Goal","text":"<p>To build a working mental model of how LLMs operate:</p> <ul> <li>What a \"language model\" is \u2014 and what it\u2019s not</li> <li>How inputs are processed via tokens and context windows</li> <li>Why order, structure, and precision in prompts matter</li> <li>What embeddings are and how they underpin many AI systems</li> <li>When and why LLMs produce surprising or incorrect answers</li> </ul>"},{"location":"chapter_8a/#what-is-a-language-model","title":"\ud83e\udde0 What Is a Language Model?","text":"<p>A language model is not a reasoning engine, chatbot, or planning agent.</p> <p>At its core, it is a system trained to do just one thing:</p> <p>Given a sequence of text, predict the most likely next token.</p> <p>That's it.</p> <pre><code>Input: \"My favorite programming language is\"\nModel: [\"Python\" (78%), \"JavaScript\" (10%), \"C++\" (5%), ...]\n</code></pre> <p>Language models are trained on massive text datasets to learn:</p> <ul> <li>Word patterns and probabilities</li> <li>Syntax and semantics</li> <li>How different topics and tones tend to flow</li> </ul> <p>They do not truly understand or reason. They excel at generating plausible continuations of text \u2014 and that\u2019s what makes them powerful (and sometimes misleading).</p>"},{"location":"chapter_8a/#how-does-it-predict-the-next-token","title":"\ud83e\udde0 How Does It Predict the Next Token?","text":"<p>Behind the scenes, most modern LLMs use a neural network architecture called a Transformer.</p> <p>The Transformer has a key mechanism called attention, which allows the model to look at all the tokens in the prompt and weigh their importance differently.</p> <p>Think of it like reading a paragraph with a highlighter \u2014 the model \"pays more attention\" to the parts that matter most when choosing the next word.</p> <p>This allows the model to:</p> <ul> <li>Understand long-range dependencies</li> <li>Make predictions based on relationships between words</li> <li>Scale to very large inputs and outputs</li> </ul>"},{"location":"chapter_8a/#tokens-and-context-windows","title":"\ud83e\udde9 Tokens and Context Windows","text":"<p>LLMs don\u2019t see whole paragraphs \u2014 they see tokens, which are fragments of text:</p> <ul> <li>A word: <code>dog</code></li> <li>A subword: <code>un</code>, <code>believ</code>, <code>able</code></li> <li>Punctuation: <code>.</code> or <code>?</code></li> </ul>"},{"location":"chapter_8a/#whats-a-context-window","title":"\u23f3 What\u2019s a Context Window?","text":"<p>Think of it like the model\u2019s short-term memory. It\u2019s the number of tokens it can \u201csee\u201d at one time.</p> <ul> <li>GPT-3.5: ~4,000 tokens (~3,000 words)</li> <li>GPT-4 (turbo): up to 128,000 tokens (~100,000 words)</li> </ul> <p>If your input + output exceeds this window:</p> <ul> <li>Earlier context is dropped</li> <li>The model starts forgetting what came before</li> </ul> <p>\ud83e\udde0 Analogy: Imagine whispering into a tunnel. The model only hears the last X words you say.</p>"},{"location":"chapter_8a/#tokenization-in-practice","title":"\ud83d\udd20 Tokenization in Practice","text":"<pre><code>from transformers import GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokens = tokenizer.tokenize(\"Language models don't reason.\")\nprint(tokens)\n</code></pre> <p>Output:</p> <pre><code>['Language', '\u0120models', '\u0120don', \"'\", 't', '\u0120reason', '.']\n</code></pre>"},{"location":"chapter_8a/#embeddings-meaning-in-vector-space","title":"\ud83e\udde0 Embeddings: Meaning in Vector Space","text":"<p>Before LLMs predict anything, they convert text into embeddings \u2014 high-dimensional vectors that encode meaning.</p> <p>Embeddings allow the model to:</p> <ul> <li>Measure similarity between texts</li> <li>Cluster related ideas</li> <li>Match prompts to known patterns</li> </ul> <pre><code>from openai import OpenAI\n\nresponse = openai.Embedding.create(input=\"How do I reset my password?\", model=\"text-embedding-ada-002\")\nvector = response['data'][0]['embedding']\n</code></pre>"},{"location":"chapter_8a/#intent-vs-prompt-not-the-same-thing","title":"\ud83e\udde0 Intent vs Prompt \u2014 Not the Same Thing","text":"<p>What you want and what you say are different.</p>"},{"location":"chapter_8a/#example","title":"Example:","text":"<p>\u274c Prompt: \"Tell me about our app.\"</p> <ul> <li>\ud83e\udd16 Output: vague, promotional, possibly off-topic</li> </ul> <p>\u2705 Prompt: \"List 3 features of our app in bullet points. Include only 1 sentence per point.\"</p> <ul> <li>\ud83e\udd16 Output: clear, usable, and structured</li> </ul>"},{"location":"chapter_8a/#system-vs-user-roles","title":"\ud83e\uddf1 System vs User Roles","text":"<p>Chat-based models use structured messages:</p> <pre><code>messages = [\n  {\"role\": \"system\", \"content\": \"You are a precise technical assistant.\"},\n  {\"role\": \"user\", \"content\": \"Summarize this customer complaint.\"}\n]\n</code></pre> <ul> <li>System message: defines tone, identity, rules</li> <li>User message: the actual task</li> </ul>"},{"location":"chapter_8a/#temperature-and-top-p-controlling-output-behavior","title":"\u2699\ufe0f Temperature and Top-p: Controlling Output Behavior","text":""},{"location":"chapter_8a/#temperature","title":"\ud83d\udd25 Temperature","text":"<ul> <li><code>0.0</code>: deterministic</li> <li><code>1.0+</code>: exploratory</li> </ul>"},{"location":"chapter_8a/#top-p","title":"\ud83c\udfb2 Top-p","text":"<ul> <li>Select from the top <code>p%</code> of possible next tokens</li> </ul> <pre><code>temperature=0.2, top_p=0.8  # Reliable for structured tasks\n</code></pre>"},{"location":"chapter_8a/#a-word-on-hallucination-at-the-end-where-it-belongs","title":"\u2757 A Word on Hallucination (At the End, Where It Belongs)","text":"<p>LLMs don\u2019t \u201cknow\u201d facts or have external memory. They guess what sounds right.</p> <p>That\u2019s why:</p> <ul> <li>They invent URLs</li> <li>Misquote legal clauses</li> <li>Fabricate names or facts</li> </ul>"},{"location":"chapter_8a/#how-to-reduce-it","title":"How to reduce it:","text":"<ul> <li>Give more structure</li> <li>Add documents to prompt (RAG)</li> <li>Ask for sources, but verify</li> </ul>"},{"location":"chapter_8a/#final-analogy-a-genius-autocomplete","title":"\ud83e\udde0 Final Analogy: A Genius Autocomplete","text":"<p>Imagine a super-smart autocomplete:</p> <ul> <li>It\u2019s read billions of documents</li> <li>But it only guesses what comes next \u2014 based on what it\u2019s seen</li> <li>It forgets everything the moment it responds</li> </ul> <p>Powerful? Yes. Grounded? Only if you design it that way.</p>"},{"location":"chapter_8a/#exit-outcome","title":"\u2705 Exit Outcome","text":"<p>You now:</p> <ul> <li>Know what a language model really is</li> <li>Understand tokens, context windows, and embeddings</li> <li>Can structure prompts with intention and control</li> <li>Have tools to start building reliable, debuggable LLM systems</li> </ul> <p>In the next chapter, we\u2019ll turn this theory into hands-on prompting \u2014 building classifiers, transformers, and smart assistants. Let\u2019s go.</p>"},{"location":"chapter_8b/","title":"Prompting in Action \u2014 LLMs as Classifiers, Transformers, and Smart Assistants","text":""},{"location":"chapter_8b/#why-are-we-here","title":"\ud83c\udf09 Why Are We Here?","text":"<p>In Chapter 8A, you learned how language models actually work \u2014 token by token, prediction by prediction. You also saw how their behavior is shaped by structure: context windows, embeddings, and prompts.</p> <p>Now it\u2019s time to put that knowledge into action.</p> <p>This chapter is about how to talk to LLMs so they do what you want. Whether it\u2019s classifying a message, rewriting text, or extracting structured data \u2014 the key is great prompting.</p>"},{"location":"chapter_8b/#goal","title":"\ud83c\udfaf Goal","text":"<p>By the end of this chapter, you\u2019ll be able to: - Write prompts that produce consistent, structured results - Use LLMs for classification, summarization, tone-shifting, and extraction - Debug and refine prompt behavior - Build reusable prompt templates that plug into real systems</p>"},{"location":"chapter_8b/#the-big-idea-prompts-are-interfaces","title":"\ud83e\udde0 The Big Idea: Prompts Are Interfaces","text":"<p>Prompts are not one-off strings. They\u2019re contracts: - Between you and the model - Between your system and the response - Between your logic and what downstream code expects</p> <p>A good prompt = consistent output + minimal surprises</p>"},{"location":"chapter_8b/#prompt-recipe-structure-first","title":"\u270d\ufe0f Prompt Recipe: Structure First","text":"<p>When prompting for apps, you want: - Determinism - Parsability (JSON or structured text) - Clarity of task</p> <p>Here\u2019s a best-practice recipe:</p> <pre><code>SYSTEM_PROMPT = \"You are a helpful and structured AI assistant. Always respond in JSON.\"\n\nUSER_PROMPT = \"\"\"\nClassify the following support message:\n- Choose one: Billing, Technical, Feedback, Other\n- Respond in JSON using a single key: category\n\nMessage: \"I was charged twice for my subscription.\"\n\"\"\"\n</code></pre>"},{"location":"chapter_8b/#prompt-use-cases-in-action","title":"\ud83e\uddea Prompt Use Cases in Action","text":""},{"location":"chapter_8b/#1-classification","title":"\ud83d\udfe9 1. Classification","text":"<pre><code>Classify this message as one of:\n- Billing\n- Technical\n- Feedback\n- Other\n\nMessage: \u201cApp keeps crashing when I open it.\u201d\n</code></pre> <p>Output:</p> <pre><code>{ \"category\": \"Technical\" }\n</code></pre>"},{"location":"chapter_8b/#2-summarization","title":"\ud83d\udfe6 2. Summarization","text":"<pre><code>Summarize this in one sentence:\n\u201cI can\u2019t log in after resetting my password. Keeps saying \u2018invalid credentials.\u2019\u201d\n</code></pre> <p>Output:</p> <pre><code>User can\u2019t log in after password reset due to invalid credential errors.\n</code></pre>"},{"location":"chapter_8b/#3-tone-shifting-rewriting","title":"\ud83d\udfe8 3. Tone Shifting / Rewriting","text":"<pre><code>Rewrite the following in a friendly, casual tone:\n\n\u201cWe regret to inform you that we cannot approve your request at this time.\u201d\n</code></pre> <p>Output:</p> <pre><code>Sorry, but we can\u2019t approve that request right now.\n</code></pre>"},{"location":"chapter_8b/#4-structured-extraction-entities-intent-urgency","title":"\ud83d\udfe5 4. Structured Extraction (Entities, Intent, Urgency)","text":"<pre><code>Extract the following fields from this message:\n- intent\n- urgency (low, medium, high)\n- product\n\nMessage: \u201cI was double charged for my Premium plan. Please fix this today!\u201d\n\nRespond in JSON only.\n</code></pre> <p>Output:</p> <pre><code>{\n  \"intent\": \"billing_issue\",\n  \"urgency\": \"high\",\n  \"product\": \"Premium plan\"\n}\n</code></pre>"},{"location":"chapter_8b/#prompt-template-reusable-component","title":"\ud83e\udde9 Prompt Template = Reusable Component","text":"<p>Rather than hardcoding prompts everywhere, define templates with: - Parameters (<code>{message}</code>) - Examples (few-shot) - Format enforcement</p> <pre><code>def classify_message(message):\n    return f\"\"\"\nClassify the message: '{message}'\n\nChoose from:\n- Billing\n- Technical\n- Feedback\n- Other\n\nRespond in JSON.\n\"\"\"\n</code></pre> <p>Use the same template in tests, logs, and services.</p>"},{"location":"chapter_8b/#debugging-prompt-failures","title":"\ud83d\udc1b Debugging Prompt Failures","text":""},{"location":"chapter_8b/#common-problems","title":"Common Problems:","text":"<ul> <li>Output format changes randomly</li> <li>LLM \u201cexplains\u201d instead of responding in JSON</li> <li>Hallucinated fields or incorrect keys</li> </ul>"},{"location":"chapter_8b/#fixes","title":"Fixes:","text":"<ul> <li>Use system prompts for behavioral control</li> <li>Set temperature = 0.2 for deterministic outputs</li> <li>Use \u201cONLY respond in JSON\u201d language (repeat if needed)</li> <li>Add output examples as few-shot anchor</li> </ul>"},{"location":"chapter_8b/#mini-project-smart-llm-triage-assistant","title":"\ud83c\udfae Mini Project: Smart LLM Triage Assistant","text":"<p>Build a function that: 1. Classifies a message into one of 4 categories 2. Extracts urgency 3. Returns JSON with:    - <code>category</code>    - <code>urgency</code>    - <code>needs_review</code> flag if confidence is low    - <code>explanation</code> field (optional chain-of-thought)</p> <p>Bonus: - Let the user query: \u201cWhy did you classify it that way?\u201d</p>"},{"location":"chapter_8b/#reflection-corner","title":"\ud83d\udcac Reflection Corner","text":"<ul> <li>What prompt structures worked best for you?</li> <li>When would you use an LLM instead of a trained classifier?</li> <li>How might you productionize these prompts as a service?</li> </ul>"},{"location":"chapter_8b/#exit-outcome","title":"\u2705 Exit Outcome","text":"<p>You now: - Can craft practical, structured prompts for real use cases - Know how to turn LLMs into useful utilities - Understand how to debug and standardize output - Have patterns you can reuse across your apps</p> <p>In the next chapter, we\u2019ll explore advanced prompting strategies like few-shot, chain-of-thought, and intent detection. Let\u2019s level up!</p>"},{"location":"chapter_8c/","title":"\ud83d\ude80 Prompting Strategies \u2014 Few-Shot, Chain-of-Thought, and Intent Extraction","text":""},{"location":"chapter_8c/#why-are-we-here","title":"\ud83c\udf09 Why Are We Here?","text":"<p>In Chapter 8B, you built LLM-based tools using well-structured prompts. But what happens when: - Your prompt works inconsistently? - The task requires examples or reasoning? - You want more control over how the model reasons?</p> <p>This chapter introduces advanced prompting strategies used in real-world systems: - Zero-shot vs. few-shot prompting - Chain-of-thought and train-of-thought prompting - Intent and entity extraction - Prompt routing and multi-stage chaining</p>"},{"location":"chapter_8c/#goal","title":"\ud83c\udfaf Goal","text":"<p>You will: - Learn when and why to use few-shot and chain-of-thought - Extract structured data like intent and urgency - Design and debug modular prompt pipelines - Build LLM systems that handle complex reasoning</p>"},{"location":"chapter_8c/#zero-shot-prompting-quick-review","title":"\ud83e\udde0 Zero-Shot Prompting (Quick Review)","text":"<p>Most prompts so far have been zero-shot:</p> <p>Give instructions, expect the LLM to figure it out</p> <pre><code>Classify this message: \"I was charged twice.\"\n</code></pre> <p>Zero-shot is fast but: - Brittle on vague or ambiguous input - Often inconsistent across edge cases</p>"},{"location":"chapter_8c/#few-shot-prompting-show-dont-just-tell","title":"\u2728 Few-Shot Prompting: Show, Don\u2019t Just Tell","text":"<p>Few-shot prompting includes examples to guide behavior:</p> <pre><code>Classify each message into: Billing, Technical, Feedback, Other\n\nExample 1: \"I was charged twice\" \u2192 Billing  \nExample 2: \"App crashes at login\" \u2192 Technical  \nExample 3: \"Love the new UI\" \u2192 Feedback\n\nMessage: \"Why am I still being billed after cancelling?\"\n</code></pre> <p>Few-shot helps: - Reduce hallucinations - Improve consistency - Anchor the format you want returned</p>"},{"location":"chapter_8c/#chain-of-thought-prompting","title":"\ud83e\udde0 Chain-of-Thought Prompting","text":"<p>Sometimes, models guess too quickly. Chain-of-thought encourages deliberate reasoning:</p> <pre><code>Q: I gave away 2 apples from a basket of 5. How many are left?  \nA: Let's think step by step.\n</code></pre> <p>Output:</p> <pre><code>- Started with 5 apples  \n- Gave away 2  \n- Remaining: 3\n</code></pre> <p>Useful for: - Math - Multi-hop logic - Debuggable output</p>"},{"location":"chapter_8c/#train-of-thought-prompting","title":"\ud83e\udde0 Train-of-Thought Prompting","text":"<p>Similar to CoT, but: - Explores multiple perspectives - Ideal for ideation, ambiguity, \u201cwhat-if\u201d scenarios</p> <pre><code>What are 3 different ways to interpret this customer message:  \n\"I\u2019m not happy with my recent experience.\"  \n</code></pre>"},{"location":"chapter_8c/#intent-extraction-your-llms-first-real-skill","title":"\ud83e\udde9 Intent Extraction \u2014 Your LLM\u2019s First Real Skill","text":"<p>Intent = the user\u2019s goal Entities = key fields, names, amounts, plans, etc.</p>"},{"location":"chapter_8c/#example-prompt","title":"\ud83d\udd01 Example Prompt:","text":"<pre><code>Extract the following from this message:\n- intent\n- urgency (low, medium, high)\n- product\n\nMessage: \u201cI was double charged for my Premium plan. Please fix this today!\u201d\n\nRespond in JSON.\n</code></pre>"},{"location":"chapter_8c/#output","title":"\u2705 Output:","text":"<pre><code>{\n  \"intent\": \"billing_issue\",\n  \"urgency\": \"high\",\n  \"product\": \"Premium plan\"\n}\n</code></pre>"},{"location":"chapter_8c/#code-snippet","title":"\ud83e\uddea Code Snippet","text":"<pre><code>def extract_intent(message):\n    return f\"\"\"\nExtract intent, urgency, and product from:\n\"{message}\"\n\nRespond in JSON with:\n- intent\n- urgency (low | medium | high)\n- product\n\"\"\"\n</code></pre>"},{"location":"chapter_8c/#prompt-composition-patterns","title":"\ud83e\udde0 Prompt Composition Patterns","text":"<p>Real systems often: - Combine prompts in stages - Route different queries to different templates - Chain LLM calls (e.g., classification \u2192 summarization)</p>"},{"location":"chapter_8c/#pattern-examples","title":"Pattern Examples:","text":"<ul> <li>Stage 1: Classify \u2192 Stage 2: Explain</li> <li>Stage 1: Extract \u2192 Stage 2: Ask follow-up</li> <li>Stage 1: Rewrite \u2192 Stage 2: Translate</li> </ul> <p>Use frameworks like: - LangChain - Guidance - PromptLayer</p>"},{"location":"chapter_8c/#prompt-debugging-cheatsheet","title":"\ud83e\uddf0 Prompt Debugging Cheatsheet","text":"Problem Solution Output isn\u2019t consistent Add few-shot examples Output isn\u2019t valid JSON Repeat format instructions twice Output too verbose Use \u201cRespond only with...\u201d LLM flips between labels Lower temperature, anchor few-shot"},{"location":"chapter_8c/#prompt-comparison-grid","title":"\ud83c\udfae Prompt Comparison Grid","text":"<p>Try running these variants:</p> Input Strategy Prompt Output \u201cI need help logging in\u201d Zero-shot Classify this message ? Few-shot With labeled examples Technical \u201cApp is too slow at night\u201d CoT Think step by step ? <p>Compare outputs side-by-side. This is how LLM engineers tune behavior.</p>"},{"location":"chapter_8c/#reflection","title":"\ud83d\udcad Reflection","text":"<ul> <li>Where in your product could intent classification help?</li> <li>Which pattern felt most reliable for debugging?</li> <li>Would you build one mega-prompt or break into modules?</li> </ul>"},{"location":"chapter_8c/#exit-outcome","title":"\u2705 Exit Outcome","text":"<p>You now: - Understand and apply few-shot and chain-of-thought prompting - Can extract structured fields like intent and urgency - Are ready to chain LLM prompts and route them modularly</p> <p>Next: Wrap your logic into a scalable, production-ready FastAPI service.</p>"},{"location":"chapter_9/","title":"Serving Your AI \u2014 FastAPI for Real-World Integration","text":""},{"location":"chapter_9/#why-are-we-here","title":"\ud83c\udf09 Why Are We Here?","text":"<p>You\u2019ve now built:</p> <ul> <li>Classical ML models</li> <li>Rule + logic pipelines</li> <li>LLM-powered classifiers, extractors, and transformers</li> </ul> <p>It\u2019s time to ship. Because a model that lives in a notebook\u2026 isn\u2019t a product.</p> <p>This chapter shows you how to turn your hybrid system \u2014 models + rules + prompts \u2014 into a real, usable API using FastAPI \u2014 with logging, fallbacks, caching, versioning, and testability.</p>"},{"location":"chapter_9/#goal","title":"\ud83c\udfaf Goal","text":"<ul> <li>Serve your hybrid AI logic as a versioned web API</li> <li>Apply engineering best practices (logging, caching, error handling)</li> <li>Prepare your AI component for team use and production deployments</li> </ul>"},{"location":"chapter_9/#why-fastapi","title":"\ud83e\udde0 Why FastAPI?","text":"<p>FastAPI is a developer-friendly web framework for building modern Python APIs:</p> <ul> <li>\u2705 Pydantic-powered request validation</li> <li>\u2705 Built-in OpenAPI docs at <code>/docs</code></li> <li>\u2705 Type hints + editor autocomplete</li> <li>\u2705 Async-ready for future scaling</li> </ul> <p>Perfect for wrapping LLM pipelines into testable, monitorable services.</p>"},{"location":"chapter_9/#recommended-project-structure","title":"\ud83d\uddc2\ufe0f Recommended Project Structure","text":"<pre><code>/ai_service\n\u251c\u2500\u2500 main.py               # FastAPI entrypoint\n\u251c\u2500\u2500 api/                  # Routes and endpoints\n\u2502   \u2514\u2500\u2500 routes.py\n\u251c\u2500\u2500 services/             # Your ML / LLM logic\n\u2502   \u2514\u2500\u2500 triage_pipeline.py\n\u251c\u2500\u2500 schemas/              # Request/response models\n\u2502   \u2514\u2500\u2500 models.py\n\u251c\u2500\u2500 config.py             # App settings and secrets\n\u2514\u2500\u2500 tests/                # Unit + integration tests\n</code></pre>"},{"location":"chapter_9/#inference-logic-servicestriage_pipelinepy","title":"\u2699\ufe0f Inference Logic (services/triage_pipeline.py)","text":"<pre><code>def triage_message(message):\n    if not message or len(message.strip()) &lt; 5:\n        return {\"action\": \"ignore\", \"reason\": \"Empty message\"}\n\n    if \"vip\" in message.lower():\n        return {\"action\": \"escalate\", \"reason\": \"VIP customer\"}\n\n    try:\n        label = call_llm_classifier(message)\n        urgency = extract_urgency(message)\n    except Exception as e:\n        return {\"action\": \"error\", \"reason\": str(e)}\n\n    return {\n        \"action\": \"route\",\n        \"category\": label,\n        \"urgency\": urgency\n    }\n</code></pre>"},{"location":"chapter_9/#pydantic-schemas-schemasmodelspy","title":"\ud83e\uddfe Pydantic Schemas (schemas/models.py)","text":"<pre><code>from pydantic import BaseModel\n\nclass TriageRequest(BaseModel):\n    message: str\n\nclass TriageResponse(BaseModel):\n    action: str\n    category: str | None = None\n    urgency: str | None = None\n    reason: str | None = None\n</code></pre>"},{"location":"chapter_9/#fastapi-endpoint-apiroutespy","title":"\ud83d\ude80 FastAPI Endpoint (api/routes.py)","text":"<pre><code>from fastapi import APIRouter\nfrom schemas.models import TriageRequest, TriageResponse\nfrom services.triage_pipeline import triage_message\nimport logging\n\nrouter = APIRouter()\n\n@router.post(\"/v1/triage\", response_model=TriageResponse)\ndef handle_triage(req: TriageRequest):\n    logging.info(f\"Input: {req.message}\")\n    result = triage_message(req.message)\n    logging.info(f\"Result: {result}\")\n    return result\n</code></pre>"},{"location":"chapter_9/#entrypoint-mainpy","title":"\ud83d\udce6 Entrypoint (main.py)","text":"<pre><code>from fastapi import FastAPI\nfrom api.routes import router\nimport uvicorn\nimport logging\n\napp = FastAPI()\napp.include_router(router)\n\nlogging.basicConfig(level=logging.INFO)\n\nif __name__ == \"__main__\":\n    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n</code></pre> <p>Test in your browser at: <code>http://localhost:8000/docs</code></p>"},{"location":"chapter_9/#caching-performance-tips","title":"\ud83d\udca1 Caching &amp; Performance Tips","text":"<ul> <li>Cache repeat queries using <code>functools.lru_cache</code> or Redis</li> <li>Cache embeddings or LLM responses to save tokens</li> </ul> <pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=128)\ndef get_label(message):\n    return call_llm_classifier(message)\n</code></pre>"},{"location":"chapter_9/#error-handling-fallbacks","title":"\ud83d\udee1\ufe0f Error Handling + Fallbacks","text":"<ul> <li>Always catch LLM exceptions</li> <li>Return consistent schema with <code>action: \"error\"</code></li> <li>Add retries using <code>tenacity</code> if needed</li> </ul>"},{"location":"chapter_9/#api-versioning","title":"\ud83d\udd10 API Versioning","text":"<ul> <li>Use route prefixes like <code>/v1/</code> and <code>/v2/</code></li> <li>Add <code>/version</code> endpoint with logic details</li> </ul>"},{"location":"chapter_9/#logging-best-practices","title":"\ud83d\udcca Logging Best Practices","text":"<ul> <li>Log: request ID, model, token usage, latency</li> <li>Don\u2019t log raw PII unless masked</li> <li>Use <code>logging</code> levels to control output</li> </ul>"},{"location":"chapter_9/#health-and-metadata-endpoints","title":"\u2705 Health and Metadata Endpoints","text":"<pre><code>@app.get(\"/health\")\ndef health():\n    return {\"status\": \"ok\"}\n\n@app.get(\"/version\")\ndef version():\n    return {\"version\": \"1.0.1\", \"model\": \"gpt-4\", \"llm_strategy\": \"few-shot\"}\n</code></pre>"},{"location":"chapter_9/#unit-tests-with-fastapi","title":"\ud83d\udd2c Unit Tests with FastAPI","text":"<pre><code>from fastapi.testclient import TestClient\nfrom main import app\n\nclient = TestClient(app)\n\ndef test_triage():\n    res = client.post(\"/v1/triage\", json={\"message\": \"I was charged twice\"})\n    assert res.status_code == 200\n    assert \"category\" in res.json()\n</code></pre>"},{"location":"chapter_9/#exit-outcome","title":"\u2705 Exit Outcome","text":"<p>You now:</p> <ul> <li>Can serve your LLM-powered logic via FastAPI</li> <li>Understand logging, testing, versioning, and caching</li> <li>Have all the tools to turn AI from notebook into deployable product</li> </ul> <p>Next: Let\u2019s scale this with observability, queues, and containers.</p>"},{"location":"final_project_ai_microservice/","title":"Final Project: AI-Powered Microservice \u2014 From Model to Production","text":""},{"location":"final_project_ai_microservice/#goal","title":"\ud83c\udfaf Goal","text":"<p>Build a complete AI system that:</p> <ul> <li>Classifies support tickets for urgency and category</li> <li>Uses both classical models and LLM fallback</li> <li>Applies rules and thresholds intelligently</li> <li>Exposes predictions via a FastAPI service</li> <li>Includes logging, validation, and observability</li> </ul> <p>This project simulates what you'd ship in a real-world application \u2014 only cleaner and more explainable.</p>"},{"location":"final_project_ai_microservice/#what-youll-build","title":"\ud83d\udd27 What You\u2019ll Build","text":"Component Description \ud83e\udde0 ML Model Logistic regression to classify urgency and category \ud83d\udcd6 LLM Fallback Use OpenAI (or mock) when confidence is low \ud83e\uddea Rule Layer Pre-checks, threshold overrides, human-review triggers \ud83c\udf10 API FastAPI service to expose <code>/predict</code> \ud83d\udcdc Logging Save inputs, predictions, confidence, fallback source \ud83e\uddf0 Bonus Add CLI, streamlit UI, or feedback loop"},{"location":"final_project_ai_microservice/#inputs-and-outputs","title":"\ud83e\udde9 Inputs and Outputs","text":""},{"location":"final_project_ai_microservice/#input","title":"Input:","text":"<pre><code>{\n  \"message\": \"Hi, I got charged twice \u2014 please fix this ASAP!\"\n}\n</code></pre>"},{"location":"final_project_ai_microservice/#output","title":"Output:","text":"<pre><code>{\n  \"urgent\": true,\n  \"category\": \"Billing\",\n  \"confidence\": 0.91,\n  \"source\": \"model\",\n  \"action\": \"route\"\n}\n</code></pre>"},{"location":"final_project_ai_microservice/#file-layout-suggestion","title":"\ud83d\udcc1 File Layout Suggestion","text":"<pre><code>/ticket_ai_microservice\n\u251c\u2500\u2500 model_logic.py         # Feature extraction + prediction logic\n\u251c\u2500\u2500 llm_fallback.py        # Call OpenAI or return simulated LLM response\n\u251c\u2500\u2500 api.py                 # FastAPI wrapper\n\u251c\u2500\u2500 utils.py               # Validation, confidence checks\n\u251c\u2500\u2500 logs.jsonl             # Append input/output logs here\n\u251c\u2500\u2500 data.csv               # Your labeled sample messages\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"final_project_ai_microservice/#build-checklist","title":"\ud83d\udee0\ufe0f Build Checklist","text":""},{"location":"final_project_ai_microservice/#part-1-rule-model-pipeline","title":"\u2705 Part 1: Rule + Model Pipeline","text":"<ul> <li>Load trained <code>LogisticRegression</code> models</li> <li>Extract features (e.g., word count, exclamations)</li> <li>Predict <code>urgent</code> and <code>category</code></li> <li>Log predictions and confidence</li> </ul>"},{"location":"final_project_ai_microservice/#part-2-add-fallback-logic","title":"\u2705 Part 2: Add Fallback Logic","text":"<ul> <li>If <code>confidence &lt; 0.5</code>, call <code>llm_fallback(prompt)</code></li> <li>Ensure LLM returns structured output (e.g., via regex or mock)</li> </ul>"},{"location":"final_project_ai_microservice/#part-3-serve-with-fastapi","title":"\u2705 Part 3: Serve with FastAPI","text":"<pre><code>POST /predict\n{\n  \"message\": \"My plan is not working. Please help!\"\n}\n</code></pre> <p>Return:</p> <ul> <li>prediction</li> <li>confidence</li> <li>source (\"model\" or \"llm\")</li> <li>action (\"route\", \"review\", or \"ignore\")</li> </ul>"},{"location":"final_project_ai_microservice/#part-4-add-logging","title":"\u2705 Part 4: Add Logging","text":"<ul> <li>Append every request/response to <code>logs.jsonl</code></li> <li>Include confidence, token usage (if LLM), and any errors</li> </ul>"},{"location":"final_project_ai_microservice/#bonus-features","title":"\ud83e\uddea Bonus Features","text":"Feature Description \ud83d\udd01 Feedback Loop Let user submit \u201ccorrect category\u201d \ud83d\udcc9 Rate Limiting Prevent spam or rapid requests \ud83e\uddd1\u200d\ud83d\udcbc Admin View Review predictions with low confidence \ud83c\udf9b\ufe0f Config File Tune thresholds without editing code \ud83c\udf0d Streamlit UI Input message + show model/LLM response live"},{"location":"final_project_ai_microservice/#reflection-prompts","title":"\ud83d\udcad Reflection Prompts","text":"<ul> <li>What decisions were hardest to trust the model for?</li> <li>What failure types did you catch with logging?</li> <li>How would you monitor this in production?</li> <li>What would you add to make this secure?</li> </ul>"},{"location":"final_project_ai_microservice/#what-youve-practiced","title":"\u2705 What You\u2019ve Practiced","text":"<ul> <li>Real model training (LogisticRegression)</li> <li>Confidence-aware fallback to LLMs</li> <li>API design and structured inputs</li> <li>Logging, explainability, and observability</li> <li>Blending AI with production software skills</li> </ul> <p>You didn\u2019t just build a model. You built a shippable, testable, explainable AI service.</p>"},{"location":"final_project_ai_microservice/#stretch-goal","title":"\ud83c\udfc6 Stretch Goal","text":"<p>Deploy your API on Render, Railway, or Hugging Face Spaces. Even a demo in Streamlit counts.</p>"},{"location":"fun_ml_toolbox/","title":"\ud83d\udee0\ufe0f The Fun ML Toolbox: Tools, Frameworks &amp; Math Made Easy!","text":""},{"location":"fun_ml_toolbox/#why-this-toolbox","title":"\ud83c\udfaf Why This Toolbox?","text":"<p>You've encountered terms like <code>scikit-learn</code>, <code>Naive Bayes</code>, <code>vectorizers</code>, and more. This chapter is your go-to guide to understanding these tools intuitively and practically\u2014giving you just enough math and mechanics to confidently use them.</p> <p>\u26a1 Quick note: If diving into math or deeper mechanics isn't your jam, feel free to skip ahead and jump back into building exciting things!</p>"},{"location":"fun_ml_toolbox/#tool-selection-matrix","title":"\ud83d\udcca Tool Selection Matrix","text":"Use Case Best Tool Classify simple text Naive Bayes Need explainability Logistic Regression Want pre-trained language models HuggingFace Transformers Fast deployment of a model FastAPI Working with raw text input CountVectorizer"},{"location":"fun_ml_toolbox/#sidebar-whats-a-feature","title":"\ud83e\udde0 Sidebar: What\u2019s a Feature?","text":"<p>A feature is just a signal or input the model uses to make a decision.</p> <p>For text, common features include:</p> <ul> <li>The presence of certain words (like \u201curgent\u201d or \u201cpayment\u201d)</li> <li>The total number of words</li> <li>Whether the sentence ends with a question mark</li> </ul> <p>You saw this in action back in:</p> <ul> <li>\u2705 Chapter 2 (vectorizer converting words to numbers)</li> <li>\u2705 Chapter 4 (Naive Bayes using word counts to calculate probabilities)</li> </ul>"},{"location":"fun_ml_toolbox/#part-1-friendly-frameworks-tools","title":"\ud83e\uddf0 Part 1: Friendly Frameworks &amp; Tools","text":""},{"location":"fun_ml_toolbox/#1-scikit-learn","title":"1. Scikit-learn","text":"<ul> <li>What: A simple, versatile Python library for classical ML.</li> <li>Use cases: Classification, regression, clustering, dimensionality reduction.</li> <li>Strengths: Easy syntax, robust documentation, extensive community support.</li> <li>Practical analogy: Your Swiss Army knife\u2014reliable, multi-functional, and handy for everyday ML tasks.</li> </ul>"},{"location":"fun_ml_toolbox/#2-transformers-huggingface","title":"2. Transformers (HuggingFace)","text":"<ul> <li>What: Advanced library for cutting-edge NLP models.</li> <li>Use cases: Text classification, sentiment analysis, text summarization, text generation.</li> <li>Strengths: Pre-trained models, simple API, powerful performance.</li> <li>Practical analogy: Supercars\u2014ready-made, powerful, and exciting to drive.</li> </ul>"},{"location":"fun_ml_toolbox/#3-fastapi","title":"3. FastAPI","text":"<ul> <li>What: Modern, high-performance web framework to rapidly serve ML models as APIs.</li> <li>Use cases: Quick deployment of models into production, building microservices.</li> <li>Strengths: Fast, intuitive, automatic documentation.</li> <li>Practical analogy: Express lane\u2014efficiently taking your ideas from concept to deployment.</li> </ul>"},{"location":"fun_ml_toolbox/#4-openai-apis-gpt-4-gpt-35","title":"4. OpenAI APIs (GPT-4, GPT-3.5)","text":"<ul> <li>What: Simple API access to powerful language models.</li> <li>Use cases: Content creation, summarization, chatbots, language understanding.</li> <li>Strengths: Instant integration, minimal setup, versatile NLP capabilities.</li> <li>Practical analogy: Personal AI genie\u2014ask it anything, and it answers immediately.</li> </ul>"},{"location":"fun_ml_toolbox/#part-2-math-intuition-made-fun","title":"\ud83c\udfb2 Part 2: Math Intuition Made Fun","text":""},{"location":"fun_ml_toolbox/#vectorization-words-numbers","title":"\ud83d\udd39 Vectorization: Words \u2192 Numbers","text":"<p>\ud83e\udde0 Try sketching it: Imagine a row of labeled boxes \u2014 \"urgent\", \"login\", \"payment\", etc. For each sentence, check which boxes get filled in. That\u2019s your vector.</p> <ul> <li>What: Transforming text data into numeric form.</li> <li>Why: Computers can\u2019t read text directly, but they excel at processing numbers.</li> <li>How it works: Each unique word is assigned a numeric position, and each text piece is represented as counts or frequencies of these words.</li> <li>Intuition: Think about sorting groceries into bins\u2014apples into bin #1, bananas into bin #2. Similarly, words go into numeric \"bins.\"</li> </ul>"},{"location":"fun_ml_toolbox/#naive-bayes-quick-probability-judgments","title":"\ud83d\udd39 Naive Bayes: Quick Probability Judgments","text":"<ul> <li>What: Probability-based classifier making rapid decisions.</li> <li>Why: Extremely fast, works well with text data, effective for spam filtering.</li> <li>How it works: Calculates probabilities based on past examples, assuming each feature (word) independently contributes to the outcome.</li> <li>Intuition: Quickly guessing \"it's raining\" from seeing a wet road and cloudy sky, even if you're ignoring how these factors might be connected.</li> </ul>"},{"location":"fun_ml_toolbox/#logistic-regression-smooth-decisions","title":"\ud83d\udd39 Logistic Regression: Smooth Decisions","text":"<ul> <li>What: Predicts the probability of a binary outcome (yes/no decisions).</li> <li>Why: Simple, efficient, interpretable, and effective in practice.</li> <li>How it works: Fits data using a logistic function to smoothly transition between two classes.</li> <li>Intuition: Deciding if you should wear a jacket based on temperature\u2014smooth transition from \"no jacket\" to \"definitely jacket.\"</li> </ul>"},{"location":"fun_ml_toolbox/#decision-boundaries-invisible-lines","title":"\ud83d\udd39 Decision Boundaries: Invisible Lines","text":"<p>\ud83e\udde0 Mental visual: Imagine drawing loops around piles of laundry. That\u2019s what a decision boundary does in a model.</p> <ul> <li>What: The boundaries a model learns to separate categories.</li> <li>Why: Helps visualize how a model classifies different data points.</li> <li>How it works: Model learns these boundaries during training by finding the optimal lines (or curves) that separate data points.</li> <li>Intuition: Like sorting laundry into piles\u2014whites, colors, darks\u2014you naturally form boundaries to classify items.</li> </ul>"},{"location":"fun_ml_toolbox/#part-3-math-for-engineerswhats-essential","title":"\ud83d\ude80 Part 3: Math for Engineers\u2014What\u2019s Essential?","text":""},{"location":"fun_ml_toolbox/#must-know-for-engineers","title":"\u2705 Must-Know for Engineers:","text":"<ul> <li>The purpose and practical applications of each method.</li> <li>When and how to apply these tools effectively.</li> </ul>"},{"location":"fun_ml_toolbox/#optional-deep-dives","title":"\u26a0\ufe0f Optional Deep Dives:","text":"<ul> <li>Detailed mathematical derivations and proofs.</li> <li>Algorithm-specific optimization techniques.</li> </ul> <p>Engineer Tip: Stay focused primarily on practical application; dive deeper only if curiosity drives you there.</p>"},{"location":"fun_ml_toolbox/#extended-mini-experiment-practical-vectorization-classification","title":"\ud83c\udfae Extended Mini-Experiment: Practical Vectorization &amp; Classification","text":"<p>\ud83d\udcd8 You saw this pattern back in Chapter 2 (plug-and-play models) and Chapter 4 (Naive Bayes confidence classifier). Here, you\u2019ll build it from scratch one more time.</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Training data\ntweets = [\"AI is amazing\", \"I love machine learning\", \"AI simplifies life\", \"I enjoy programming\"]\nlabels = [\"positive\", \"positive\", \"positive\", \"neutral\"]\n\n# Vectorization\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(tweets)\nprint(\"Vocabulary:\", vectorizer.get_feature_names_out())\nprint(\"Vectors:\", X.toarray())\n\n# Classification\nmodel = MultinomialNB()\nmodel.fit(X, labels)\n\n# Predicting new tweet\nnew_tweet = vectorizer.transform([\"I love AI\"])\nprediction = model.predict(new_tweet)\n\nprint(\"Prediction:\", prediction)\n</code></pre> <p>\ud83c\udf89 Great job! You\u2019ve just performed vectorization and classification, two core ML steps, in an intuitive, practical way!</p>"},{"location":"fun_ml_toolbox/#quick-reference-cheat-sheet","title":"\ud83d\udccc Quick Reference Cheat Sheet","text":"Tool Best for... Quick Analogy Scikit-learn Fast ML prototyping Swiss Army knife Transformers Advanced NLP tasks Pre-built NLP supercars FastAPI ML model serving &amp; quick deployment Express lane to production OpenAI GPT APIs Automating text-based tasks Personal AI genie Vectorization Converting words \u2192 numeric form Grocery sorting bins Naive Bayes Quick probability judgments Weather guessing Logistic Regression Smooth binary decisions Jacket-wearing decisions Decision Boundaries Separating categories Laundry sorting piles"},{"location":"fun_ml_toolbox/#level-up-badge-unlocked","title":"\ud83c\udfc5 Level-Up Badge Unlocked!","text":"<p>\ud83c\udf96\ufe0f \"Toolbox Master\" Badge \u2014 You mastered the Fun ML Toolbox, gaining clear, practical insights into essential ML frameworks and intuitive math!</p> <p>You're now well-equipped with both intuitive math and powerful tools! Ready to keep building amazing AI projects? Let's keep going! \ud83d\ude80</p>"},{"location":"project_1/","title":"\ud83e\udde0 Checkpoint Project 1 : From Rules to Predictions","text":""},{"location":"project_1/#why-now","title":"\ud83e\ude9c Why Now?","text":"<p>You\u2019ve just learned how models make predictions \u2014 and more importantly, how to handle uncertainty using confidence scores and fallback logic.</p> <p>Now it\u2019s time to zoom out and compare three different ways to solve the same problem:</p> <ul> <li>Hard-coded logic</li> <li>Pattern matching</li> <li>Machine learning</li> </ul> <p>This is your chance to see where each technique shines \u2014 and where it breaks.</p>"},{"location":"project_1/#goal","title":"\ud83c\udfaf Goal","text":"<p>Use everything you\u2019ve learned so far to compare three ways of solving a real-world classification problem:</p> <ol> <li>Rule-based logic</li> <li>Pattern-based heuristics (like regex)</li> <li>Machine learning (your first trained model)</li> </ol> <p>By the end, you\u2019ll reflect on where each approach worked \u2014 and where it fell short.</p>"},{"location":"project_1/#the-scenario","title":"\ud83e\udde0 The Scenario","text":"<p>Imagine you\u2019re building a system that tags incoming messages (emails, tickets, chats) as urgent or not urgent.</p> <p>Your team has a growing inbox, and needs a better way to prioritize what to handle first. Your job is to prototype and evaluate different solutions.</p>"},{"location":"project_1/#what-youll-build","title":"\ud83d\udee0\ufe0f What You'll Build","text":"<p>A Jupyter notebook or script with these parts:</p>"},{"location":"project_1/#part-1-rule-based-classifier","title":"Part 1: Rule-Based Classifier","text":"<ul> <li>Write simple <code>if</code>/<code>else</code> logic like:</li> </ul> <pre><code>if \"urgent\" in message.lower():\n    return True\n</code></pre> <ul> <li>Add more rules based on exclamation marks, capital letters, or known keywords</li> </ul>"},{"location":"project_1/#part-2-regex-pattern-based","title":"Part 2: Regex / Pattern-Based","text":"<ul> <li>Use regular expressions to match variants like:</li> </ul> <pre><code>\\b(asap|immediately|refund|angry)\\b\n</code></pre> <ul> <li>Tune the pattern matching based on message phrasing</li> </ul>"},{"location":"project_1/#part-3-ml-based-classifier","title":"Part 3: ML-Based Classifier","text":"<ul> <li>Create a dataset of 10\u201320 example messages with a label (<code>urgent</code> = 1 or 0)</li> <li>Extract simple features:</li> <li>Word count</li> <li>Number of exclamations</li> <li>Presence of urgent-sounding words</li> <li>Train a <code>LogisticRegression</code> model</li> <li>Print weights, accuracy, and predictions</li> </ul>"},{"location":"project_1/#part-4-reflection","title":"Part 4: Reflection","text":"<ul> <li>Where did each approach do well?</li> <li>Where did rules break down?</li> <li>Where did the model make mistakes?</li> <li>Which system would you trust in production (and why)?</li> </ul>"},{"location":"project_1/#sample-dataset-start-here-if-you-want","title":"\ud83e\uddea Sample Dataset (Start Here If You Want)","text":"Message Urgent? I need help with this ASAP! 1 Can you check this when you\u2019re free? 0 THIS IS BROKEN. I\u2019M ANGRY. 1 Just a heads up on the new pricing 0 Please call me back immediately!!! 1 <p>Create more \u2014 mix calm, polite, angry, confusing messages.</p>"},{"location":"project_1/#bonus-challenges","title":"\ud83c\udf1f Bonus Challenges","text":"<ul> <li>Try visualizing your model\u2019s predictions (scatterplot with matplotlib)</li> <li>Let users paste in a message and see which system flags it as urgent</li> <li>Build a feedback loop: update the model when a human tags a message differently</li> </ul>"},{"location":"project_1/#self-check","title":"\u2705 Self Check","text":"<ul> <li>[ ] I implemented all 3 systems (rules, regex, ML)</li> <li>[ ] I evaluated each one and noted strengths/weaknesses</li> <li>[ ] I trained and interpreted a model with at least 10\u201315 examples</li> <li>[ ] I reflected on tradeoffs and explained which system I\u2019d use when</li> <li>[ ] Bonus: I added UI or feedback interaction</li> </ul>"},{"location":"project_1/#what-youve-practiced","title":"\ud83c\udfc1 What You\u2019ve Practiced","text":"<ul> <li>Framing a problem for ML (vs logic)</li> <li>Designing features</li> <li>Training a model from scratch</li> <li>Comparing rule-based vs learning systems</li> <li>Thinking like a builder who chooses the right tool for the job</li> </ul>"},{"location":"project_2/","title":"\u2705 Checkpoint Project #2: AI-Powered Auto-Responder","text":""},{"location":"project_2/#why-this-project","title":"\ud83e\udde0 Why This Project?","text":"<p>After learning:</p> <ul> <li>How to classify support messages (multi-class)</li> <li>How to interpret model confidence</li> <li>How to use thresholds and fallback logic</li> </ul> <p>\u2026it\u2019s time to build a real-world decision system that blends AI prediction with human review.</p> <p>This project is a lightweight simulation of how AI systems operate in high-stakes environments \u2014 with automation, thresholds, and fallbacks.</p>"},{"location":"project_2/#project-goal","title":"\ud83c\udfaf Project Goal","text":"<p>Build a simple system that:</p> <ol> <li> <p>Classifies incoming support messages into categories:</p> <ul> <li>Billing</li> <li>Technical</li> <li>General Inquiry</li> <li>Cancel/Unsubscribe</li> <li>Other</li> </ul> </li> <li> <p>Applies a confidence threshold:</p> <ul> <li>If confidence &gt; 0.7: send auto-response using a predefined template</li> <li>Else: flag for human review</li> </ul> </li> </ol>"},{"location":"project_2/#system-design-overview","title":"\ud83e\uddf1 System Design Overview","text":"<p>Your system will:</p> <ul> <li>Take raw text input (support ticket)</li> <li>Return:</li> <li><code>category</code></li> <li><code>confidence</code></li> <li><code>action</code> \u2192 \u201cauto-response\u201d or \u201chuman-review\u201d</li> <li>(Optional) Suggested response if confident</li> </ul>"},{"location":"project_2/#dataset","title":"\ud83d\uddc3\ufe0f Dataset","text":"<p>Create a dataset of ~50\u201360 support messages manually labeled.</p> Message Category \u201cI want a refund on my last invoice\u201d Billing \u201cThe app crashes when I try to sync\u201d Technical \u201cWhat are your office hours?\u201d General Inquiry \u201cPlease cancel my subscription immediately\u201d Cancel \u201cHi, just saying thanks!\u201d Other <p>Include:</p> <ul> <li>Easy and hard-to-classify messages</li> <li>Polite, angry, vague, and edge-case phrasing</li> </ul>"},{"location":"project_2/#implementation-checklist","title":"\ud83d\udd28 Implementation Checklist","text":""},{"location":"project_2/#step-1-feature-extraction","title":"Step 1: Feature Extraction","text":"<p>Use:</p> <ul> <li>Word count</li> <li>Exclamation/question marks</li> <li>Keyword presence (e.g., \u201crefund\u201d, \u201ccrash\u201d, \u201cunsubscribe\u201d)</li> <li>TF-IDF or CountVectorizer (recommended)</li> </ul>"},{"location":"project_2/#step-2-train-multi-class-classifier","title":"Step 2: Train Multi-Class Classifier","text":"<p>Use logistic regression, random forest, or any scikit-learn classifier.</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Train model\nmodel = LogisticRegression(multi_class='multinomial')\nmodel.fit(X_train, y_train)\n</code></pre> <p>Evaluate using:</p> <ul> <li><code>classification_report()</code></li> <li><code>predict_proba()</code></li> </ul>"},{"location":"project_2/#step-3-build-confidence-based-decision-logic","title":"Step 3: Build Confidence-Based Decision Logic","text":"<pre><code>def auto_responder(text):\n    x = vectorizer.transform([text])\n    proba = model.predict_proba(x)[0]\n    top_category = model.classes_[proba.argmax()]\n    confidence = proba.max()\n\n    if confidence &gt; 0.7:\n        return {\n            \"category\": top_category,\n            \"confidence\": round(confidence, 2),\n            \"action\": \"auto-response\",\n            \"response\": lookup_template(top_category)\n        }\n    else:\n        return {\n            \"category\": top_category,\n            \"confidence\": round(confidence, 2),\n            \"action\": \"human-review\"\n        }\n</code></pre>"},{"location":"project_2/#bonus-features","title":"\ud83e\uddea Bonus Features","text":"<ul> <li>Return second-best category if confidence is low</li> <li>Let user simulate adjusting the confidence threshold</li> <li>Track and log predictions over time</li> <li>Visualize confusion matrix using Seaborn</li> </ul>"},{"location":"project_2/#optional-add-a-cli","title":"\ud83d\udcbb Optional: Add a CLI","text":"<pre><code>while True:\n    msg = input(\"Paste support message (or 'q' to quit): \")\n    if msg == 'q':\n        break\n    print(auto_responder(msg))\n</code></pre>"},{"location":"project_2/#what-to-submit","title":"\ud83d\udccb What to Submit","text":"<ul> <li>Python script or notebook</li> <li>A CSV of your labeled dataset</li> <li>Output from 5\u201310 test inputs</li> <li>A 1\u20132 paragraph reflection:</li> <li>What caused misclassifications?</li> <li>Did the threshold help reduce bad predictions?</li> <li>How would you improve it for production?</li> </ul>"},{"location":"project_2/#what-youve-practiced","title":"\u2705 What You\u2019ve Practiced","text":"<ul> <li>Multi-class classification with real decisions</li> <li>Confidence scoring and thresholds</li> <li>Designing automation vs fallback flows</li> <li>Thinking like a product-minded ML engineer</li> </ul>"},{"location":"project_2_ticket_triage/","title":"\u2705 Checkpoint Project #2: Smart Ticket Triage System","text":""},{"location":"project_2_ticket_triage/#why-this-project","title":"\ud83e\udde0 Why This Project?","text":"<p>You\u2019ve now learned:</p> <ul> <li>How to train a binary classifier (urgent vs not urgent)</li> <li>How to go beyond accuracy (precision, recall, threshold tuning)</li> <li>How to handle multi-class outputs (ticket categories)</li> <li>How to debug model confidence and confusion</li> </ul> <p>Now it\u2019s time to build a complete mini-system that brings all of this together.</p> <p>This project simulates what you\u2019d actually ship in a real product: a lightweight, testable ML system for automatically prioritizing and routing support tickets.</p>"},{"location":"project_2_ticket_triage/#project-goal","title":"\ud83c\udfaf Project Goal","text":"<p>Build a working system that:</p> <ol> <li>Determines if a support ticket is urgent</li> <li>Classifies it into a category (like Billing, Tech, Feedback, or Other)</li> <li>Applies confidence thresholds to decide whether to:<ul> <li>Auto-route it to the right team</li> <li>Escalate low-confidence tickets to human review</li> </ul> </li> </ol>"},{"location":"project_2_ticket_triage/#system-design-overview","title":"\ud83e\uddf1 System Design Overview","text":"<p>Your system will:</p> <ul> <li>Take a raw support ticket (just a text string)</li> <li>Return:</li> <li><code>urgency</code> \u2192 Yes / No</li> <li><code>category</code> \u2192 Billing / Tech / Feedback / Other</li> <li><code>confidence</code> \u2192 Percentage</li> <li><code>action</code> \u2192 Auto-route / Human review</li> </ul>"},{"location":"project_2_ticket_triage/#dataset-you-build-it","title":"\ud83d\uddc3\ufe0f Dataset (You Build It)","text":"<p>Create a small dataset of ~40\u201360 support tickets with two labels:</p> Message Urgent? Category \u201cHELP \u2014 I got double charged!!!\u201d 1 Billing \u201cJust wanted to say the UI is much better now\u201d 0 Feedback \u201cApp crashes when I press \u2018Sync\u2019\u201d 1 Tech \u201cWhat time is your office open?\u201d 0 Other ... ... ... <p>Make sure to include:</p> <ul> <li>Urgent &amp; non-urgent tickets in each category</li> <li>Calm, angry, polite, and vague phrasings</li> <li>A few hard-to-classify edge cases</li> </ul>"},{"location":"project_2_ticket_triage/#implementation-checklist","title":"\ud83d\udd28 Implementation Checklist","text":""},{"location":"project_2_ticket_triage/#step-1-feature-extraction","title":"Step 1: Feature Extraction","text":"<p>Extract simple features from text:</p> <ul> <li>Message length</li> <li>Word count</li> <li>Number of exclamation marks</li> <li>Keyword flags (e.g., \u201crefund\u201d, \u201cbroken\u201d, \u201clove\u201d, \u201casap\u201d)</li> <li>TF-IDF (optional for bonus NLP version)</li> </ul>"},{"location":"project_2_ticket_triage/#step-2-train-binary-classifier-urgent","title":"Step 2: Train Binary Classifier (Urgent?)","text":"<p>Train a <code>LogisticRegression</code> model to predict urgency:</p> <pre><code>urgent_model = LogisticRegression()\nurgent_model.fit(X_train, y_urgent)\n</code></pre> <p>Use:</p> <ul> <li><code>predict_proba()</code> to get urgency confidence</li> <li>Threshold tuning (<code>0.3</code>, <code>0.5</code>, etc.) to decide final call</li> </ul>"},{"location":"project_2_ticket_triage/#step-3-train-multi-class-classifier-category","title":"Step 3: Train Multi-Class Classifier (Category)","text":"<p>Train a second <code>LogisticRegression(multi_class='multinomial')</code> model to predict ticket category:</p> <pre><code>category_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\ncategory_model.fit(X_train, y_category)\n</code></pre> <p>Evaluate with:</p> <ul> <li><code>classification_report()</code></li> <li><code>confusion_matrix()</code></li> </ul>"},{"location":"project_2_ticket_triage/#step-4-decision-logic-layer","title":"Step 4: Decision Logic Layer","text":"<p>Build a function like:</p> <pre><code>def triage_ticket(text):\n    features = extract_features(text)\n\n    urgency_proba = urgent_model.predict_proba([features])[0][1]\n    category_proba = category_model.predict_proba([features])[0]\n\n    is_urgent = urgency_proba &gt; 0.4\n    top_category = category_model.classes_[category_proba.argmax()]\n    confidence = category_proba.max()\n\n    action = \"auto-route\" if confidence &gt; 0.6 else \"human-review\"\n\n    return {\n        \"urgent\": is_urgent,\n        \"category\": top_category,\n        \"confidence\": round(confidence, 2),\n        \"action\": action\n    }\n</code></pre>"},{"location":"project_2_ticket_triage/#step-5-build-a-simple-cli","title":"Step 5: Build a Simple CLI","text":"<pre><code>while True:\n    msg = input(\"Paste support ticket (or 'q' to quit): \")\n    if msg == 'q':\n        break\n    result = triage_ticket(msg)\n    print(result)\n</code></pre>"},{"location":"project_2_ticket_triage/#bonus-features","title":"\ud83e\uddea Bonus Features","text":"<ul> <li>Add second-best category if confidence is low</li> <li>Let users give feedback (\u201ccorrect category was ___\u201d)</li> <li>Store predictions and allow re-training</li> <li>Export logs of all predictions and actions</li> <li>Add a Streamlit UI (optional)</li> </ul>"},{"location":"project_2_ticket_triage/#what-to-submit","title":"\ud83d\udccb What to Submit","text":"<ul> <li>Python script or notebook</li> <li>Sample outputs from 5 test messages</li> <li>Screenshots or text output from CLI</li> <li>1\u20132 paragraph reflection:</li> <li>What worked well?</li> <li>What confused your model?</li> <li>What might help improve it in a real system?</li> </ul>"},{"location":"project_2_ticket_triage/#what-youve-practiced","title":"\u2705 What You\u2019ve Practiced","text":"<ul> <li>Designing a real-world ML flow with both binary and multi-class models</li> <li>Feature engineering and data labeling</li> <li>Confidence thresholds and decision logic</li> <li>Creating developer-friendly AI tools</li> <li>Thinking through tradeoffs between automation vs human-in-the-loop</li> </ul>"},{"location":"project_3/","title":"\ud83e\uddea Checkpoint Project #3: LLM-Powered FAQ Bot","text":"<p>This is your first real-world mini-project using an LLM. You\u2019ll apply everything you learned in Chapters 8a\u20138c:</p> <p>\u2705 Instructional prompting \u2705 Context injection \u2705 Fallback handling \u2705 Confidence-aware prompting</p>"},{"location":"project_3/#where-you-are","title":"\ud83d\udccd Where You Are","text":"<p>You\u2019ve just finished:</p> <ul> <li>8a: How LLMs work (tokens, hallucination, context windows)</li> <li>8b: Prompting styles (zero-shot, few-shot, train-of-thought)</li> <li>8c: Structured prompting + graceful fallbacks</li> </ul> <p>Before we jump into scaling, APIs, and agents \u2014 let\u2019s make something real.</p>"},{"location":"project_3/#your-mission","title":"\ud83d\ude80 Your Mission","text":"<p>Build a simple LLM assistant that answers questions using only a company FAQ file.</p> <p>It should:</p> <ul> <li>Inject relevant answers into the prompt</li> <li>Tell the user if it\u2019s not sure</li> <li>Be callable via CLI or Streamlit</li> </ul> <p>\ud83d\udcc4 Project Link: project_3_faq_bot.md</p> <p>This project does not require vector search or LangChain. You\u2019ll simulate retrieval with basic keyword matching.</p>"},{"location":"project_3/#after-this","title":"\ud83d\udd1c After This","text":"<p>Once you complete this checkpoint, you\u2019ll be ready to:</p> <ul> <li>Turn your assistant into an API (Chapter 9)</li> <li>Add retry, logging, and caching (Chapter 10)</li> <li>Plug it into tools and memory systems (Chapters 11+)</li> </ul>"},{"location":"project_3_faq_bot/","title":"\u2705 Checkpoint Project #3: LLM-Powered FAQ Bot","text":""},{"location":"project_3_faq_bot/#why-this-project","title":"\ud83e\udde0 Why This Project?","text":"<p>After learning: - How LLMs work (tokens, context windows, hallucination) - How to write and structure prompts - How to inject role instructions and fallbacks</p> <p>\u2026it\u2019s time to build something useful and test your prompting skills.</p>"},{"location":"project_3_faq_bot/#project-goal","title":"\ud83c\udfaf Project Goal","text":"<p>Create a working command-line or Streamlit assistant that answers questions based on a company FAQ \u2014 but only using information in the FAQ.</p> <p>The assistant should: - Receive a question - Inject relevant FAQ content into the prompt - Ask the LLM to answer - Fall back gracefully if unsure</p>"},{"location":"project_3_faq_bot/#system-design-overview","title":"\ud83e\uddf1 System Design Overview","text":"<p>Your pipeline will:</p> <ol> <li>Take a user question  </li> <li>Search your FAQ for relevant content (manually or via keyword match)  </li> <li>Inject it into a prompt  </li> <li>Call OpenAI (e.g., <code>gpt-3.5-turbo</code>)  </li> <li>Return the answer or fallback if not confident</li> </ol>"},{"location":"project_3_faq_bot/#dataset","title":"\ud83d\uddc3\ufe0f Dataset","text":"<p>Use or create a small <code>faq.txt</code> or <code>faq.json</code> file with ~10\u201320 questions and answers. Example:</p> <pre><code>[\n  {\n    \"question\": \"How do I reset my password?\",\n    \"answer\": \"Click 'Forgot password' on the login screen and follow the instructions.\"\n  },\n  {\n    \"question\": \"How can I cancel my subscription?\",\n    \"answer\": \"Go to Billing \u2192 Manage Subscription and select Cancel.\"\n  }\n]\n</code></pre>"},{"location":"project_3_faq_bot/#implementation-checklist","title":"\ud83d\udd28 Implementation Checklist","text":""},{"location":"project_3_faq_bot/#step-1-build-a-simple-keyword-search","title":"Step 1: Build a Simple Keyword Search","text":"<pre><code>def search_faq(question, faq_list):\n    return [entry for entry in faq_list if any(word in entry[\"question\"] for word in question.lower().split())]\n</code></pre>"},{"location":"project_3_faq_bot/#step-2-design-the-prompt-template","title":"Step 2: Design the Prompt Template","text":"<pre><code>PROMPT_TEMPLATE = '''\nYou are a helpful assistant answering questions about our company FAQ.\nOnly use the information below to answer. If unsure, say \u201cSorry, I\u2019m not sure.\u201d\n\nFAQ:\n{faq_chunks}\n\nQuestion: {question}\nAnswer:\n'''\n</code></pre>"},{"location":"project_3_faq_bot/#step-3-call-openai","title":"Step 3: Call OpenAI","text":"<pre><code>import openai\n\ndef ask_llm(prompt):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content.strip()\n</code></pre>"},{"location":"project_3_faq_bot/#step-4-build-the-loop","title":"Step 4: Build the Loop","text":"<pre><code>while True:\n    q = input(\"Ask a question (or 'q' to quit): \")\n    if q == 'q': break\n\n    faq_hits = search_faq(q, faq_data)\n    faq_context = \"\\n\".join([f\"- {x['answer']}\" for x in faq_hits[:3]])\n\n    prompt = PROMPT_TEMPLATE.format(faq_chunks=faq_context, question=q)\n    answer = ask_llm(prompt)\n    print(\"Answer:\", answer)\n</code></pre>"},{"location":"project_3_faq_bot/#bonus-features","title":"\ud83e\uddea Bonus Features","text":"<ul> <li>Add a fallback if <code>faq_hits == []</code></li> <li>Track if the answer contains \u201cSorry\u201d \u2192 count as fallback</li> <li>Let the user give feedback (thumbs up/down)</li> <li>Add a confidence disclaimer automatically (e.g. \u201cBased on our FAQ\u2026\u201d)</li> </ul>"},{"location":"project_3_faq_bot/#optional-streamlit-ui","title":"\ud83d\udcbb Optional: Streamlit UI","text":"<pre><code>import streamlit as st\nst.title(\"FAQ Assistant\")\nq = st.text_input(\"Ask a question:\")\nif q:\n    # same flow as CLI\n    st.write(answer)\n</code></pre>"},{"location":"project_3_faq_bot/#what-to-submit","title":"\ud83d\udccb What to Submit","text":"<ul> <li>Python script or notebook</li> <li>Your <code>faq.json</code> or <code>.txt</code> file</li> <li>Screenshots or console output from 5\u201310 test questions</li> <li>A 1\u20132 paragraph reflection:</li> <li>When did the model hallucinate?</li> <li>What made answers better or worse?</li> <li>How did your prompt affect the result?</li> </ul>"},{"location":"project_3_faq_bot/#what-youve-practiced","title":"\u2705 What You\u2019ve Practiced","text":"<ul> <li>Prompt templating</li> <li>Manual context injection</li> <li>Hallucination prevention strategies</li> <li>Building a minimal retrieval-based LLM tool</li> </ul>"}]}