
# How LLMs Work â€” Context, Embeddings, and Reasoning

---

## ðŸŒ‰ Why Are We Here?

In the last chapter, you built logic pipelines combining rules, models, and LLMs. But youâ€™ve probably noticed something weird:

- Sometimes the LLM gives a brilliant answer.
- Sometimes it sounds confident but is dead wrong.
- Sometimes changing one word in your prompt *completely* changes the outcome.

Whatâ€™s going on?

To build anything reliable with LLMs, you need to understand how they actually â€œthink.â€ This chapter is that foundation.

> Youâ€™re not just learning how to prompt. Youâ€™re learning how the machine predicts.

---

## ðŸŽ¯ Goal

To build a working mental model of how LLMs operate:

- What a "language model" is â€” and what itâ€™s not
- How inputs are processed via tokens and context windows
- Why order, structure, and precision in prompts matter
- What embeddings are and how they underpin many AI systems
- When and why LLMs produce surprising or incorrect answers

---

## ðŸ§  What Is a Language Model?

A **language model** is not a reasoning engine, chatbot, or planning agent.

At its core, it is a system trained to do just one thing:
> **Given a sequence of text, predict the most likely next token.**

That's it.

```text
Input: "My favorite programming language is"
Model: ["Python" (78%), "JavaScript" (10%), "C++" (5%), ...]
```

Language models are trained on massive text datasets to learn:

- Word patterns and probabilities
- Syntax and semantics
- How different topics and tones tend to flow

They do not truly *understand* or *reason*. They excel at generating plausible continuations of text â€” and thatâ€™s what makes them powerful (and sometimes misleading).

---

### ðŸ§  How Does It Predict the Next Token?

Behind the scenes, most modern LLMs use a neural network architecture called a **Transformer**.

The Transformer has a key mechanism called **attention**, which allows the model to look at all the tokens in the prompt and weigh their importance differently.

> Think of it like reading a paragraph with a highlighter â€” the model "pays more attention" to the parts that matter most when choosing the next word.

This allows the model to:

- Understand long-range dependencies
- Make predictions based on relationships between words
- Scale to very large inputs and outputs

---

## ðŸ§© Tokens and Context Windows

LLMs donâ€™t see whole paragraphs â€” they see **tokens**, which are fragments of text:

- A word: `dog`
- A subword: `un`, `believ`, `able`
- Punctuation: `.` or `?`

### â³ Whatâ€™s a Context Window?

Think of it like the modelâ€™s short-term memory. Itâ€™s the number of tokens it can â€œseeâ€ at one time.

- GPT-3.5: ~4,000 tokens (~3,000 words)
- GPT-4 (turbo): up to 128,000 tokens (~100,000 words)

If your input + output exceeds this window:

- Earlier context is dropped
- The model starts forgetting what came before

> ðŸ§  Analogy: Imagine whispering into a tunnel. The model only hears the last X words you say.

---

## ðŸ”  Tokenization in Practice

```python
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokens = tokenizer.tokenize("Language models don't reason.")
print(tokens)
```

Output:
```text
['Language', 'Ä models', 'Ä don', "'", 't', 'Ä reason', '.']
```

---

## ðŸ§  Embeddings: Meaning in Vector Space

Before LLMs predict anything, they convert text into **embeddings** â€” high-dimensional vectors that encode meaning.

Embeddings allow the model to:

- Measure similarity between texts
- Cluster related ideas
- Match prompts to known patterns

```python
from openai import OpenAI

response = openai.Embedding.create(input="How do I reset my password?", model="text-embedding-ada-002")
vector = response['data'][0]['embedding']
```

---

## ðŸ§  Intent vs Prompt â€” Not the Same Thing

What you *want* and what you *say* are different.

### Example:

âŒ Prompt: "Tell me about our app."

- ðŸ¤– Output: vague, promotional, possibly off-topic

âœ… Prompt: "List 3 features of our app in bullet points. Include only 1 sentence per point."

- ðŸ¤– Output: clear, usable, and structured

---

## ðŸ§± System vs User Roles

Chat-based models use structured messages:
```python
messages = [
  {"role": "system", "content": "You are a precise technical assistant."},
  {"role": "user", "content": "Summarize this customer complaint."}
]
```

- **System message**: defines tone, identity, rules
- **User message**: the actual task

---

## âš™ï¸ Temperature and Top-p: Controlling Output Behavior

### ðŸ”¥ Temperature

- `0.0`: deterministic
- `1.0+`: exploratory

### ðŸŽ² Top-p

- Select from the top `p%` of possible next tokens

```python
temperature=0.2, top_p=0.8  # Reliable for structured tasks
```

---

## â— A Word on Hallucination (At the End, Where It Belongs)

LLMs donâ€™t â€œknowâ€ facts or have external memory. They guess what *sounds* right.

Thatâ€™s why:

- They invent URLs
- Misquote legal clauses
- Fabricate names or facts

### How to reduce it:

- Give more structure
- Add documents to prompt (RAG)
- Ask for sources, but verify

---

## ðŸ§  Final Analogy: A Genius Autocomplete

Imagine a super-smart autocomplete:

- Itâ€™s read billions of documents
- But it only guesses what comes next â€” based on what itâ€™s seen
- It forgets everything the moment it responds

> Powerful? Yes. Grounded? Only if you design it that way.

---

## âœ… Exit Outcome

You now:

- Know what a language model really is
- Understand tokens, context windows, and embeddings
- Can structure prompts with intention and control
- Have tools to start building reliable, debuggable LLM systems

> In the next chapter, weâ€™ll turn this theory into hands-on prompting â€” building classifiers, transformers, and smart assistants. Letâ€™s go.
