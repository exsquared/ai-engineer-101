# Chapter 5: Evaluation Beyond Accuracy â€” How to Know If a Model Is *Actually* Working

---

## ğŸ¯ Goal

To learn how to **evaluate models intelligently** â€” beyond just "how many did it get right?"  
Weâ€™ll explore *why accuracy can lie*, and how to use **precision**, **recall**, and **false positive/negative rates** to debug and improve your model like an engineer â€” not just a data scientist.

---

### âš–ï¸ The Accuracy Trap

Letâ€™s say you built a model that predicts whether a support ticket is **urgent**.

Your model says:
> â€œIâ€™m 95% accurate!â€

Sounds great, right?

But hereâ€™s the catch:

- Only **5%** of your tickets are actually urgent.
- Your model is just saying **â€œnot urgentâ€ every time** â€” and still getting 95% right.

Thatâ€™s not intelligence â€” thatâ€™s cheating.

> **High accuracy doesnâ€™t mean your model is useful.**

---

### ğŸ” The Real Questions

What you *really* care about is:

- ğŸ§¨ **Did it miss any urgent tickets?** (Thatâ€™s bad.)
- ğŸš¨ **Did it raise too many false alarms?** (Also bad.)
- ğŸ¯ **Did it correctly flag the ones we needed to act on?** (Thatâ€™s the sweet spot.)

---

### ğŸ§  Think Like a Dev, Not a Statistician

Letâ€™s bring this into your world.

```python
def is_ticket_urgent(text):
    return model.predict(text)  # Returns True or False
```

If your model messes up:

- A *true urgent* ticket goes unnoticed (bad customer experience).
- A *non-urgent* ticket clogs the queue (wasted time).

So instead of â€œaccuracyâ€, letâ€™s ask:

| Type             | What it means                            | Impact                                |
|------------------|------------------------------------------|----------------------------------------|
| âœ… True Positive | Correctly marked urgent                  | Good â€” success                         |
| âŒ False Positive| Said urgent, but wasnâ€™t                  | Bad â€” wasted priority                  |
| âŒ False Negative| Missed an actual urgent message          | Worse â€” customer suffers               |
| âœ… True Negative | Correctly marked not urgent              | Good â€” low priority handled right      |

---

### ğŸ·ï¸ Precision and Recall â€” In Plain English

#### ğŸ¯ Precision = â€œOf all the things I said were urgent, how many actually were?â€

- High precision = You donâ€™t cry wolf
- Low precision = You flag lots of things that arenâ€™t urgent

#### ğŸ•µï¸ Recall = â€œOf all the things that *were* urgent, how many did I actually find?â€

- High recall = You catch most real urgent cases
- Low recall = You miss lots of real issues

---

### âš–ï¸ The Tradeoff

Think of this like a **spam filter**:

> You can tweak your model to be more cautious or more aggressive â€” but you canâ€™t max both.

---

### ğŸ§ª Let's Code It

```python
from sklearn.metrics import classification_report

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
```

Sample output:

```
              precision    recall  f1-score   support

    NotUrgent       0.96      0.98      0.97       80
       Urgent       0.85      0.75      0.80       20
```

---

### ğŸ§© Exercise: Tuning the Threshold

```python
y_scores = model.predict_proba(X_test)[:, 1]  # Probabilities for 'urgent'

threshold = 0.3  # Be more aggressive
y_custom = (y_scores > threshold).astype(int)

print(classification_report(y_test, y_custom))
```

Try thresholds: `0.3`, `0.5`, `0.7`.

---

### ğŸ”„ When to Use What

| You care about...        | Focus on       |
|--------------------------|----------------|
| Avoiding false alarms    | Precision       |
| Not missing real cases   | Recall          |
| Balanced performance     | F1 Score        |

---

### ğŸ’­ Reflection Questions

1. When is high accuracy *not* useful?
2. Can you think of a product where false positives are dangerous?
3. Whatâ€™s more painful in your system: missing an error, or flagging too many?
4. How would you explain precision/recall to a PM?

---

### ğŸ’» Hands-On Exercise

Extend your earlier project (checkpoint #1):

1. Split your message dataset into train/test
2. Train your `LogisticRegression` model
3. Use `.predict_proba()` to extract confidence scores
4. Try different thresholds (`0.3`, `0.5`, `0.7`)
5. Print precision/recall/F1 at each threshold

> Bonus: Add a user-facing confidence score â€” and route low-confidence tickets to human review.

---

### âœ… Exit Outcome

- Go beyond â€œdid it get it right?â€
- Measure *how well* your model is doing in the ways that matter
- Debug performance using confusion matrices and probability scores

---

### â­ï¸ Up Next

In the next chapter, weâ€™ll shift from binary to **multi-class** predictions â€” and what changes when you have more than two possible answers.
