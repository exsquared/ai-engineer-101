# Serve Your Model Like an Engineer  
*Turning AI into a Real Backend Service*

---

## ğŸ§­ Why Are We Here?

Youâ€™ve built a model. Youâ€™ve prompted it, tuned it, and tested it.

But thatâ€™s only half the battle.

In the real world, models donâ€™t sit in notebooks. They live behind APIs, serve real users, and face real problems: timeouts, garbage input, unexpected load, and silent failures.

> This chapter is about turning your model into a **trustworthy, production-grade web service**.

And that means engineering it like you would any other backend service â€” but with some AI-specific twists.

---

## ğŸ”§ What You'll Learn

By the end of this chapter, youâ€™ll:

âœ… Build a FastAPI service to wrap your model  
âœ… Handle bad input with validation and error messages  
âœ… Return clean, traceable responses  
âœ… Think like an engineer, not a demo hacker

> This isnâ€™t just â€œwrap a model in FastAPI.â€ This is: â€œmake it usable, safe, and explainable.â€

---

## ğŸ—ï¸ The Minimal Viable Serving Layer

Letâ€™s start with a simple example:

You have a classifier (say: spam vs not-spam). You want an API like:

```
POST /predict
{
  "message": "I love your product, give me a refund!"
}
```

And the response:

```
{
  "label": "spam",
  "confidence": 0.87
}
```

Letâ€™s build it.

---

## ğŸ’» Step 1: Define a Clean Schema with Pydantic

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class Ticket(BaseModel):
    message: str

class Prediction(BaseModel):
    label: str
    confidence: float
```

Pydantic is FastAPIâ€™s secret weapon â€” it validates, parses, and documents your input/output like an engineerâ€™s dream.

---

## âš™ï¸ Step 2: Stub a Dummy Model Logic

You can replace this later with your real model or API call.

```python
def classify(text):
    if "refund" in text.lower():
        return "spam", 0.91
    return "not-spam", 0.64
```

---

## ğŸš€ Step 3: Build Your Prediction Endpoint

```python
@app.post("/predict", response_model=Prediction)
def predict(ticket: Ticket):
    label, confidence = classify(ticket.message)
    return Prediction(label=label, confidence=confidence)
```

Try it with `curl` or [Hoppscotch](https://hoppscotch.io):
```bash
curl -X POST http://localhost:8000/predict   -H "Content-Type: application/json"   -d '{"message": "refund please"}'
```

---

## ğŸ§  Letâ€™s Engineer It Better

That basic example works â€” but itâ€™s fragile.

Letâ€™s now add the real engineering scaffolding:
- âœ… Input validation
- âœ… Error handling
- âœ… Logging
- âœ… Confidence-aware logic
- âœ… API metadata

---

## ğŸ§± Part 1: Input Validation

Add custom constraints:
```python
from pydantic import Field

class Ticket(BaseModel):
    message: str = Field(min_length=5, max_length=500)
```

Now FastAPI will reject bad input *before* your model even runs.

Try it:
```bash
curl -X POST http://localhost:8000/predict -d '{"message": "ok"}'
```
Returns:
```json
{"detail":[{"loc":["body","message"],"msg":"ensure this value has at least 5 characters"}]}
```

---

## ğŸ§± Part 2: Error Handling

Wrap model calls in try/catch and return clean errors:

```python
from fastapi import HTTPException

@app.post("/predict", response_model=Prediction)
def predict(ticket: Ticket):
    try:
        label, confidence = classify(ticket.message)
        return Prediction(label=label, confidence=confidence)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

## ğŸ§± Part 3: Logging

Always log:

- What was asked
- What was returned
- What failed

```python
import logging
logger = logging.getLogger("uvicorn")

@app.post("/predict")
def predict(ticket: Ticket):
    logger.info(f"Predicting for: {ticket.message}")
    ...
```

Bonus:

- Structure your logs (use JSON format)
- Add request IDs for traceability

---

## ğŸ§± Part 4: Add Confidence-Based Output Logic

```python
@app.post("/predict")
def predict(ticket: Ticket):
    label, confidence = classify(ticket.message)

    if confidence < 0.6:
        return {
            "label": "uncertain",
            "confidence": round(confidence, 2),
            "note": "Low confidence â€” consider human review"
        }
```

This is where AI becomes usable. Donâ€™t just return predictions â€” **wrap them in responsibility**.

---

## ğŸ“¦ Microproject: Build a Real Prediction API

### Requirements:

- FastAPI with `/predict`
- Uses your real model (from Chapter 4 or 5)
- Logs every request
- Returns warning if confidence < 0.6
- Uses schema validation (e.g. message length)

### Bonus:

- Add a `/health` endpoint
- Track total requests processed
- Log to a file with rotation

---

## â“Reflection Questions

1. Whatâ€™s the difference between a working API and a production-ready one?
2. What input would break your current pipeline?
3. Where would you add versioning or metadata in the future?

---

## ğŸ§  Best Practices for Serving AI in Production

âœ… Use structured input/output schemas (Pydantic)  
âœ… Always log inputs and outputs (with request IDs)  
âœ… Validate inputs at the API layer â€” not deep in your model logic  
âœ… Avoid hardcoding thresholds â€” expose them as config  
âœ… Never trust a model blindly â€” return confidence, warnings, disclaimers  
âœ… Add health checks â€” let your API tell you when itâ€™s sick  
âœ… Plan for monitoring from day one â€” even if itâ€™s just print logs for now  
âœ… Prefer `POST` for inference â€” even if it feels like `GET`  
âœ… Keep your endpoints single-purpose and stateless  
âœ… Think versioning â€” models, prompts, API contracts

> ğŸ›¡ï¸ If the model fails, the system shouldnâ€™t.

---

## âœ… You Now Know:

âœ… How to wrap your model behind a clean FastAPI API  
âœ… How to validate, log, and handle errors like an engineer  
âœ… How to think about â€œtrustworthy outputsâ€ with confidence logic  
âœ… How to structure a serving layer for the real world

In the next chapter, weâ€™ll make it fault-tolerant and cache-friendly. Because things will fail â€” and your system has to survive that.