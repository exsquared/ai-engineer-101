# Hybrid Systems â€” When Rules and Learning Work Together

## ğŸŒ‰ Why Are We Here?

In the last chapter, we built a model that could route tickets across multiple departments â€” from billing to tech support to feedback. It was smart. But not smart enough.

Because hereâ€™s the thing about real-world AI systems:
> They *donâ€™t just guess* â€” they *decide with caution*.

What happens when the model isnâ€™t confident? Or when it sees something itâ€™s never encountered before? What about business rules â€” like escalating VIP users, or ignoring empty messages?

This is where hybrid systems shine.

In this chapter, youâ€™ll learn how to combine:

- ğŸ§  Machine learning (classifiers, confidence scores)
- ğŸ§¾ Hand-written rules (edge cases, exceptions)
- ğŸ¤– LLMs (as a backup brain for ambiguous inputs)

This is how production AI really works: **not all-learning, not all-logic â€” but a smart, explainable blend.**

---

## ğŸ¯ Goal

To design a robust, flexible pipeline that combines classical models, business rules, and fallback strategies â€” and knows when to ask for help.

---

## ğŸ§  The Setup: Why Multi-Class Isnâ€™t Enough

Remember our ticket classifier?
It could choose from `Billing`, `Technical`, `Feedback`, or `Other`.

But what happens when:

- The message is: â€œHello?â€
- The message is blank, or just an emoji
- A new ticket type appears: â€œI want to delete my accountâ€ (which doesnâ€™t fit any current label)
- The model is only **41% confident** in its guess
- The message is from a **VIP customer**

> None of these are classification problems. Theyâ€™re design problems.

Weâ€™ve now moved from *â€œwhat class?â€* to *â€œwhat should we do?â€*

Thatâ€™s the key shift: from prediction â†’ to action. And itâ€™s why we need a hybrid approach.

---

## ğŸ—ï¸ Whatâ€™s a Hybrid System?

A hybrid system is like a traffic cop:

- Sometimes it lets the model take the wheel.
- Sometimes it steps in to redirect, override, or pause.

> Think of it as a smart AI assistant with a supervisor â€” the rules.

You might:

- Use rules **before** the model to catch garbage inputs
- Use the model to make predictions
- Use rules **after** to decide what to do with the prediction
- Use an LLM when the modelâ€™s answer isnâ€™t confident or convincing

---

## ğŸ”§ Building the Logic + Model Pipeline

Letâ€™s sketch the core idea:

```python
def triage_pipeline(message, user_tags):
    if not message or len(message.strip()) < 5:
        return {"action": "ignore", "reason": "Empty or too short"}

    if "vip" in user_tags:
        return {"action": "escalate", "reason": "VIP customer"}

    prediction = model.predict(message)
    confidence = max(model.predict_proba([message])[0])

    if confidence < 0.5:
        return {"action": "defer_to_llm", "input": message, "confidence": confidence}

    return {"action": "route", "label": prediction, "confidence": confidence}
```

Youâ€™ve just built:

- âœ… Pre-model logic (message validation, VIP handling)
- âœ… Model prediction
- âœ… Post-model routing
- âœ… Fallback hook for LLMs

This is real-world AI plumbing â€” flexible, transparent, and safe.

---

## ğŸ¤– When to Use an LLM

LLMs are not magic. But they *are* useful when things get messy:

- The model has low confidence
- The user says something open-ended
- You want to explain, rephrase, or reclassify

```python
if confidence < 0.5:
    prompt = f"Classify this message: '{message}'"
    llm_response = call_llm(prompt)
    return {"action": "llm_fallback", "suggested_label": llm_response}
```

Think of the LLM as a junior analyst â€” smart, flexible, and verbose. But not in charge of critical decisions without supervision.

---

## ğŸ” Real-World Analogy: Triage Nurse

In hospitals, the triage nurse doesnâ€™t diagnose â€” they decide where a patient should go:

- Chest pain? â†’ Cardiologist
- Broken bone? â†’ Ortho
- Unclear symptoms? â†’ Doctor review

Your pipeline does the same:

- Obvious input? â†’ Model predicts
- Sensitive case? â†’ Rule escalates
- Ambiguous? â†’ LLM suggests

This analogy helps product teams, PMs, and engineers alike reason about AI behavior.

---

## ğŸ§ª Build Your Own Hybrid System

Take your ticket classifier from Chapter 6 and wrap it in a pipeline:

1. Pre-check for empty messages
2. Route VIPs straight to escalation
3. Run the model and extract confidence
4. Defer to an LLM if confidence < 0.5
5. Log every decision and why it happened

> Bonus: Print a side-by-side comparison of model vs. LLM label.

---

## ğŸ’¬ Reflection Corner

- Whatâ€™s a case in your product where youâ€™d want the model to *not* decide?
- Where might you use hard rules to override model behavior?
- Whatâ€™s your threshold for â€œlow confidenceâ€? Why?
- When does it make sense to blend prediction + human judgment?

---

## âœ… Exit Outcome

You now understand:

- Why real AI systems are not just models
- How to build a safe, explainable ML pipeline
- When to rely on logic, and when to ask for help

This is the foundation for **trustworthy AI in production.**

---

## â­ï¸ Coming Up

In the next chapter, weâ€™ll take a bold step forward into the world of Large Language Models (LLMs) â€” the powerful engines behind tools like ChatGPT and GitHub Copilot.

Youâ€™ve already used LLMs as fallbacks. Now, youâ€™ll learn how to work with them directly:

- Crafting smart prompts
- Automating workflows
- Using LLMs for classification, summarization, and structured reasoning

This is your entry into the modern AI toolkit â€” fast, flexible, and incredibly powerful.

Letâ€™s unlock it together. ğŸš€
