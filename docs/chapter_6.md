# Chapter 6: Beyond Yes/No ‚Äî Handling Multi-Class Predictions

---

## üéØ Goal

To understand how models handle **more than two outcomes**, what changes in evaluation and debugging, and how to build and test a multi-class classifier ‚Äî just like you‚Äôd ship in a real system.

---

### üß† Real-World Motivation

So far, you‚Äôve predicted things like:

- Is this urgent or not?  
- Yes or no?  
- Spam or ham?

But many real-world problems aren‚Äôt binary.

Let‚Äôs take support tickets again. This time, you want to classify tickets into:

- `Billing`
- `Technical Support`
- `Product Feedback`
- `Other`

This is no longer a yes/no ‚Äî it‚Äôs a **multi-class** decision.

---

### üß© What Changes?

You‚Äôre still training a model.
You still have features and labels.

But now:

- The label is not 0 or 1 ‚Äî it‚Äôs one of several classes.
- The model has to **pick the best class** for each input.
- Evaluation gets trickier.

---

### üß™ Code Walkthrough ‚Äî First Multi-Class Model

Let‚Äôs reuse what we already know ‚Äî with a twist.

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

# Example dataset
data = [
    ("Refund request for last month", "Billing"),
    ("App crashes on login", "Technical"),
    ("Love the new dashboard design!", "Feedback"),
    ("What time does support close?", "Other"),
]

# Convert to features (you can add NLP later)
# Example: use a simple vectorizer or dummy values
# X = ... 
y = [label for _, label in data]

# Placeholder for actual vectorization
X = [[1, 0], [0, 1], [1, 1], [0, 0]]  # Dummy features

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)

model = LogisticRegression(multi_class='multinomial', solver='lbfgs')
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
```

---

### üß† Debugging Multi-Class Is Different

You can‚Äôt just fix a threshold. You need to:

- **Look at the confusion matrix**
- **Identify which classes are overlapping**
- **Improve your features or data balance**

---

### üìä Visual Debug: Confusion Matrix

```python
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix

cm = confusion_matrix(y_test, y_pred, labels=model.classes_)
ConfusionMatrixDisplay(cm, display_labels=model.classes_).plot()
```

---

### üí° Class Imbalance Matters More Now

If only 5% of tickets are ‚ÄúFeedback‚Äù, the model might:

- Predict ‚ÄúBilling‚Äù 70% of the time ‚Äî and still look ‚Äúaccurate‚Äù
- Ignore rare classes

> Tip: Use `class_weight='balanced'` in `LogisticRegression` to help counter imbalance

---

### üí≠ Reflection Questions

1. Have you shipped features with more than 2 possible outcomes?
2. What happens when one class is rare ‚Äî but important?
3. Would you prefer your model to say ‚Äúunsure‚Äù rather than guess incorrectly?

---

### üíª Exercise: Your First Multi-Class Classifier

Build your own ticket classifier:

1. Create 20‚Äì30 example messages with 3‚Äì4 categories (Billing, Tech, Feedback, Other)
2. Extract features (start simple: keyword flags, message length)
3. Train a LogisticRegression model with `multi_class='multinomial'`
4. Print the classification report
5. Plot the confusion matrix

---

### üèÅ Small Project: Multi-Class Ticket Router

Use what you‚Äôve learned to build a smart routing engine.

#### Scenario:

You‚Äôre automating ticket routing for a helpdesk. Tickets need to go to the right team:

- `Billing`
- `Tech`
- `Feedback`
- `Other`

#### Build:

- A labeled dataset (30‚Äì40 examples)
- Feature extractor (keywords, patterns, length)
- Multinomial model
- Evaluation: precision/recall + confusion matrix
- CLI: paste in a message ‚Üí see predicted department

> Bonus: Show top-2 predictions if the model is unsure

---

### ‚úÖ Exit Outcome

You now know:

- How to build a multi-class model
- How to evaluate and debug per class
- How to deploy a real-world routing tool

---

### ‚è≠Ô∏è Coming Up

Next, we explore how to **blend rules and learning** into hybrid systems ‚Äî because sometimes the best solution isn‚Äôt pure AI or pure logic, but a smart mix of both.
