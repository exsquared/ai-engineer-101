# Final Project: AI-Powered Microservice â€” From Model to Production

---

## ğŸ¯ Goal

Build a complete AI system that:

- Classifies support tickets for urgency and category
- Uses both classical models and LLM fallback
- Applies rules and thresholds intelligently
- Exposes predictions via a FastAPI service
- Includes logging, validation, and observability

> This project simulates what you'd ship in a real-world application â€” only cleaner and more explainable.

---

## ğŸ”§ What Youâ€™ll Build

| Component | Description |
|----------|-------------|
| ğŸ§  ML Model | Logistic regression to classify urgency and category |
| ğŸ“– LLM Fallback | Use OpenAI (or mock) when confidence is low |
| ğŸ§ª Rule Layer | Pre-checks, threshold overrides, human-review triggers |
| ğŸŒ API | FastAPI service to expose `/predict` |
| ğŸ“œ Logging | Save inputs, predictions, confidence, fallback source |
| ğŸ§° Bonus | Add CLI, streamlit UI, or feedback loop |

---

## ğŸ§© Inputs and Outputs

### Input:

```json
{
  "message": "Hi, I got charged twice â€” please fix this ASAP!"
}
```

### Output:

```json
{
  "urgent": true,
  "category": "Billing",
  "confidence": 0.91,
  "source": "model",
  "action": "route"
}
```

---

## ğŸ“ File Layout Suggestion

```
/ticket_ai_microservice
â”œâ”€â”€ model_logic.py         # Feature extraction + prediction logic
â”œâ”€â”€ llm_fallback.py        # Call OpenAI or return simulated LLM response
â”œâ”€â”€ api.py                 # FastAPI wrapper
â”œâ”€â”€ utils.py               # Validation, confidence checks
â”œâ”€â”€ logs.jsonl             # Append input/output logs here
â”œâ”€â”€ data.csv               # Your labeled sample messages
â””â”€â”€ requirements.txt
```

---

## ğŸ› ï¸ Build Checklist

### âœ… Part 1: Rule + Model Pipeline

- Load trained `LogisticRegression` models
- Extract features (e.g., word count, exclamations)
- Predict `urgent` and `category`
- Log predictions and confidence

### âœ… Part 2: Add Fallback Logic

- If `confidence < 0.5`, call `llm_fallback(prompt)`
- Ensure LLM returns structured output (e.g., via regex or mock)

### âœ… Part 3: Serve with FastAPI

```http
POST /predict
{
  "message": "My plan is not working. Please help!"
}
```

Return:

- prediction
- confidence
- source ("model" or "llm")
- action ("route", "review", or "ignore")

### âœ… Part 4: Add Logging

- Append every request/response to `logs.jsonl`
- Include confidence, token usage (if LLM), and any errors

---

## ğŸ§ª Bonus Features

| Feature | Description |
|--------|-------------|
| ğŸ” Feedback Loop | Let user submit â€œcorrect categoryâ€ |
| ğŸ“‰ Rate Limiting | Prevent spam or rapid requests |
| ğŸ§‘â€ğŸ’¼ Admin View | Review predictions with low confidence |
| ğŸ›ï¸ Config File | Tune thresholds without editing code |
| ğŸŒ Streamlit UI | Input message + show model/LLM response live |

---

## ğŸ’­ Reflection Prompts

- What decisions were hardest to trust the model for?
- What failure types did you catch with logging?
- How would you monitor this in production?
- What would you add to make this secure?

---

## âœ… What Youâ€™ve Practiced

- Real model training (LogisticRegression)
- Confidence-aware fallback to LLMs
- API design and structured inputs
- Logging, explainability, and observability
- Blending AI with production software skills

> You didnâ€™t just build a model. You built a **shippable, testable, explainable AI service**.

---

### ğŸ† Stretch Goal

Deploy your API on Render, Railway, or Hugging Face Spaces.  
Even a demo in Streamlit counts.
